{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "tf.keras.config.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "M: int = 100\n",
    "N: int = 5\n",
    "N_CLASS: int = 3\n",
    "\n",
    "X, Y = make_classification(n_samples=M, n_features=N, n_classes=N_CLASS,\n",
    "                           n_informative=N-1, n_redundant=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = torch.nn.functional.one_hot(torch.tensor(Y).long(), 3)\n",
    "Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset int train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85, 5]) torch.Size([85, 3])\n",
      "torch.Size([15, 5]) torch.Size([15, 3])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid = torch.tensor(X[:85]), torch.tensor(X[85:])\n",
    "Y_train, Y_valid = Y_hat[:85], Y_hat[85:]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.is_trainable = False\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def construct(self):\n",
    "        pass\n",
    "\n",
    "    def backpro(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perceptron or full conect layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of the $l$-th layer:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W}^{(l)} &\\in \\mathbb{R}^{Q^{(l-1)} \\times Q^{(l)}} \\\\\n",
    "\\mathbf{b}^{(l)} &\\in \\mathbb{R}^{Q^{(l)}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $Q^{(l)}$ is the number of units of $l$-th layer,\n",
    "and $Q^{(l-1)}$ is the number of units from past layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted sum <br>\n",
    "$$\n",
    "\\mathbf{Z}^{(l)}(\\sigma^{(l-1)}) = \n",
    "\\sigma^{(l-1)} \\mathbf{W}^{(l)} + \\mathbf{b}^{(l)} \\\\\n",
    "\\mathbf{Z}^{(l)} : \\mathbb{R}^{M \\times Q^{(l-1)}} \\rightarrow\n",
    "\\mathbb{R}^{M \\times Q^{(l)}}\n",
    "$$\n",
    "where output from past layer $\\sigma^{(l-1)} \\in \\mathbb{R}^{M \\times Q^{(l-1)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the activation function $l$-th\n",
    "$$\n",
    "\\sigma^{(l)}(\\mathbf{Z}^{(l)}) = f(\\mathbf{Z}^{(l)}) \\\\\n",
    "\\sigma^{(l)} : \\mathbb{R}^{M \\times Q^{(l)}} \\rightarrow\n",
    "\\mathbb{R}^{M \\times Q^{(l)}}\n",
    "$$\n",
    "where $f$ is an arbitrary activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(Layer):\n",
    "    def __call__(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def backpro(self, delta: torch.Tensor, \n",
    "                a: torch.Tensor) -> torch.Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, units: int, act_f: ActivationFunction):\n",
    "        self.is_trainable = True\n",
    "        self.units = units\n",
    "        self.actf = act_f\n",
    "\n",
    "    def copy_param(self, b: np.array, w: np.array):\n",
    "        self.w.copy_(torch.tensor(w))\n",
    "        self.b.copy_(torch.tensor(b))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = torch.matmul(x, self.w) + self.b\n",
    "        return self.actf(z)\n",
    "\n",
    "    def construct(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n = x.shape[-1]\n",
    "        self.w = torch.randn(n, self.units)\n",
    "        self.b = torch.randn(self.units)\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def numel(self):\n",
    "        return self.b.shape, self.w.shape, self.b.numel() + self.w.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W^{(l)}}} = \\delta \n",
    "\\frac{\\partial \\sigma^{(l)}}{\\partial \\mathbf{Z^{(l)}}}\n",
    "\\frac{\\partial \\mathbf{Z^{(l)}}}{\\partial \\mathbf{W^{(l)}}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b^{(l)}}} = \\delta \n",
    "\\frac{\\partial \\sigma^{(l)}}{\\partial \\mathbf{Z^{(l)}}}\n",
    "\\frac{\\partial \\mathbf{Z^{(l)}}}{\\partial \\mathbf{b^{(l)}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New derivative\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{Z^{(l)}}}{\\partial \\sigma^{(l-1)}} \\in\n",
    "\\mathbb{R}^{(M \\times Q^{(l)}) \\times (M \\times Q^{(l-1)})}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\sigma^{(l-1)} \\mathbf{W}^{(l)} + \\mathbf{b}^{(l)} =\n",
    "\\begin{bmatrix}\n",
    "    \\sigma_{1}^T \\mathbf{w}_{1} + b_{1}^{(l)} &\n",
    "    \\sigma_{1}^T \\mathbf{w}_{2} + b_{2}^{(l)} &\n",
    "    \\cdots &\n",
    "    \\sigma_{1}^T \\mathbf{w}_{Q^{(l)}} + b_{Q^{(l)}}^{(l)} \\\\\n",
    "    \\sigma_{2}^T \\mathbf{w}_{1} + b_{1}^{(l)} &\n",
    "    \\sigma_{2}^T \\mathbf{w}_{2} + b_{2}^{(l)} &\n",
    "    \\cdots &\n",
    "    \\sigma_{2}^T \\mathbf{w}_{Q^{(l)}} + b_{Q^{(l)}}^{(l)} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\sigma_{M}^T \\mathbf{w}_{1} + b_{1}^{(l)} &\n",
    "    \\sigma_{M}^T \\mathbf{w}_{2} + b_{2}^{(l)} &\n",
    "    \\cdots &\n",
    "    \\sigma_{M}^T \\mathbf{w}_{Q^{(l)}} + b_{Q^{(l)}}^{(l)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\sigma_{p}^T = \\begin{bmatrix} \\sigma^{(l-1)}_{p1} & \\sigma^{(l-1)}_{p2} & \\cdots & \\sigma^{(l-1)}_{pQ^{(l-1)}}\\end{bmatrix}$ and $\\mathbf{w}_{q} = \\begin{bmatrix} w_{1q}^{(l)} & w_{2q}^{(l)} & \\cdots & w_{Q^{(l-1)}q}^{(l)}\\end{bmatrix}^T$. <br>\n",
    "therefore\n",
    "$$\n",
    "\\frac{\\partial (\\sigma^{(l-1)} \\mathbf{W}^{(l)} + \\mathbf{b}^{(l)})_{pq}}\n",
    "{\\partial \\sigma^{(l-1)}_{ij}} =\n",
    "\\frac{\\partial \\sigma_{p}^T \\mathbf{w}_{q}}{\\partial \\sigma^{(l-1)}_{ij}} = \\begin{cases}\n",
    "    w_{jq} & \\text{ if } p = i \\\\\n",
    "    0 & \\text{ if } p \\neq i\n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p,i = 1, ..., M$, $q = 1, ..., Q^{(l)}$ adnd $j = 1, ..., Q^{(l-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{Z^{(l)}}}{\\partial \\sigma^{(l-1)}} =\n",
    "\\mathbf{W}^{(l)^T} \\otimes \\mathbb{I}\n",
    "$$\n",
    "where $\\otimes$ is Kronecker product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(DenseLayer)\n",
    "def backpro(self, delta: torch.Tensor, out : torch.Tensor,\n",
    "            input_: torch.Tensor, lr: float):\n",
    "    m = len(input_)\n",
    "\n",
    "    # activation derivative\n",
    "    delta = self.actf.backpro(delta, out)\n",
    "\n",
    "    # weighted sum derivative\n",
    "    identity = torch.eye(self.units)\n",
    "    w_der = torch.kron(input_.unsqueeze(1).unsqueeze(3), \n",
    "                       identity.unsqueeze(0).unsqueeze(2))\n",
    "    w_der = torch.einsum('pq,pqij->ij', delta, w_der)\n",
    "    self.w -= lr * w_der\n",
    "    self.b -= lr * delta.sum(axis=0)\n",
    "\n",
    "    # rest of layers\n",
    "    identity = torch.eye(m).unsqueeze(1).unsqueeze(3)\n",
    "    w_der = torch.kron(self.w.T.unsqueeze(0).unsqueeze(2),\n",
    "                       identity)\n",
    "    delta = torch.einsum('pq,pqij->ij', delta, w_der)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(ActivationFunction):\n",
    "    def __call__(self, z):\n",
    "        z_exp = torch.exp(z)\n",
    "        return z_exp / z_exp.sum(1, keepdims=True)\n",
    "\n",
    "    def backpro(self, delta, a):\n",
    "        m, q = a.shape\n",
    "        diag_a = torch.diag_embed(a)\n",
    "        outer_a = torch.einsum('ij,ik->ijk', a, a)\n",
    "            \n",
    "        sus_a = diag_a - outer_a\n",
    "            \n",
    "        soft_der = torch.zeros((m, q, m, q), dtype=a.dtype)\n",
    "        idx = torch.arange(m)\n",
    "        soft_der[idx, :, idx, :] = sus_a\n",
    "        return torch.einsum('pq,pqij->ij', delta, soft_der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid funcion\n",
    "$$\n",
    "\\text{sigmoid}(\\mathbf{Z}) = \\frac{1}{1 + \\exp{(-\\mathbf{Z})}} \\\\\n",
    "\\text{sigmoid} : \\mathbb{R}^{M \\times Q} \\rightarrow\n",
    "\\mathbb{R}^{M \\times Q}\n",
    "$$\n",
    "where $\\exp(-\\mathbf{Z})$ is element-wise power $\\exp(-z_{ij})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid derivative\n",
    "$$\n",
    "\\frac{\\partial \\text{sigmoid}}{\\partial \\mathbf{Z}} \\in\n",
    "\\mathbb{R}^{(M \\times Q) \\times (M \\times Q)}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\text{sigmoid}(\\mathbf{Z}) = \\begin{bmatrix}\n",
    "    \\text{sigmoid}(z_{11}) & \n",
    "    \\text{sigmoid}(z_{12}) &\n",
    "    \\cdots &\n",
    "    \\text{sigmoid}(z_{1Q}) \\\\\n",
    "    \\text{sigmoid}(z_{21}) & \n",
    "    \\text{sigmoid}(z_{22}) &\n",
    "    \\cdots &\n",
    "    \\text{sigmoid}(z_{2Q}) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\text{sigmoid}(z_{M1}) & \n",
    "    \\text{sigmoid}(z_{M2}) &\n",
    "    \\cdots &\n",
    "    \\text{sigmoid}(z_{MQ}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore\n",
    "$$\n",
    "\\frac{\\partial \\text{sigmoid}(z_{pq})}{\\partial z_{ij}} =\n",
    "\\begin{cases}\n",
    "    \\text{sigmoid}(z_{pq})\n",
    "    (1 - \\text{sigmoid}(z_{pq})) & \\text{ if } p=i, q=j \\\\ \n",
    "    0 & \\text{ otherwise} \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p,i = 1, ..., M$ and $q,j = 1, ..., Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, z):\n",
    "        return 1 / (1 + torch.exp(-z))\n",
    "    \n",
    "    def backpro(self, delta, a):\n",
    "        m, n = a.shape\n",
    "        der = a * (1 - a)\n",
    "        result = torch.zeros((m, n, m, n), dtype=a.dtype)\n",
    "        idx = torch.arange(m), torch.arange(n)\n",
    "        result[idx[0][:, None], idx[1], idx[0][:, None], idx[1]] = der\n",
    "        return torch.einsum('pq,pqij->ij', delta, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU function\n",
    "$$\n",
    "\\text{ReLU}(\\mathbf{Z}) = \\max(\\mathbf{Z}, 0) \\\\\n",
    "\\text{ReLU} : \\mathbb{R}^{M \\times Q} \\rightarrow\n",
    "\\mathbb{R}^{M \\times Q}\n",
    "$$\n",
    "where $\\max(\\mathbf{Z}, 0)$ is element-wise $\\max(z_{ij}, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU derivative\n",
    "$$\n",
    "\\frac{\\partial \\text{ReLU}}{\\partial \\mathbf{Z}} \\in\n",
    "\\mathbb{R}^{(M \\times Q) \\times (M \\times Q)}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\frac{\\partial \\max(z_{ij},0)}{\\partial z_{ij}} =\n",
    "\\begin{cases}\n",
    "    1 & \\text{ if } z_{ij} > 0 \\\\\n",
    "    0 & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "therefore\n",
    "$$\n",
    "\\frac{\\partial \\text{ReLU}_{pq}}{\\partial z_{ij}} =\n",
    "\\frac{\\partial \\max(z_{pq}, 0)}{\\partial z_{ij}} =\n",
    "\\begin{cases}\n",
    "    \\frac{\\partial \\max(z_{ij},0)}{\\partial z_{ij}} & \\text{ if } p=i, q=j \\\\\n",
    "    0 & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(ActivationFunction):\n",
    "    def __call__(self, z):\n",
    "        return torch.max(z, torch.zeros_like(z))\n",
    "\n",
    "    def backpro(self, delta, a):\n",
    "        m, n = a.shape\n",
    "    \n",
    "        #relu_der = (a > 0).float()\n",
    "        relu_der = (a > 0).double()\n",
    "        result = torch.zeros((m, n, m, n), dtype=a.dtype)\n",
    "        \n",
    "        idx = torch.arange(m), torch.arange(n)\n",
    "        result[idx[0][:, None], idx[1], idx[0][:, None], idx[1]] = relu_der\n",
    "        return torch.einsum('pq,pqij->ij', delta, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Layer):\n",
    "    def __call__(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "        pass\n",
    "\n",
    "    def backpro(self, y_pred: torch.Tensor, \n",
    "                y_true: torch.Tensor) -> torch.Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        e = y_pred - y_true\n",
    "        return (e**2).sum().item() / len(y_true)\n",
    "    \n",
    "    def backpro(self, y_pred, y_true):\n",
    "        return 2 * (y_pred - y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CE(Loss):\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        loss = y_true * torch.log(y_pred)\n",
    "        return - loss.sum().item() / len(y_true)\n",
    "\n",
    "    def backpro(self, y_pred, y_true):\n",
    "        return -(y_true / y_pred) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other useful layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer(Layer):\n",
    "    def __init__(self, n_features: int):\n",
    "        self.n_features = n_features\n",
    "        self.m_samples = 10\n",
    "\n",
    "    def construct(self):\n",
    "        x_fake = torch.randn(self.m_samples, self.n_features)\n",
    "        return x_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: list[Layer], loss: Loss):\n",
    "        self.layers = layers[1:]\n",
    "        self.loss = loss\n",
    "\n",
    "        data = layers[0].construct()\n",
    "        for l in self.layers:\n",
    "            data = l.construct(data)\n",
    "\n",
    "    def summary(self):\n",
    "        num_params = 0\n",
    "        for i,l in enumerate(self.layers):\n",
    "            if not l.is_trainable: continue\n",
    "            numel = l.numel()\n",
    "            print(f'layer[{i}] - {numel[:-1]} - total: {numel[-1]}')\n",
    "            num_params += numel[-1]\n",
    "        print('\\nTotal parameters:')\n",
    "        print(f'\\ttotal: {num_params}')\n",
    "\n",
    "    def copy_params(self, tf_weights: list):\n",
    "        for l in self.layers:\n",
    "            if not l.is_trainable: continue\n",
    "            l.copy_param(tf_weights.pop(1).numpy(),\n",
    "                         tf_weights.pop(0).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "\\sigma^{(0)} := \\mathbf{X} \\\\\n",
    "\\textbf{for } l=1 \\text{ to } L \\textbf{ do} \\\\\n",
    "\\quad \\mathbf{Z}^{(l)} = \n",
    "\\sigma^{(l-1)} \\mathbf{W}^{(l)} + \\mathbf{b}^{(l)} \\\\\n",
    "\\quad \\sigma^{(l)} = f(\\mathbf{Z}^{(l)}) \\\\\n",
    "\\textbf{end for}\n",
    "\\end{array}\n",
    "$$\n",
    "where $L$ is the number of layers in the Model. <br>\n",
    "Consequently, the prediction is:\n",
    "$$\n",
    "\\mathbf{\\hat{Y}} = \\sigma^{(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    out = x.clone()\n",
    "    for l in self.layers:\n",
    "        out = l.forward(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)\n",
    "def __forward__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    self.outs = [x.clone()]\n",
    "    for l in self.layers:\n",
    "        self.outs.append(l.forward(self.outs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)\n",
    "def evaluate(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    y_pred = self.predict(x)\n",
    "    return self.loss(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "\\delta := \\frac{\\partial L}{\\partial \\sigma^{(L)}} \\\\\n",
    "\\textbf{for } l = L \\text{ to } 1 \\textbf{ do} \\\\\n",
    "\\quad \\delta := \\delta\n",
    "\\frac{\\partial \\sigma^{(l)}}{\\partial \\mathbf{Z}^{(l)}} \\\\\n",
    "\\quad\n",
    "\\frac{\\partial L}{\\partial \\theta^{(l)}} =\n",
    "\\delta \\frac{\\partial \\mathbf{Z}^{(l)}}{\\partial \\theta^{(l)}} \\\\\n",
    "\\quad \\delta := \\delta\n",
    "\\frac{\\partial \\mathbf{Z}^{(l)}}{\\partial \\sigma^{(l-1)}} \\\\\n",
    "\\textbf{end for}\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\theta^{(l)} = (\\mathbf{b}^{(l)}, \\mathbf{W}^{(l)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor,\n",
    "           lr: float):\n",
    "    delta = self.loss.backpro(self.outs[-1], y_true)\n",
    "\n",
    "    for l in reversed(self.layers):\n",
    "        delta = l.backpro(delta, self.outs.pop(), self.outs[-1], lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Model)\n",
    "def fit(self, x_train: torch.Tensor, y_train: torch.Tensor,\n",
    "        epochs: int, lr: float, batch_size: int,\n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        num_batch = 0\n",
    "        for batch in range(0, len(y_train), batch_size):\n",
    "            num_batch += 1\n",
    "            x_b = x_train[batch:batch+batch_size]\n",
    "            y_b = y_train[batch:batch+batch_size]\n",
    "\n",
    "            self.__forward__(x_b)\n",
    "            loss += self.loss(self.outs[-1], y_b)\n",
    "\n",
    "            self.update(x_b, y_b, lr)\n",
    "\n",
    "        loss = round(loss / num_batch, 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        print(f'epoch: {epoch} - L: {loss} - L_v: {loss_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCHS = 16\n",
    "BATCH = len(X_train) // 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 1.0874\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m60\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m77\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m24\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161</span> (1.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m161\u001b[0m (1.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161</span> (1.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m161\u001b[0m (1.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TFModel = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=10, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(units=7, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=N_CLASS, activation='softmax')\n",
    "])\n",
    "\n",
    "TFModel.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=LR),\n",
    "    metrics = [tf.keras.metrics.Accuracy()]\n",
    ")\n",
    "\n",
    "TFModel.evaluate(X_train[:1], Y_train[:1])\n",
    "\n",
    "TFModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer[0] - (torch.Size([10]), torch.Size([5, 10])) - total: 60\n",
      "layer[1] - (torch.Size([7]), torch.Size([10, 7])) - total: 77\n",
      "layer[2] - (torch.Size([3]), torch.Size([7, 3])) - total: 24\n",
      "\n",
      "Total parameters:\n",
      "\ttotal: 161\n"
     ]
    }
   ],
   "source": [
    "model = Model([InputLayer(N),\n",
    "               DenseLayer(10, Sigmoid()),\n",
    "               DenseLayer(7, Relu()),\n",
    "               DenseLayer(N_CLASS, Softmax())],\n",
    "               CE())\n",
    "\n",
    "model.copy_params(TFModel.weights[:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(tensor_true, tensor_pred) -> float:\n",
    "    \"\"\"\n",
    "     Calculates the percentage error between two tensors or floats.\n",
    "\n",
    "     If the arguments are simple floats or ints, calculate the percentage error between them.\n",
    "     If the arguments are Numpy ndarray and PyTorch tensor, calculate the percentage error between them.\n",
    "     If the argumens are PyTorch tensors, calculate the percentage error between them.\n",
    "\n",
    "     Args:\n",
    "         tensor_true: The true tensor or true float.\n",
    "         pred_tensor: The predicted tensor or the predicted float.\n",
    "\n",
    "     Returns:\n",
    "         The percentage error between the tensors or floats.\n",
    "     \"\"\"\n",
    "    if isinstance(tensor_true, (float, int)) and isinstance(tensor_pred, (float, int)):\n",
    "        return np.abs(tensor_true - tensor_pred) / np.abs(tensor_true) * 100\n",
    "    elif type(tensor_true) is np.ndarray and type(tensor_pred) is torch.Tensor:\n",
    "        e = np.abs(tensor_true - tensor_pred.numpy()) / np.abs(tensor_true)\n",
    "        return np.mean(e) * 100\n",
    "    e = torch.abs(tensor_true - tensor_pred) / torch.abs(tensor_true)\n",
    "    return torch.mean(e) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.406321093568787e-15"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.predict(X_valid, batch_size=len(X_valid))\n",
    "predict = model.predict(X_valid)\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0000e+00 - loss: 1.1178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.972987550739853e-14"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.evaluate(X_train, Y_train, batch_size=len(X_train))[0]\n",
    "predict = model.evaluate(X_train, Y_train)\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.0000e+00 - loss: 1.1153 - val_accuracy: 0.0000e+00 - val_loss: 1.1336\n",
      "Epoch 2/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.0000e+00 - loss: 1.1140 - val_accuracy: 0.0000e+00 - val_loss: 1.1336\n",
      "Epoch 3/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0000e+00 - loss: 1.1127 - val_accuracy: 0.0000e+00 - val_loss: 1.1336\n",
      "Epoch 4/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 1.1116 - val_accuracy: 0.0000e+00 - val_loss: 1.1336\n",
      "Epoch 5/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.0000e+00 - loss: 1.1104 - val_accuracy: 0.0000e+00 - val_loss: 1.1333\n",
      "Epoch 6/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 1.1092 - val_accuracy: 0.0000e+00 - val_loss: 1.1331\n",
      "Epoch 7/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 1.1081 - val_accuracy: 0.0000e+00 - val_loss: 1.1329\n",
      "Epoch 8/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 1.1070 - val_accuracy: 0.0000e+00 - val_loss: 1.1327\n",
      "Epoch 9/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.0000e+00 - loss: 1.1059 - val_accuracy: 0.0000e+00 - val_loss: 1.1326\n",
      "Epoch 10/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 1.1049 - val_accuracy: 0.0000e+00 - val_loss: 1.1325\n",
      "Epoch 11/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0000e+00 - loss: 1.1039 - val_accuracy: 0.0000e+00 - val_loss: 1.1324\n",
      "Epoch 12/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0000e+00 - loss: 1.1029 - val_accuracy: 0.0000e+00 - val_loss: 1.1323\n",
      "Epoch 13/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 1.1020 - val_accuracy: 0.0000e+00 - val_loss: 1.1323\n",
      "Epoch 14/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.0000e+00 - loss: 1.1012 - val_accuracy: 0.0000e+00 - val_loss: 1.1323\n",
      "Epoch 15/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0000e+00 - loss: 1.1004 - val_accuracy: 0.0000e+00 - val_loss: 1.1323\n",
      "Epoch 16/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 1.0996 - val_accuracy: 0.0000e+00 - val_loss: 1.1324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x246bda19ad0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFModel.fit(X_train, Y_train, batch_size=BATCH, epochs=EPOCHS,\n",
    "            shuffle=False, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - L: 1.1128 - L_v: 1.1336\n",
      "epoch: 1 - L: 1.1098 - L_v: 1.1336\n",
      "epoch: 2 - L: 1.1066 - L_v: 1.1336\n",
      "epoch: 3 - L: 1.1037 - L_v: 1.1336\n",
      "epoch: 4 - L: 1.1009 - L_v: 1.1333\n",
      "epoch: 5 - L: 1.0981 - L_v: 1.1331\n",
      "epoch: 6 - L: 1.0954 - L_v: 1.1329\n",
      "epoch: 7 - L: 1.0927 - L_v: 1.1327\n",
      "epoch: 8 - L: 1.0901 - L_v: 1.1326\n",
      "epoch: 9 - L: 1.0876 - L_v: 1.1325\n",
      "epoch: 10 - L: 1.0852 - L_v: 1.1324\n",
      "epoch: 11 - L: 1.0829 - L_v: 1.1324\n",
      "epoch: 12 - L: 1.0807 - L_v: 1.1323\n",
      "epoch: 13 - L: 1.0786 - L_v: 1.1323\n",
      "epoch: 14 - L: 1.0765 - L_v: 1.1323\n",
      "epoch: 15 - L: 1.0745 - L_v: 1.1324\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, EPOCHS, LR, BATCH, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
