{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.config import set_floatx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_floatx('float64')\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors \n",
    "Mean Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(a_true: np.ndarray, a_pred: torch.Tensor) -> float:\n",
    "    e = np.abs(a_true - a_pred.numpy()) / np.abs(a_true)\n",
    "    return np.mean(e) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one example/entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0000, -0.5000,  7.0000,  0.5000,  5.5000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = 5 # number of classes\n",
    "\n",
    "Z = torch.randint(-20, 21, (1, Q)) / 2\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent $\\text{softmax}$ like $\\sigma$.\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_{j} = \\frac{\\exp (\\mathbf{z})_{j}}{\\sum_{k=1}^{Q} \\exp (z_{k})}\n",
    "$$\n",
    "where $Q$ is the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.36317782e-05, 4.51422496e-04, 8.16191019e-01, 1.22709357e-03,\n",
       "        1.82116833e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOFTMAX = Softmax()\n",
    "tf_soft_1 = SOFTMAX(Z)\n",
    "tf_soft_1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our softmax funcion\n",
    "def softmax_1(z: torch.Tensor) -> torch.Tensor:\n",
    "    exp = torch.exp(z)\n",
    "    return exp / exp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.36317782e-05, 4.51422496e-04, 8.16191019e-01, 1.22709357e-03,\n",
       "        1.82116833e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_soft_1 = softmax_1(Z)\n",
    "my_soft_1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.607709770028611e-15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# error\n",
    "error(tf_soft_1.numpy(), my_soft_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple examples/entrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 100 # number of examples/entrys\n",
    "\n",
    "Z = torch.randint(-20, 21, (M, Q)) / 2\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z} = \\begin{bmatrix}\n",
    "    \\mathbf{z}_{1,:} \\\\\n",
    "    \\mathbf{z}_{2,:} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{z}_{M,:}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "then its softmax over each row is like\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{z}_{M,:}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_soft_2 = SOFTMAX(Z)\n",
    "tf_soft_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our function\n",
    "def softmax_2(z: torch.Tensor) -> torch.Tensor:\n",
    "    exp = torch.exp(z)\n",
    "    return exp / exp.sum(dim=-1, keepdims=True)\n",
    "\n",
    "my_soft_2 = softmax_2(Z)\n",
    "my_soft_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2248504348326294e-14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(tf_soft_2.numpy(), my_soft_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivative one softmax respect to one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "[[-3.04374453e-07 -3.42528322e-14 -8.38139170e-12  3.05137337e-07\n",
      "  -7.54468837e-10]]\n"
     ]
    }
   ],
   "source": [
    "N_FEATURE = 3 # select one feature to derivative\n",
    "\n",
    "Z = torch.randint(-20, 21, (1, Q)) / 2\n",
    "\n",
    "def der_soft_1(z):\n",
    "    exp = np.exp(z)\n",
    "    return exp[0,N_FEATURE] / np.sum(exp)\n",
    "\n",
    "gradient = jacobian(der_soft_1)\n",
    "grad = gradient(Z.numpy())\n",
    "print(grad.shape)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}}\n",
    "\\in \\mathbb{R} \\times (1 \\times Q) \\Leftrightarrow 1 \\times Q\n",
    "$$\n",
    "because $\\mathbf{z} \\in 1 \\times Q$ and $\\sigma(\\mathbf{z})_{j} \\in \\mathbb{R}$. <br>\n",
    "Then its jacobian in **Numerator layout** is:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{Q}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "there are two different types of the derivatives:\n",
    "1. $\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i=j}}$\n",
    "\n",
    "2. $\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i\\neq j}}$\n",
    "\n",
    "First case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i=j}} = \n",
    "\\sigma(\\mathbf{z})_{j} (1 - \\sigma(\\mathbf{z})_j)\n",
    "$$\n",
    "\n",
    "Second case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i\\neq j}} =\n",
    "-\\sigma(\\mathbf{z})_{j} \\sigma(\\mathbf{z})_{i}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    -\\sigma(\\mathbf{z})_{j} \\sigma(\\mathbf{z})_{1} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z})_{j}(1 - \\sigma(\\mathbf{z})_j) &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{j} \\sigma(\\mathbf{z})_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "or as vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}} =\n",
    "\\sigma(\\mathbf{z})_j \\odot\n",
    "\\begin{bmatrix}\n",
    "    -\\sigma(\\mathbf{z})_{1} &\n",
    "    \\cdots &\n",
    "    1 - \\sigma(\\mathbf{z})_{j} &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{Q}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.04374453e-07, -3.42528322e-14, -8.38139170e-12,\n",
       "         3.05137337e-07, -7.54468837e-10]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_soft_1(z, j):\n",
    "    soft = softmax_1(z)\n",
    "    soft_j = soft[0,j].item()\n",
    "    soft *= -1\n",
    "    soft[0,j] += 1\n",
    "    return soft_j * soft\n",
    "\n",
    "my_grad = my_der_soft_1(Z, N_FEATURE)\n",
    "my_grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.16346590378772e-15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(grad, my_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivative of multiple softmax respecto to one example/entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del gradient, grad\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.35476576e-01, -1.58367140e-02, -4.78227326e-04,\n",
       "          -1.17018368e-01, -2.14326618e-03]],\n",
       "\n",
       "        [[-1.58367140e-02,  8.84019584e-02, -2.90059535e-04,\n",
       "          -7.09752282e-02, -1.29995665e-03]],\n",
       "\n",
       "        [[-4.78227326e-04, -2.90059535e-04,  2.95080833e-03,\n",
       "          -2.14326618e-03, -3.92552894e-05]],\n",
       "\n",
       "        [[-1.17018368e-01, -7.09752282e-02, -2.14326618e-03,\n",
       "           1.99742315e-01, -9.60545261e-03]],\n",
       "\n",
       "        [[-2.14326618e-03, -1.29995665e-03, -3.92552894e-05,\n",
       "          -9.60545261e-03,  1.30879307e-02]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = torch.randint(-20, 21, (1, Q)) / 2\n",
    "\n",
    "def der_soft_2(z):\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "gradient = jacobian(der_soft_2)\n",
    "grad = gradient(Z.numpy())\n",
    "print(grad.shape)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} \\in\n",
    "(1 \\times Q) \\times (1 \\times Q)\n",
    "$$\n",
    "but to simplify we will use an easier notation:\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} \\in \n",
    "Q \\times Q\n",
    "$$\n",
    "we will ignore the 1's axes for now. <br>\n",
    "The derivative is like:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} &=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{1}}{\\partial \\mathbf{z}} \\\\\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{2}}{\\partial \\mathbf{z}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{Q}}{\\partial \\mathbf{z}}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{1}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{1}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{1}}{\\partial z_{Q}} \\\\\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{2}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{2}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{2}}{\\partial z_{Q}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{Q}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{Q}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{Q}}{\\partial z_{Q}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "then using this derivatives:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i=j}} &= \n",
    "\\sigma(\\mathbf{z})_{j} (1 - \\sigma(\\mathbf{z})_j) \\\\\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i\\neq j}} &=\n",
    "-\\sigma(\\mathbf{z})_{j} \\sigma(\\mathbf{z})_{i}\n",
    "\\end{align*}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z})_{1} (1 - \\sigma(\\mathbf{z})_1) &\n",
    "    -\\sigma(\\mathbf{z})_{1} \\sigma(\\mathbf{z})_{2} &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{1} \\sigma(\\mathbf{z})_{Q} \\\\\n",
    "    -\\sigma(\\mathbf{z})_{2} \\sigma(\\mathbf{z})_{1} &\n",
    "    \\sigma(\\mathbf{z})_{2} (1 - \\sigma(\\mathbf{z})_2) &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{2} \\sigma(\\mathbf{z})_{Q} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -\\sigma(\\mathbf{z})_{Q} \\sigma(\\mathbf{z})_{1} &\n",
    "    -\\sigma(\\mathbf{z})_{Q} \\sigma(\\mathbf{z})_{2} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z})_{Q} (1 - \\sigma(\\mathbf{z})_Q)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "or as vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} =\n",
    "\\text{diag}(\\sigma(\\mathbf{z})) - \\sigma(\\mathbf{z}) \\sigma(\\mathbf{z})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.35476576e-01, -1.58367140e-02, -4.78227326e-04,\n",
       "        -1.17018368e-01, -2.14326618e-03],\n",
       "       [-1.58367140e-02,  8.84019584e-02, -2.90059535e-04,\n",
       "        -7.09752282e-02, -1.29995665e-03],\n",
       "       [-4.78227326e-04, -2.90059535e-04,  2.95080833e-03,\n",
       "        -2.14326618e-03, -3.92552894e-05],\n",
       "       [-1.17018368e-01, -7.09752282e-02, -2.14326618e-03,\n",
       "         1.99742315e-01, -9.60545261e-03],\n",
       "       [-2.14326618e-03, -1.29995665e-03, -3.92552894e-05,\n",
       "        -9.60545261e-03,  1.30879307e-02]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_soft_2(z):\n",
    "    soft = softmax_1(z)[0,:] # is necessary for Pytorch to work\n",
    "    return torch.diag(soft) - torch.outer(soft, soft)\n",
    "\n",
    "my_grad = my_der_soft_2(Z)\n",
    "my_grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.896657617298521e-15"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(grad[0,:,0,:], my_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivative of multiple softmax respect to multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z} \\in M \\times Q\n",
    "$$\n",
    "where $M$ is the number of examples. Then softmax function is:\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{Z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{Z}_{M,:})\n",
    "\\end{bmatrix} \\in M \\times Q\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{Z}_{p,:} = \\begin{bmatrix}\n",
    "    Z_{p1} & Z_{p2} & \\cdots & Z_{pQ}\n",
    "\\end{bmatrix} \\in 1 \\times Q\n",
    "$$\n",
    "for all $p = 1, ..., M$. Therefore\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}_{p,:}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{p,:})_{1} & \n",
    "    \\sigma(\\mathbf{Z}_{p,:})_{2} & \n",
    "    \\cdots & \n",
    "    \\sigma(\\mathbf{Z}_{p,:})_{Q}\n",
    "\\end{bmatrix} \\in 1 \\times Q\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del gradient, grad\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "Z = torch.randint(-20, 21, (M, Q)) / 2\n",
    "\n",
    "def der_soft_3(z):\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "gradient = jacobian(der_soft_3)\n",
    "grad = gradient(Z.numpy())\n",
    "print(grad.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
