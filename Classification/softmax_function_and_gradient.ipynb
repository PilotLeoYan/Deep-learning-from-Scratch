{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.config import set_floatx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_floatx('float64')\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors \n",
    "Mean Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(a_true: np.ndarray, a_pred: torch.Tensor) -> float:\n",
    "    e = np.abs(a_true - a_pred.numpy()) / np.abs(a_true)\n",
    "    return np.mean(e) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one example/entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8., -10.,  -3.,  10.,   4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = 5 # number of classes\n",
    "\n",
    "Z = torch.randint(-20, 21, (1, Q)) / 2\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent $\\text{softmax}$ like $\\sigma$.\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_{j} = \\frac{\\exp (\\mathbf{z})_{j}}{\\sum_{k=1}^{Q} \\exp (z_{k})}\n",
    "$$\n",
    "where $Q$ is the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.51922872e-08, 2.05605249e-09, 2.25473534e-06, 9.97525110e-01,\n",
       "        2.47261754e-03]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOFTMAX = Softmax()\n",
    "tf_soft_1 = SOFTMAX(Z)\n",
    "tf_soft_1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our softmax funcion\n",
    "def softmax_1(z: torch.Tensor) -> torch.Tensor:\n",
    "    exp = torch.exp(z)\n",
    "    return exp / exp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.51922872e-08, 2.05605249e-09, 2.25473534e-06, 9.97525110e-01,\n",
       "        2.47261754e-03]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_soft_1 = softmax_1(Z)\n",
    "my_soft_1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7620633875561403e-14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# error\n",
    "error(tf_soft_1.numpy(), my_soft_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple examples/entrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 100 # number of examples/entrys\n",
    "\n",
    "Z = torch.randint(-20, 21, (M, Q)) / 2\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z} = \\begin{bmatrix}\n",
    "    \\mathbf{z}_{1,:} \\\\\n",
    "    \\mathbf{z}_{2,:} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{z}_{M,:}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "then its softmax over each row is like\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{z}_{M,:}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_soft_2 = SOFTMAX(Z)\n",
    "tf_soft_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our function\n",
    "def softmax_2(z: torch.Tensor) -> torch.Tensor:\n",
    "    exp = torch.exp(z)\n",
    "    return exp / exp.sum(dim=-1, keepdims=True)\n",
    "\n",
    "my_soft_2 = softmax_2(Z)\n",
    "my_soft_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1598623514224193e-14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(tf_soft_2.numpy(), my_soft_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivative one softmax respect to one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "[[-6.77838593e-02 -3.07738245e-06 -1.39712947e-10  6.90284415e-02\n",
      "  -1.24150469e-03]]\n"
     ]
    }
   ],
   "source": [
    "N_FEATURE = 3 # select one feature to derivative\n",
    "\n",
    "Z = torch.randint(-20, 21, (1, Q)) / 2\n",
    "\n",
    "def der_soft_1(z):\n",
    "    exp = np.exp(z)\n",
    "    return exp[0,N_FEATURE] / np.sum(exp)\n",
    "\n",
    "gradient = jacobian(der_soft_1)\n",
    "grad = gradient(Z.numpy())\n",
    "print(grad.shape)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}}\n",
    "\\in \\mathbb{R} \\times (1 \\times Q) \\Leftrightarrow 1 \\times Q\n",
    "$$\n",
    "because $\\mathbf{z} \\in 1 \\times Q$ and $\\sigma(\\mathbf{z})_{j} \\in \\mathbb{R}$. <br>\n",
    "Then its jacobian in **Numerator layout** is:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{Q}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "there are two different types of the derivatives:\n",
    "1. $\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i=j}}$\n",
    "\n",
    "2. $\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i\\neq j}}$\n",
    "\n",
    "First case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i=j}} = \n",
    "\\sigma(\\mathbf{z})_{j} (1 - \\sigma(\\mathbf{z})_j)\n",
    "$$\n",
    "\n",
    "Second case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i\\neq j}} =\n",
    "-\\sigma(\\mathbf{z})_{j} \\sigma(\\mathbf{z})_{i}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    -\\sigma(\\mathbf{z})_{j} \\sigma(\\mathbf{z})_{1} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z})_{j}(1 - \\sigma(\\mathbf{z})_j) &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{j} \\sigma(\\mathbf{z})_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "or as vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}} =\n",
    "\\sigma(\\mathbf{z})_j \\odot\n",
    "\\begin{bmatrix}\n",
    "    -\\sigma(\\mathbf{z})_{1} &\n",
    "    \\cdots &\n",
    "    1 - \\sigma(\\mathbf{z})_{j} &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{Q}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.77838593e-02, -3.07738245e-06, -1.39712947e-10,\n",
       "         6.90284415e-02, -1.24150469e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_soft_1(z, j):\n",
    "    soft = softmax_1(z)\n",
    "    soft_j = soft[0,j].item()\n",
    "    soft *= -1\n",
    "    soft[0,j] += 1\n",
    "    return soft_j * soft\n",
    "\n",
    "my_grad = my_der_soft_1(Z, N_FEATURE)\n",
    "my_grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0266523470111423e-14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(grad, my_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivative of multiple softmax respect to one example/entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del gradient, grad\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 2.12703216e-01, -8.58746885e-05, -5.71188305e-02,\n",
       "          -1.55265079e-01, -2.33431605e-04]],\n",
       "\n",
       "        [[-8.58746885e-05,  2.79756608e-04, -5.20856315e-05,\n",
       "          -1.41583426e-04, -2.12862071e-07]],\n",
       "\n",
       "        [[-5.71188305e-02, -5.20856315e-05,  1.51485530e-01,\n",
       "          -9.41730309e-02, -1.41583426e-04]],\n",
       "\n",
       "        [[-1.55265079e-01, -1.41583426e-04, -9.41730309e-02,\n",
       "           2.49964557e-01, -3.84863653e-04]],\n",
       "\n",
       "        [[-2.33431605e-04, -2.12862071e-07, -1.41583426e-04,\n",
       "          -3.84863653e-04,  7.60091546e-04]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = torch.randint(-20, 21, (1, Q)) / 2\n",
    "\n",
    "def der_soft_2(z):\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "gradient = jacobian(der_soft_2)\n",
    "grad = gradient(Z.numpy())\n",
    "print(grad.shape)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} \\in\n",
    "(1 \\times Q) \\times (1 \\times Q)\n",
    "$$\n",
    "but to simplify we will use an easier notation:\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} \\in \n",
    "Q \\times Q\n",
    "$$\n",
    "we will ignore the 1's axes for now. <br>\n",
    "The derivative is like:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} &=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{1}}{\\partial \\mathbf{z}} \\\\\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{2}}{\\partial \\mathbf{z}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{Q}}{\\partial \\mathbf{z}}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{1}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{1}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{1}}{\\partial z_{Q}} \\\\\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{2}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{2}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{2}}{\\partial z_{Q}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{Q}}{\\partial z_{1}} &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{Q}}{\\partial z_{2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial \\sigma(\\mathbf{z})_{Q}}{\\partial z_{Q}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "then using this derivatives:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i=j}} &= \n",
    "\\sigma(\\mathbf{z})_{j} (1 - \\sigma(\\mathbf{z})_j) \\\\\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i\\neq j}} &=\n",
    "-\\sigma(\\mathbf{z})_{j} \\sigma(\\mathbf{z})_{i}\n",
    "\\end{align*}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} =\n",
    "\\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{z})_{1} (1 - \\sigma(\\mathbf{z})_1) &\n",
    "    -\\sigma(\\mathbf{z})_{1} \\sigma(\\mathbf{z})_{2} &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{1} \\sigma(\\mathbf{z})_{Q} \\\\\n",
    "    -\\sigma(\\mathbf{z})_{2} \\sigma(\\mathbf{z})_{1} &\n",
    "    \\sigma(\\mathbf{z})_{2} (1 - \\sigma(\\mathbf{z})_2) &\n",
    "    \\cdots &\n",
    "    -\\sigma(\\mathbf{z})_{2} \\sigma(\\mathbf{z})_{Q} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -\\sigma(\\mathbf{z})_{Q} \\sigma(\\mathbf{z})_{1} &\n",
    "    -\\sigma(\\mathbf{z})_{Q} \\sigma(\\mathbf{z})_{2} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{z})_{Q} (1 - \\sigma(\\mathbf{z})_Q)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "or as vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}} =\n",
    "\\text{diag}(\\sigma(\\mathbf{z})) - \\sigma(\\mathbf{z}) \\sigma(\\mathbf{z})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.12703216e-01, -8.58746885e-05, -5.71188305e-02,\n",
       "        -1.55265079e-01, -2.33431605e-04],\n",
       "       [-8.58746885e-05,  2.79756608e-04, -5.20856315e-05,\n",
       "        -1.41583426e-04, -2.12862071e-07],\n",
       "       [-5.71188305e-02, -5.20856315e-05,  1.51485530e-01,\n",
       "        -9.41730309e-02, -1.41583426e-04],\n",
       "       [-1.55265079e-01, -1.41583426e-04, -9.41730309e-02,\n",
       "         2.49964557e-01, -3.84863653e-04],\n",
       "       [-2.33431605e-04, -2.12862071e-07, -1.41583426e-04,\n",
       "        -3.84863653e-04,  7.60091546e-04]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_soft_2(z):\n",
    "    soft = softmax_1(z)[0,:] # is necessary for Pytorch to work\n",
    "    return torch.diag(soft) - torch.outer(soft, soft)\n",
    "\n",
    "my_grad = my_der_soft_2(Z)\n",
    "my_grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.497936617049648e-15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(grad[0,:,0,:], my_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivative of multiple softmax respect to multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Z} \\in M \\times Q\n",
    "$$\n",
    "where $M$ is the number of examples. Then softmax function is:\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{Z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{Z}_{M,:})\n",
    "\\end{bmatrix} \\in M \\times Q\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{Z}_{i,:} = \\begin{bmatrix}\n",
    "    Z_{i1} & Z_{i2} & \\cdots & Z_{iQ}\n",
    "\\end{bmatrix} \\in 1 \\times Q\n",
    "$$\n",
    "for all $i = 1, ..., M$. Therefore\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}_{i,:}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{1} & \n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{2} & \n",
    "    \\cdots & \n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{Q}\n",
    "\\end{bmatrix} \\in 1 \\times Q\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del gradient, grad\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "Z = torch.randint(-20, 21, (M, Q)) / 2\n",
    "\n",
    "def der_soft_3(z):\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "gradient = jacobian(der_soft_3)\n",
    "grad = gradient(Z.numpy())\n",
    "print(grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\mathrm{d} {\\color{cyan}\\sigma(\\mathbf{Z})}}{\\mathrm{d} {\\color{Orange}\\mathbf{Z}}} \\in {\\color{cyan}(M \\times Q)} \\times {\\color{Orange}(M \\times Q)}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\mathrm{d} {\\color{cyan}\\sigma(\\mathbf{Z})_{pq}}}{\\mathrm{d} {\\color{Orange}\\mathbf{Z}_{ij}}} \\in {\\color{cyan}(1 \\times 1)} \\times {\\color{Orange}(1 \\times 1)}\n",
    "$$\n",
    "therefore, the last derivative is:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{pq}}{\\partial \\mathbf{Z}_{ij}} =\n",
    "\\begin{cases}\n",
    "    0 & \\text{ if } p\\neq i \\\\ \n",
    "    \\sigma(\\mathbf{Z})_{pq}(1 - \\sigma(\\mathbf{Z})_{ij}) & \\text{ if } p=i, q=j \\\\ \n",
    "    -\\sigma(\\mathbf{Z})_{pq} \\sigma(\\mathbf{Z})_{ij} & \\text{ if } p=i, q\\neq j \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p,i = 1, ..., M$ and $q,j = 1, .., Q$. The last 2 cases looks so similar to past derivative $\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial \\mathbf{z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5, 100, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_soft_3(z):\n",
    "    m, class_ = z.shape\n",
    "    s = softmax_2(z)\n",
    "    der = torch.zeros((m, class_, m, class_))\n",
    "    for p in range(m):\n",
    "        for q in range(class_):\n",
    "            for i in range(m):\n",
    "                for j in range(class_):\n",
    "                    if p != i: continue\n",
    "                    if q == j:\n",
    "                        der[p,q,i,j] = s[p,q] * (1 - s[p,q])\n",
    "                    else:\n",
    "                        der[p,q,i,j] = -s[p,q] * s[i,j]\n",
    "    return der\n",
    "\n",
    "my_grad = my_der_soft_3(Z)\n",
    "my_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3353362552065126e-14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 1e-100 to avoid division by zero\n",
    "error(grad + 1e-100, my_grad + 1e-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is too slow, its complexity is $O(M^2Q^2) \\approx O(n^4)$, but we can observe some similarities between this derivative and a past derivative, like:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i=p,:}}\n",
    "\\approx \\frac{\\partial \\sigma(\\mathbf{z})}{\\partial \\mathbf{z}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial {\\color{cyan}\\sigma(\\mathbf{Z})_{p,:}}}{\\partial {\\color{Orange}\\mathbf{Z}_{i,:}}} \\in {\\color{cyan}(1 \\times Q)} \\times {\\color{Orange}(1 \\times Q)}\n",
    "$$\n",
    "**Remark**: yes we need 1's axes. <br>\n",
    "Then we have 2 cases:\n",
    "1. $\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i=p,:}}$\n",
    "\n",
    "2. $\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i\\neq p,:}}$\n",
    "\n",
    "First case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i=p,:}} = \\text{diag}(\\sigma(\\mathbf{Z}_{p,:})) - \\sigma(\\mathbf{Z}_{p,:}) \\sigma(\\mathbf{Z}_{p,:})^T\n",
    "$$\n",
    "\n",
    "Second case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i\\neq p,:}} = \\mathbf{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5, 100, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_der_soft_4(z):\n",
    "    m, class_ = z.shape\n",
    "    der = torch.zeros((m,class_,m,class_))\n",
    "    for i in range(m):\n",
    "        der[i,:,i,:] = my_der_soft_2(z[None,i,:])\n",
    "    return der\n",
    "\n",
    "my_grad = my_der_soft_4(Z)\n",
    "my_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.468651931025567e-14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 1e-100 to avoid division by zero\n",
    "error(grad + 1e-100, my_grad + 1e-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5, 100, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorized form\n",
    "def my_der_soft_5(z):\n",
    "    m, n_classes = z.shape\n",
    "    soft = softmax_2(z)\n",
    "\n",
    "    diag_soft = torch.diag_embed(soft)\n",
    "    outer_soft = torch.einsum('ij,ik->ijk', soft, soft)\n",
    "    return (diag_soft - outer_soft).unsqueeze(2).expand(-1, -1, m, -1)\n",
    "\n",
    "my_grad = my_der_soft_5(Z)\n",
    "my_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.562481426712466e+100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(grad + 1e-100, my_grad + 1e-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma(\\mathbf{z})_{j} &= \\frac{\\exp(\\mathbf{z})_{j}}{\\sum_{k=1}^{Q}\\exp(z_k)} \\\\\n",
    "&= \\frac{\\exp(\\mathbf{z})_{j}}{\\exp(z_1) + ... + \\exp(z_j) + ... + \\exp(z_Q)}\n",
    "\\end{align*}\n",
    "$$\n",
    "then, when $i = j$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i=j}} &=\n",
    "\\frac{\\exp(\\mathbf{z})_{j}(\\sum_{k=1}^{Q}\\exp(z_k)) - \\exp(\\mathbf{z})_{j}^{2}}\n",
    "{(\\sum_{k=1}^{Q}\\exp(z_k))^{2}} \\\\\n",
    "&= \\frac{\\exp(\\mathbf{z})_{j}(\\sum_{k=1}^{Q}\\exp(z_k) - \\exp(\\mathbf{z})_{j})}\n",
    "{(\\sum_{k=1}^{Q}\\exp(z_k))^{2}} \\\\\n",
    "&= \\frac{\\exp(\\mathbf{z})_{j}}{\\sum_{k=1}^{Q}\\exp(z_k)}\n",
    "\\left( \n",
    "    \\frac{\\sum_{k=1}^{Q}\\exp(z_k) - \\exp(\\mathbf{z})_{j}}\n",
    "    {\\sum_{k=1}^{Q}\\exp(z_k)}\n",
    "\\right) \\\\\n",
    "&= \\sigma(\\mathbf{z})_j \\left(\n",
    "    \\frac{\\sum_{k=1}^{Q}\\exp(z_k)}{\\sum_{k=1}^{Q}\\exp(z_k)} -\n",
    "    \\frac{\\exp(\\mathbf{z})_{j}}{\\sum_{k=1}^{Q}\\exp(z_k)}\n",
    "\\right) \\\\\n",
    "&= \\sigma(\\mathbf{z})_j (1 - \\sigma(\\mathbf{z})_j)\n",
    "\\end{align*}\n",
    "$$\n",
    "when $i \\neq j$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sigma(\\mathbf{z})_{j}}{\\partial z_{i\\neq j}} &=\n",
    "- \\frac{\\exp(\\mathbf{z})_{j} \\exp(\\mathbf{z})_i}{(\\sum_{k=1}^{Q}\\exp(z_k))^{2}} \\\\\n",
    "&= - \\frac{\\exp(\\mathbf{z})_j}{\\sum_{k=1}^{Q}\\exp(z_k)} \\left(\n",
    "    \\frac{\\exp(\\mathbf{z})_i}{\\sum_{k=1}^{Q}\\exp(z_k)}\n",
    "\\right) \\\\\n",
    "&= - \\sigma(\\mathbf{z})_j \\sigma(\\mathbf{z})_i\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
