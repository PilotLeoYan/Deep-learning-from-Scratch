{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "tf.keras.config.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "M: int = 100\n",
    "N: int = 5\n",
    "N_CLASS: int = 3\n",
    "\n",
    "X, Y = make_classification(n_samples=M, n_features=N, n_classes=N_CLASS,\n",
    "                           n_informative=N-1, n_redundant=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = torch.nn.functional.one_hot(torch.tensor(Y).long(), 3)\n",
    "Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85, 5]) torch.Size([85, 3])\n",
      "torch.Size([15, 5]) torch.Size([15, 3])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid = torch.tensor(X[:85]), torch.tensor(X[85:])\n",
    "Y_train, Y_valid = Y_hat[:85], Y_hat[85:]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainables parameters\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W} &\\in \\mathbb{R}^{N \\times Q} \\\\\n",
    "\\mathbf{b} &\\in \\mathbb{R}^{Q}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $N$ is the number of features and $Q$ is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self, n_features: int, n_classes: int):\n",
    "        self.w = torch.randn(n_features, n_classes)\n",
    "        self.b = torch.randn(n_classes)\n",
    "\n",
    "    def copy_params(self, tf_model) -> None:\n",
    "        \"\"\"Copy the parameters from a TensorFlow model to this PyTorch model.\n",
    "\n",
    "        Args:\n",
    "            tf_model: A TensorFlow model from which to copy the parameters.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.w.copy_(torch.tensor(tf_model.weights[0].numpy()[:,0]))\n",
    "        self.b.copy_(torch.tensor(tf_model.weights[1].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum and softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum\n",
    "$$\n",
    "\\mathbf{Z}(\\mathbf{X}) = \\mathbf{X} \\mathbf{W} + \\mathbf{b} \\\\\n",
    "\\mathbf{Z} : \\mathbb{R}^{M \\times N} \\rightarrow \\mathbb{R}^{M \\times Q}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax function\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}_{i,:})_{j} = \\frac{\\exp(\\mathbf{Z}_{i,:})_{j}}\n",
    "{\\sum_{k=1}^{Q}(\\exp(\\mathbf{Z}_{i,:})_{k})}\n",
    "$$\n",
    "then:\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}_{i,:}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{1} &\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{2} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{Z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{Z}_{M,:})\n",
    "\\end{bmatrix} \\\\\n",
    "\\sigma(\\mathbf{Z}) : \\mathbb{R}^{M \\times Q} \\rightarrow \\mathbb{R}^{M \\times Q}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # weighted sum\n",
    "    z = torch.matmul(x, self.w) + self.b\n",
    "    # softmax function\n",
    "    z_exp = torch.exp(z)\n",
    "    y_pred = z_exp / z_exp.sum(1, keepdims=True)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function: Cross Entropy-loss:\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum_{i=1}^{M} \\sum_{k=1}^{Q}(\n",
    "    Y_{ik} \\log(\\hat{Y}_{ik})\n",
    ") \\\\\n",
    "L(\\mathbf{\\hat{Y}}) : \\mathbb{R}^{M \\times Q} \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "**Remark**: for this case $\\mathbf{\\hat{Y}}$ is $\\sigma(\\mathbf{Z})$. It is not obligatory to use softmax for CE.<br>\n",
    "Vectorized form:\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum_{i=1}^{M} \\left(\n",
    "    \\mathbf{y}_{i,:}^T \\log(\\mathbf{\\hat{y}}_{i,:})\n",
    "\\right)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum \\left(\n",
    "    \\mathbf{Y} \\odot \\log(\\mathbf{\\hat{Y}})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = self.predict(x)\n",
    "    loss = y_true * torch.log(y_pred)\n",
    "    return - loss.sum().item() / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} =\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}}\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}}\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}\n",
    "$$\n",
    "where their shapes are:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{N \\times Q} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{Q} \\\\\n",
    "\\frac{\\partial L}{\\partial \\sigma} &\\in \\mathbb{R}^{M \\times Q} \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}} &\\in \\mathbb{R}^{(M \\times Q) \\times (M \\times Q)} \\\\\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{(M \\times Q) \\times (N \\times Q)} \\\\\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{(M \\times Q) \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}$\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1N} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2N} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{M1} & x_{M2} & \\cdots & x_{MN}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\\\\n",
    "    \\mathbf{x}_{2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{W} = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1Q} \\\\\n",
    "    w_{21} & w_{22} & \\cdots & w_{2Q} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{N1} & w_{N2} & \\cdots & w_{NQ} \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\mathbf{w}_{1} &\n",
    "    \\mathbf{w}_{2} &\n",
    "    \\cdots &\n",
    "    \\mathbf{w}_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\mathbf{x}_{p}^T = \\begin{bmatrix} x_{p1} & x_{p2} & \\cdots & x_{pN} \\end{bmatrix}$ \n",
    "and $\\mathbf{w}_{q} = \\begin{bmatrix} w_{1q} & w_{2q} & \\cdots & w_{Nq} \\end{bmatrix}^T$\n",
    ", then\n",
    "$$\n",
    "\\mathbf{XW} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\mathbf{w}_{1} & \\mathbf{x}_{1}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{1}^T \\mathbf{w}_{Q} \\\\\n",
    "    \\mathbf{x}_{2}^T \\mathbf{w}_{1} & \\mathbf{x}_{2}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{2}^T \\mathbf{w}_{Q} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T \\mathbf{w}_{1} & \\mathbf{x}_{M}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{M}^T \\mathbf{w}_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{XW}_{pq}}{\\partial w_{ij}} = \n",
    "\\frac{\\partial \\mathbf{x}_{p}^T\\mathbf{w}_{q}}{\\partial w_{ij}} = \\begin{cases}\n",
    "    x_{pi} & \\text{ if } q=j \\\\ \n",
    "    0 & \\text{ if } q\\neq j \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p = 1, ..., M$, $q, j = 1, ..., Q$ and $i = 1, ..., N$. <br>\n",
    "Vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} = \n",
    "\\mathbb{I} \\otimes \\mathbf{X}\n",
    "$$\n",
    "where $\\otimes$ is Kronecker product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}$\n",
    "$$\n",
    "\\mathbf{XW + b} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{1}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{1}^T \\mathbf{w}_{Q}+b_Q \\\\\n",
    "    \\mathbf{x}_{2}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{2}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{2}^T \\mathbf{w}_{Q}+b_Q  \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{M}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{M}^T \\mathbf{w}_{Q}+b_Q \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{(XW+b)}_{pq}}{\\partial b_{i}} = \n",
    "\\frac{\\partial \\mathbf{x}_{p}^T\\mathbf{w}_{q} + b_{q}}{\\partial b_{i}} = \\begin{cases}\n",
    "    1 & \\text{ if } q=i \\\\ \n",
    "    0 & \\text{ if } q\\neq i\n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p = 1, ..., M$ and $q,i = 1, ..., Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax derivative summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i=p,:}} = \\text{diag}(\\sigma(\\mathbf{Z}_{p,:})) - \\sigma(\\mathbf{Z}_{p,:}) \\sigma(\\mathbf{Z}_{p,:})^T\n",
    "$$\n",
    "\n",
    "Second case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i\\neq p,:}} = \\mathbf{0}\n",
    "$$\n",
    "Please check [Softmax Function and Gradient](softmax_function_and_gradient.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-entropy derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this CE:\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\hat{Y}}) &= - \\frac{1}{M} \\sum_{i=1}^{M} \\left(\n",
    "    \\mathbf{y}_{i,:}^T \\log(\\mathbf{\\hat{y}}_{i,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\left(\n",
    "    \\mathbf{y}_{1,:}^T \\log(\\mathbf{\\hat{y}}_{1,:}) +\n",
    "    \\mathbf{y}_{2,:}^T \\log(\\mathbf{\\hat{y}}_{2,:}) +\n",
    "    ... + \n",
    "    \\mathbf{y}_{M,:}^T \\log(\\mathbf{\\hat{y}}_{M,:})\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p,:}} &= \\begin{bmatrix}\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p1}} & \n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{pQ}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{1 \\times Q} \\\\\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{Y}}} &= \\begin{bmatrix}\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{1,:}} \\\\\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{2,:}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{M,:}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{M \\times Q}\n",
    "\\end{align*}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p,:}} &=\n",
    "-\\frac{1}{M} \\frac{\\partial}{\\partial \\mathbf{\\hat{y}}_{p,:}} \\left(\n",
    "    \\mathbf{y}_{1,:}^T \\log(\\mathbf{\\hat{y}}_{1,:}) +\n",
    "    ... +\n",
    "    \\mathbf{y}_{p,:}^T \\log(\\mathbf{\\hat{y}}_{p,:}) +\n",
    "    ... + \n",
    "    \\mathbf{y}_{M,:}^T \\log(\\mathbf{\\hat{y}}_{M,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\frac{\\partial}{\\partial \\mathbf{\\hat{y}}_{p,:}} \\left(\n",
    "    \\mathbf{y}_{p,:}^T \\log(\\mathbf{\\hat{y}}_{p,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\left(\n",
    "    \\mathbf{y}_{p,:} \\oslash \\mathbf{\\hat{y}}_{p,:}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\oslash$ is element-wise divide. <br>\n",
    "Therefore\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{Y}}} &= -\\frac{1}{M} \\begin{bmatrix}\n",
    "    \\mathbf{y}_{1,:} \\oslash \\mathbf{\\hat{y}}_{1,:} \\\\\n",
    "    \\mathbf{y}_{2,:} \\oslash \\mathbf{\\hat{y}}_{2,:} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{y}_{M,:} \\oslash \\mathbf{\\hat{y}}_{M,:}\n",
    "\\end{bmatrix} \\\\\n",
    "&= -\\frac{1}{M} \\mathbf{Y} \\oslash \\mathbf{\\hat{Y}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pull it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} &=\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Cyan} (M \\times Q)} \\times ({\\color{Cyan} M \\times Q} \\times {\\color{Orange} M \\times Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Orange} M \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial L}{\\partial \\mathbf{W}}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &=\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} \n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Orange} (M \\times Q)} \\times ({\\color{Orange} M \\times Q} \\times {\\color{Magenta} N \\times Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} N \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial L}{\\partial \\mathbf{b}}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &=\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} \n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Orange} (M \\times Q)} \\times ({\\color{Orange} M \\times Q} \\times {\\color{Magenta} Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor,\n",
    "           y_pred: torch.Tensor, lr: float) -> None:\n",
    "    m, n = x.shape\n",
    "    m, n_classes = y_true.shape\n",
    "\n",
    "    \n",
    "\n",
    "    # weighted sum\n",
    "    identity = torch.eye(n_classes)\n",
    "    ## weight grad\n",
    "    torch.kron(x.unsqueeze(1).unsqueeze(3), identity.unsqueeze(0).unsqueeze(2))\n",
    "    ## bias grad\n",
    "    identity.unsqueeze(0).expand(m, -1, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
