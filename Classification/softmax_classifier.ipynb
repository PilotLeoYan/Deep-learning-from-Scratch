{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "tf.keras.config.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "M: int = 100\n",
    "N: int = 5\n",
    "N_CLASS: int = 3\n",
    "\n",
    "X, Y = make_classification(n_samples=M, n_features=N, n_classes=N_CLASS,\n",
    "                           n_informative=N-1, n_redundant=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = torch.nn.functional.one_hot(torch.tensor(Y).long(), 3)\n",
    "Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85, 5]) torch.Size([85, 3])\n",
      "torch.Size([15, 5]) torch.Size([15, 3])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid = torch.tensor(X[:85]), torch.tensor(X[85:])\n",
    "Y_train, Y_valid = Y_hat[:85], Y_hat[85:]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainables parameters\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W} &\\in \\mathbb{R}^{N \\times Q} \\\\\n",
    "\\mathbf{b} &\\in \\mathbb{R}^{Q}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $N$ is the number of features and $Q$ is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self, n_features: int, n_classes: int):\n",
    "        self.w = torch.randn(n_features, n_classes)\n",
    "        self.b = torch.randn(n_classes)\n",
    "\n",
    "    def copy_params(self, tf_model) -> None:\n",
    "        \"\"\"Copy the parameters from a TensorFlow model to this PyTorch model.\n",
    "\n",
    "        Args:\n",
    "            tf_model: A TensorFlow model from which to copy the parameters.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.w.copy_(torch.tensor(tf_model.weights[0].numpy()))\n",
    "        self.b.copy_(torch.tensor(tf_model.weights[1].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum and softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum\n",
    "$$\n",
    "\\mathbf{Z}(\\mathbf{X}) = \\mathbf{X} \\mathbf{W} + \\mathbf{b} \\\\\n",
    "\\mathbf{Z} : \\mathbb{R}^{M \\times N} \\rightarrow \\mathbb{R}^{M \\times Q}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax function\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}_{i,:})_{j} = \\frac{\\exp(\\mathbf{Z}_{i,:})_{j}}\n",
    "{\\sum_{k=1}^{Q}(\\exp(\\mathbf{Z}_{i,:})_{k})}\n",
    "$$\n",
    "then:\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}_{i,:}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{1} &\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{2} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{Z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{Z}_{M,:})\n",
    "\\end{bmatrix} \\\\\n",
    "\\sigma(\\mathbf{Z}) : \\mathbb{R}^{M \\times Q} \\rightarrow \\mathbb{R}^{M \\times Q}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # weighted sum\n",
    "    z = torch.matmul(x, self.w) + self.b\n",
    "    # softmax function\n",
    "    z_exp = torch.exp(z)\n",
    "    y_pred = z_exp / z_exp.sum(1, keepdims=True)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function: Cross Entropy-loss:\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum_{i=1}^{M} \\sum_{k=1}^{Q}(\n",
    "    Y_{ik} \\log(\\hat{Y}_{ik})\n",
    ") \\\\\n",
    "L(\\mathbf{\\hat{Y}}) : \\mathbb{R}^{M \\times Q} \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "**Remark**: for this case $\\mathbf{\\hat{Y}}$ is $\\sigma(\\mathbf{Z})$. It is not obligatory to use softmax for CE.<br>\n",
    "Vectorized form:\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum_{i=1}^{M} \\left(\n",
    "    \\mathbf{y}_{i,:}^T \\log(\\mathbf{\\hat{y}}_{i,:})\n",
    "\\right)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum \\left(\n",
    "    \\mathbf{Y} \\odot \\log(\\mathbf{\\hat{Y}})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = self.predict(x)\n",
    "    loss = y_true * torch.log(y_pred)\n",
    "    return - loss.sum().item() / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} =\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}}\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}}\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}\n",
    "$$\n",
    "where their shapes are:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{N \\times Q} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{Q} \\\\\n",
    "\\frac{\\partial L}{\\partial \\sigma} &\\in \\mathbb{R}^{M \\times Q} \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}} &\\in \\mathbb{R}^{(M \\times Q) \\times (M \\times Q)} \\\\\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{(M \\times Q) \\times (N \\times Q)} \\\\\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{(M \\times Q) \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}$\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1N} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2N} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{M1} & x_{M2} & \\cdots & x_{MN}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\\\\n",
    "    \\mathbf{x}_{2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{W} = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1Q} \\\\\n",
    "    w_{21} & w_{22} & \\cdots & w_{2Q} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{N1} & w_{N2} & \\cdots & w_{NQ} \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\mathbf{w}_{1} &\n",
    "    \\mathbf{w}_{2} &\n",
    "    \\cdots &\n",
    "    \\mathbf{w}_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\mathbf{x}_{p}^T = \\begin{bmatrix} x_{p1} & x_{p2} & \\cdots & x_{pN} \\end{bmatrix}$ \n",
    "and $\\mathbf{w}_{q} = \\begin{bmatrix} w_{1q} & w_{2q} & \\cdots & w_{Nq} \\end{bmatrix}^T$\n",
    ", then\n",
    "$$\n",
    "\\mathbf{XW} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\mathbf{w}_{1} & \\mathbf{x}_{1}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{1}^T \\mathbf{w}_{Q} \\\\\n",
    "    \\mathbf{x}_{2}^T \\mathbf{w}_{1} & \\mathbf{x}_{2}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{2}^T \\mathbf{w}_{Q} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T \\mathbf{w}_{1} & \\mathbf{x}_{M}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{M}^T \\mathbf{w}_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{XW}_{pq}}{\\partial w_{ij}} = \n",
    "\\frac{\\partial \\mathbf{x}_{p}^T\\mathbf{w}_{q}}{\\partial w_{ij}} = \\begin{cases}\n",
    "    x_{pi} & \\text{ if } q=j \\\\ \n",
    "    0 & \\text{ if } q\\neq j \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p = 1, ..., M$, $q, j = 1, ..., Q$ and $i = 1, ..., N$. <br>\n",
    "Vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} = \n",
    "\\mathbb{I} \\otimes \\mathbf{X}\n",
    "$$\n",
    "where $\\otimes$ is Kronecker product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}$\n",
    "$$\n",
    "\\mathbf{XW + b} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{1}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{1}^T \\mathbf{w}_{Q}+b_Q \\\\\n",
    "    \\mathbf{x}_{2}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{2}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{2}^T \\mathbf{w}_{Q}+b_Q  \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{M}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{M}^T \\mathbf{w}_{Q}+b_Q \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{(XW+b)}_{pq}}{\\partial b_{i}} = \n",
    "\\frac{\\partial \\mathbf{x}_{p}^T\\mathbf{w}_{q} + b_{q}}{\\partial b_{i}} = \\begin{cases}\n",
    "    1 & \\text{ if } q=i \\\\ \n",
    "    0 & \\text{ if } q\\neq i\n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p = 1, ..., M$ and $q,i = 1, ..., Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax derivative summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i=p,:}} = \\text{diag}(\\sigma(\\mathbf{Z}_{p,:})) - \\sigma(\\mathbf{Z}_{p,:}) \\sigma(\\mathbf{Z}_{p,:})^T\n",
    "$$\n",
    "\n",
    "Second case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i\\neq p,:}} = \\mathbf{0}\n",
    "$$\n",
    "Please check [Softmax Function and Gradient](softmax_function_and_gradient.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-entropy derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this CE:\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\hat{Y}}) &= - \\frac{1}{M} \\sum_{i=1}^{M} \\left(\n",
    "    \\mathbf{y}_{i,:}^T \\log(\\mathbf{\\hat{y}}_{i,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\left(\n",
    "    \\mathbf{y}_{1,:}^T \\log(\\mathbf{\\hat{y}}_{1,:}) +\n",
    "    \\mathbf{y}_{2,:}^T \\log(\\mathbf{\\hat{y}}_{2,:}) +\n",
    "    ... + \n",
    "    \\mathbf{y}_{M,:}^T \\log(\\mathbf{\\hat{y}}_{M,:})\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p,:}} &= \\begin{bmatrix}\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p1}} & \n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{pQ}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{1 \\times Q} \\\\\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{Y}}} &= \\begin{bmatrix}\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{1,:}} \\\\\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{2,:}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{M,:}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{M \\times Q}\n",
    "\\end{align*}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p,:}} &=\n",
    "-\\frac{1}{M} \\frac{\\partial}{\\partial \\mathbf{\\hat{y}}_{p,:}} \\left(\n",
    "    \\mathbf{y}_{1,:}^T \\log(\\mathbf{\\hat{y}}_{1,:}) +\n",
    "    ... +\n",
    "    \\mathbf{y}_{p,:}^T \\log(\\mathbf{\\hat{y}}_{p,:}) +\n",
    "    ... + \n",
    "    \\mathbf{y}_{M,:}^T \\log(\\mathbf{\\hat{y}}_{M,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\frac{\\partial}{\\partial \\mathbf{\\hat{y}}_{p,:}} \\left(\n",
    "    \\mathbf{y}_{p,:}^T \\log(\\mathbf{\\hat{y}}_{p,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\left(\n",
    "    \\mathbf{y}_{p,:} \\oslash \\mathbf{\\hat{y}}_{p,:}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\oslash$ is element-wise divide. <br>\n",
    "Therefore\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{Y}}} &= -\\frac{1}{M} \\begin{bmatrix}\n",
    "    \\mathbf{y}_{1,:} \\oslash \\mathbf{\\hat{y}}_{1,:} \\\\\n",
    "    \\mathbf{y}_{2,:} \\oslash \\mathbf{\\hat{y}}_{2,:} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{y}_{M,:} \\oslash \\mathbf{\\hat{y}}_{M,:}\n",
    "\\end{bmatrix} \\\\\n",
    "&= -\\frac{1}{M} \\mathbf{Y} \\oslash \\mathbf{\\hat{Y}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pull it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} &=\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Cyan} (M \\times Q)} \\times ({\\color{Cyan} M \\times Q} \\times {\\color{Orange} M \\times Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Orange} M \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial L}{\\partial \\mathbf{W}}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &=\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} \n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Orange} (M \\times Q)} \\times ({\\color{Orange} M \\times Q} \\times {\\color{Magenta} N \\times Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} N \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial L}{\\partial \\mathbf{b}}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &=\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} \n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Orange} (M \\times Q)} \\times ({\\color{Orange} M \\times Q} \\times {\\color{Magenta} Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "We can simply compute:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \n",
    "\\frac{1}{M} \\mathbf{X}^T \\left(\n",
    "    \\sigma - \\mathbf{Y}\n",
    "\\right)\n",
    "$$\n",
    "but the objective of this repository is to understand piece by piece and not simply give things for done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor,\n",
    "           y_pred: torch.Tensor, lr: float) -> None:\n",
    "    m, n = x.shape\n",
    "    m, n_classes = y_true.shape\n",
    "\n",
    "    # cross entropy der\n",
    "    delta = -(y_true / y_pred) / m\n",
    "\n",
    "    # softmax der\n",
    "    diag_a = torch.diag_embed(y_pred)\n",
    "    outer_a = torch.einsum('ij,ik->ijk', y_pred, y_pred)\n",
    "        \n",
    "    sus_a = diag_a - outer_a\n",
    "        \n",
    "    soft_der = torch.zeros((m, n_classes, m, n_classes), dtype=y_pred.dtype)\n",
    "    idx = torch.arange(m)\n",
    "    soft_der[idx, :, idx, :] = sus_a\n",
    "    delta = torch.einsum('pq,pqij->ij', delta, soft_der)\n",
    "\n",
    "    # weighted sum der\n",
    "    identity = torch.eye(n_classes)\n",
    "\n",
    "    ## weight der\n",
    "    w_der = torch.kron(x.unsqueeze(1).unsqueeze(3), identity.unsqueeze(0).unsqueeze(2))\n",
    "    w_der = torch.einsum('pq,pqij->ij', delta, w_der)\n",
    "    self.w -= lr * w_der\n",
    "\n",
    "    ## bias der\n",
    "    self.b -= lr * delta.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def accuracy(self, y_true, y_pred):\n",
    "    preds = y_pred.argmax(axis=-1)\n",
    "    compare = (y_true.argmax(axis=-1) == preds).type(torch.float32)\n",
    "    return compare.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def fit(self, x_train: torch.Tensor, y_train: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor) -> None:\n",
    "    \"\"\"fit the model using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        x_train: Input tensor of shape (n_samples, num_features).\n",
    "        y_train: Target tensor one hot of shape (n_samples, n_classes).\n",
    "        epochs: Number of epochs to train.\n",
    "        lr: learning rate (0, 1).\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples, num_features).\n",
    "        y_valid: Input tensor one hot of shape (n_valid_samples, n_valid_classes).\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        num_batch = 0\n",
    "        for batch in range(0, len(y_train), batch_size):\n",
    "            num_batch += 1\n",
    "            x_b = x_train[batch:batch+batch_size]\n",
    "            y_b = y_train[batch:batch+batch_size]\n",
    "\n",
    "            y_pred = self.predict(x_b)\n",
    "            loss += self.evaluate(x_b, y_b)\n",
    "\n",
    "            self.update(x_b, y_b, y_pred, lr)\n",
    "\n",
    "        loss = round(loss / num_batch, 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        acc = round(self.accuracy(y_valid, self.predict(x_valid)), 4)\n",
    "        print(f'epoch: {epoch} - MSE: {loss} - MSE_v: {loss_v} - acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCHS = 16\n",
    "BATCH = len(X_train) // 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 0.5870   \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m18\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> (144.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18\u001b[0m (144.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> (144.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18\u001b[0m (144.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TFModel = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=N_CLASS, activation='softmax')\n",
    "])\n",
    "\n",
    "TFModel.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=LR),\n",
    "    metrics = [tf.keras.metrics.Accuracy()]\n",
    ")\n",
    "\n",
    "TFModel.evaluate(X_train[:1], Y_train[:1])\n",
    "\n",
    "TFModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifier(N, N_CLASS)\n",
    "model.copy_params(TFModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(tensor_true, tensor_pred) -> float:\n",
    "    \"\"\"\n",
    "     Calculates the percentage error between two tensors or floats.\n",
    "\n",
    "     If the arguments are simple floats or ints, calculate the percentage error between them.\n",
    "     If the arguments are Numpy ndarray and PyTorch tensor, calculate the percentage error between them.\n",
    "     If the argumens are PyTorch tensors, calculate the percentage error between them.\n",
    "\n",
    "     Args:\n",
    "         tensor_true: The true tensor or true float.\n",
    "         pred_tensor: The predicted tensor or the predicted float.\n",
    "\n",
    "     Returns:\n",
    "         The percentage error between the tensors or floats.\n",
    "     \"\"\"\n",
    "    if isinstance(tensor_true, (float, int)) and isinstance(tensor_pred, (float, int)):\n",
    "        return np.abs(tensor_true - tensor_pred) / np.abs(tensor_true) * 100\n",
    "    elif type(tensor_true) is np.ndarray and type(tensor_pred) is torch.Tensor:\n",
    "        e = np.abs(tensor_true - tensor_pred.numpy()) / np.abs(tensor_true)\n",
    "        return np.mean(e) * 100\n",
    "    e = torch.abs(tensor_true - tensor_pred) / torch.abs(tensor_true)\n",
    "    return torch.mean(e) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3897221166873968e-14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.predict(X_train, batch_size=len(X_train))\n",
    "predict = model.predict(X_train)\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0000e+00 - loss: 1.4981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4821332404403658e-14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.evaluate(X_train, Y_train, batch_size=len(X_train))[0]\n",
    "predict = model.evaluate(X_train, Y_train)\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: 1.4361 - val_accuracy: 0.0000e+00 - val_loss: 2.5554\n",
      "Epoch 2/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 1.4017 - val_accuracy: 0.0000e+00 - val_loss: 2.5119\n",
      "Epoch 3/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 1.3695 - val_accuracy: 0.0000e+00 - val_loss: 2.4709\n",
      "Epoch 4/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 1.3394 - val_accuracy: 0.0000e+00 - val_loss: 2.4325\n",
      "Epoch 5/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.3114 - val_accuracy: 0.0000e+00 - val_loss: 2.3964\n",
      "Epoch 6/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 1.2852 - val_accuracy: 0.0000e+00 - val_loss: 2.3625\n",
      "Epoch 7/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 1.2609 - val_accuracy: 0.0000e+00 - val_loss: 2.3308\n",
      "Epoch 8/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 1.2382 - val_accuracy: 0.0000e+00 - val_loss: 2.3009\n",
      "Epoch 9/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 1.2171 - val_accuracy: 0.0000e+00 - val_loss: 2.2728\n",
      "Epoch 10/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 1.1975 - val_accuracy: 0.0000e+00 - val_loss: 2.2464\n",
      "Epoch 11/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 1.1792 - val_accuracy: 0.0000e+00 - val_loss: 2.2214\n",
      "Epoch 12/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 1.1621 - val_accuracy: 0.0000e+00 - val_loss: 2.1978\n",
      "Epoch 13/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.1462 - val_accuracy: 0.0000e+00 - val_loss: 2.1754\n",
      "Epoch 14/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 0.0000e+00 - loss: 1.1313 - val_accuracy: 0.0000e+00 - val_loss: 2.1542\n",
      "Epoch 15/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 1.1173 - val_accuracy: 0.0000e+00 - val_loss: 2.1341\n",
      "Epoch 16/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 1.1043 - val_accuracy: 0.0000e+00 - val_loss: 2.1148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1be4d6e0210>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFModel.fit(X_train, Y_train, batch_size=BATCH, epochs=EPOCHS,\n",
    "            shuffle=False, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5113473177242127,\n",
       " 1.4588145850299226,\n",
       " 1.4099029469181386,\n",
       " 1.3645286729260422,\n",
       " 1.3225660314128795,\n",
       " 1.2838546242105078,\n",
       " 1.2482081432709895,\n",
       " 1.2154234432149613,\n",
       " 1.1852890086294243,\n",
       " 1.157592202953437,\n",
       " 1.132125010540958,\n",
       " 1.1086882488277352,\n",
       " 1.0870944005342684,\n",
       " 1.0671692996326831,\n",
       " 1.0487529223319147,\n",
       " 1.0316995130502224]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFModel.history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 1.5113 - MSE_v: 2.5554 - acc: 0.2\n",
      "epoch: 1 - MSE: 1.4588 - MSE_v: 2.5119 - acc: 0.2\n",
      "epoch: 2 - MSE: 1.4099 - MSE_v: 2.4709 - acc: 0.2\n",
      "epoch: 3 - MSE: 1.3645 - MSE_v: 2.4325 - acc: 0.2\n",
      "epoch: 4 - MSE: 1.3226 - MSE_v: 2.3964 - acc: 0.3333\n",
      "epoch: 5 - MSE: 1.2839 - MSE_v: 2.3625 - acc: 0.3333\n",
      "epoch: 6 - MSE: 1.2482 - MSE_v: 2.3308 - acc: 0.3333\n",
      "epoch: 7 - MSE: 1.2154 - MSE_v: 2.3009 - acc: 0.3333\n",
      "epoch: 8 - MSE: 1.1853 - MSE_v: 2.2728 - acc: 0.3333\n",
      "epoch: 9 - MSE: 1.1576 - MSE_v: 2.2464 - acc: 0.3333\n",
      "epoch: 10 - MSE: 1.1321 - MSE_v: 2.2214 - acc: 0.3333\n",
      "epoch: 11 - MSE: 1.1087 - MSE_v: 2.1978 - acc: 0.3333\n",
      "epoch: 12 - MSE: 1.0871 - MSE_v: 2.1754 - acc: 0.3333\n",
      "epoch: 13 - MSE: 1.0672 - MSE_v: 2.1542 - acc: 0.3333\n",
      "epoch: 14 - MSE: 1.0488 - MSE_v: 2.1341 - acc: 0.3333\n",
      "epoch: 15 - MSE: 1.0317 - MSE_v: 2.1148 - acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, EPOCHS, LR, BATCH, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7911208798267837e-13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.weights[0].numpy()\n",
    "predict = model.w\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.522590863153646e-14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.weights[1].numpy()\n",
    "predict = model.b\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2648, -1.0024, -0.5563])\n",
      "tensor([[-0.0956,  0.9671, -1.0026],\n",
      "        [-0.8430,  0.7695, -0.9093],\n",
      "        [-0.3887, -2.2423,  1.4185],\n",
      "        [ 0.7278, -0.4257, -0.2401],\n",
      "        [ 0.7755, -0.8383, -0.7018]])\n"
     ]
    }
   ],
   "source": [
    "model2 = SoftmaxClassifier(N, N_CLASS)\n",
    "\n",
    "print(model2.b)\n",
    "print(model2.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.505723176395554"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 4.4024 - MSE_v: 5.3508 - acc: 0.2\n",
      "epoch: 1 - MSE: 4.2623 - MSE_v: 5.1971 - acc: 0.2\n",
      "epoch: 2 - MSE: 4.1249 - MSE_v: 5.0448 - acc: 0.2\n",
      "epoch: 3 - MSE: 3.9904 - MSE_v: 4.8941 - acc: 0.2\n",
      "epoch: 4 - MSE: 3.8588 - MSE_v: 4.745 - acc: 0.2\n",
      "epoch: 5 - MSE: 3.7305 - MSE_v: 4.5978 - acc: 0.2\n",
      "epoch: 6 - MSE: 3.6054 - MSE_v: 4.4527 - acc: 0.2\n",
      "epoch: 7 - MSE: 3.4837 - MSE_v: 4.3099 - acc: 0.2\n",
      "epoch: 8 - MSE: 3.3655 - MSE_v: 4.1696 - acc: 0.2\n",
      "epoch: 9 - MSE: 3.2508 - MSE_v: 4.032 - acc: 0.2\n",
      "epoch: 10 - MSE: 3.1397 - MSE_v: 3.8974 - acc: 0.2\n",
      "epoch: 11 - MSE: 3.0321 - MSE_v: 3.7659 - acc: 0.2\n",
      "epoch: 12 - MSE: 2.9281 - MSE_v: 3.6377 - acc: 0.2\n",
      "epoch: 13 - MSE: 2.8275 - MSE_v: 3.513 - acc: 0.2\n",
      "epoch: 14 - MSE: 2.7304 - MSE_v: 3.3919 - acc: 0.2\n",
      "epoch: 15 - MSE: 2.6367 - MSE_v: 3.2743 - acc: 0.2\n",
      "epoch: 16 - MSE: 2.5462 - MSE_v: 3.1605 - acc: 0.1333\n",
      "epoch: 17 - MSE: 2.459 - MSE_v: 3.0505 - acc: 0.1333\n",
      "epoch: 18 - MSE: 2.375 - MSE_v: 2.9441 - acc: 0.1333\n",
      "epoch: 19 - MSE: 2.294 - MSE_v: 2.8416 - acc: 0.1333\n",
      "epoch: 20 - MSE: 2.2161 - MSE_v: 2.7427 - acc: 0.1333\n",
      "epoch: 21 - MSE: 2.1411 - MSE_v: 2.6475 - acc: 0.1333\n",
      "epoch: 22 - MSE: 2.0689 - MSE_v: 2.5561 - acc: 0.1333\n",
      "epoch: 23 - MSE: 1.9996 - MSE_v: 2.4682 - acc: 0.2\n",
      "epoch: 24 - MSE: 1.9329 - MSE_v: 2.384 - acc: 0.2\n",
      "epoch: 25 - MSE: 1.8689 - MSE_v: 2.3034 - acc: 0.2667\n",
      "epoch: 26 - MSE: 1.8075 - MSE_v: 2.2265 - acc: 0.2667\n",
      "epoch: 27 - MSE: 1.7486 - MSE_v: 2.1531 - acc: 0.2667\n",
      "epoch: 28 - MSE: 1.6922 - MSE_v: 2.0833 - acc: 0.3333\n",
      "epoch: 29 - MSE: 1.6381 - MSE_v: 2.0171 - acc: 0.3333\n",
      "epoch: 30 - MSE: 1.5864 - MSE_v: 1.9546 - acc: 0.3333\n",
      "epoch: 31 - MSE: 1.537 - MSE_v: 1.8956 - acc: 0.3333\n",
      "epoch: 32 - MSE: 1.4899 - MSE_v: 1.8403 - acc: 0.3333\n",
      "epoch: 33 - MSE: 1.445 - MSE_v: 1.7884 - acc: 0.3333\n",
      "epoch: 34 - MSE: 1.4023 - MSE_v: 1.7401 - acc: 0.3333\n",
      "epoch: 35 - MSE: 1.3617 - MSE_v: 1.6952 - acc: 0.3333\n",
      "epoch: 36 - MSE: 1.3232 - MSE_v: 1.6536 - acc: 0.3333\n",
      "epoch: 37 - MSE: 1.2869 - MSE_v: 1.6151 - acc: 0.2667\n",
      "epoch: 38 - MSE: 1.2526 - MSE_v: 1.5798 - acc: 0.2667\n",
      "epoch: 39 - MSE: 1.2202 - MSE_v: 1.5473 - acc: 0.3333\n",
      "epoch: 40 - MSE: 1.1898 - MSE_v: 1.5176 - acc: 0.3333\n",
      "epoch: 41 - MSE: 1.1613 - MSE_v: 1.4904 - acc: 0.3333\n",
      "epoch: 42 - MSE: 1.1345 - MSE_v: 1.4656 - acc: 0.3333\n",
      "epoch: 43 - MSE: 1.1095 - MSE_v: 1.443 - acc: 0.3333\n",
      "epoch: 44 - MSE: 1.0862 - MSE_v: 1.4224 - acc: 0.3333\n",
      "epoch: 45 - MSE: 1.0644 - MSE_v: 1.4038 - acc: 0.3333\n",
      "epoch: 46 - MSE: 1.0441 - MSE_v: 1.3868 - acc: 0.3333\n",
      "epoch: 47 - MSE: 1.0252 - MSE_v: 1.3714 - acc: 0.3333\n",
      "epoch: 48 - MSE: 1.0077 - MSE_v: 1.3574 - acc: 0.3333\n",
      "epoch: 49 - MSE: 0.9914 - MSE_v: 1.3448 - acc: 0.3333\n",
      "epoch: 50 - MSE: 0.9762 - MSE_v: 1.3333 - acc: 0.3333\n",
      "epoch: 51 - MSE: 0.9622 - MSE_v: 1.323 - acc: 0.3333\n",
      "epoch: 52 - MSE: 0.9491 - MSE_v: 1.3136 - acc: 0.3333\n",
      "epoch: 53 - MSE: 0.9371 - MSE_v: 1.3051 - acc: 0.4\n",
      "epoch: 54 - MSE: 0.9259 - MSE_v: 1.2974 - acc: 0.4\n",
      "epoch: 55 - MSE: 0.9155 - MSE_v: 1.2905 - acc: 0.4667\n",
      "epoch: 56 - MSE: 0.9059 - MSE_v: 1.2842 - acc: 0.4667\n",
      "epoch: 57 - MSE: 0.8969 - MSE_v: 1.2786 - acc: 0.4667\n",
      "epoch: 58 - MSE: 0.8887 - MSE_v: 1.2735 - acc: 0.4667\n",
      "epoch: 59 - MSE: 0.881 - MSE_v: 1.2689 - acc: 0.4667\n",
      "epoch: 60 - MSE: 0.8739 - MSE_v: 1.2648 - acc: 0.4667\n",
      "epoch: 61 - MSE: 0.8673 - MSE_v: 1.2612 - acc: 0.4667\n",
      "epoch: 62 - MSE: 0.8611 - MSE_v: 1.2579 - acc: 0.4667\n",
      "epoch: 63 - MSE: 0.8554 - MSE_v: 1.255 - acc: 0.4667\n",
      "epoch: 64 - MSE: 0.8501 - MSE_v: 1.2524 - acc: 0.4667\n",
      "epoch: 65 - MSE: 0.8452 - MSE_v: 1.2501 - acc: 0.4667\n",
      "epoch: 66 - MSE: 0.8406 - MSE_v: 1.2481 - acc: 0.4667\n",
      "epoch: 67 - MSE: 0.8363 - MSE_v: 1.2464 - acc: 0.4667\n",
      "epoch: 68 - MSE: 0.8323 - MSE_v: 1.2449 - acc: 0.4667\n",
      "epoch: 69 - MSE: 0.8286 - MSE_v: 1.2436 - acc: 0.4667\n",
      "epoch: 70 - MSE: 0.8251 - MSE_v: 1.2424 - acc: 0.4667\n",
      "epoch: 71 - MSE: 0.8218 - MSE_v: 1.2415 - acc: 0.4667\n",
      "epoch: 72 - MSE: 0.8188 - MSE_v: 1.2407 - acc: 0.4667\n",
      "epoch: 73 - MSE: 0.8159 - MSE_v: 1.2401 - acc: 0.4667\n",
      "epoch: 74 - MSE: 0.8133 - MSE_v: 1.2396 - acc: 0.4667\n",
      "epoch: 75 - MSE: 0.8108 - MSE_v: 1.2393 - acc: 0.4667\n",
      "epoch: 76 - MSE: 0.8084 - MSE_v: 1.239 - acc: 0.4667\n",
      "epoch: 77 - MSE: 0.8062 - MSE_v: 1.2389 - acc: 0.4667\n",
      "epoch: 78 - MSE: 0.8042 - MSE_v: 1.2388 - acc: 0.4667\n",
      "epoch: 79 - MSE: 0.8022 - MSE_v: 1.2389 - acc: 0.4667\n",
      "epoch: 80 - MSE: 0.8004 - MSE_v: 1.239 - acc: 0.4667\n",
      "epoch: 81 - MSE: 0.7987 - MSE_v: 1.2392 - acc: 0.4667\n",
      "epoch: 82 - MSE: 0.7971 - MSE_v: 1.2394 - acc: 0.4667\n",
      "epoch: 83 - MSE: 0.7956 - MSE_v: 1.2397 - acc: 0.4667\n",
      "epoch: 84 - MSE: 0.7941 - MSE_v: 1.2401 - acc: 0.4667\n",
      "epoch: 85 - MSE: 0.7928 - MSE_v: 1.2405 - acc: 0.4667\n",
      "epoch: 86 - MSE: 0.7915 - MSE_v: 1.241 - acc: 0.4667\n",
      "epoch: 87 - MSE: 0.7903 - MSE_v: 1.2415 - acc: 0.4667\n",
      "epoch: 88 - MSE: 0.7892 - MSE_v: 1.2421 - acc: 0.4667\n",
      "epoch: 89 - MSE: 0.7881 - MSE_v: 1.2426 - acc: 0.4667\n",
      "epoch: 90 - MSE: 0.7871 - MSE_v: 1.2432 - acc: 0.4667\n",
      "epoch: 91 - MSE: 0.7862 - MSE_v: 1.2439 - acc: 0.4667\n",
      "epoch: 92 - MSE: 0.7852 - MSE_v: 1.2445 - acc: 0.4667\n",
      "epoch: 93 - MSE: 0.7844 - MSE_v: 1.2452 - acc: 0.4667\n",
      "epoch: 94 - MSE: 0.7836 - MSE_v: 1.2459 - acc: 0.4667\n",
      "epoch: 95 - MSE: 0.7828 - MSE_v: 1.2466 - acc: 0.4667\n",
      "epoch: 96 - MSE: 0.7821 - MSE_v: 1.2473 - acc: 0.5333\n",
      "epoch: 97 - MSE: 0.7814 - MSE_v: 1.248 - acc: 0.6\n",
      "epoch: 98 - MSE: 0.7807 - MSE_v: 1.2488 - acc: 0.5333\n",
      "epoch: 99 - MSE: 0.7801 - MSE_v: 1.2495 - acc: 0.5333\n",
      "epoch: 100 - MSE: 0.7795 - MSE_v: 1.2502 - acc: 0.5333\n",
      "epoch: 101 - MSE: 0.7789 - MSE_v: 1.251 - acc: 0.5333\n",
      "epoch: 102 - MSE: 0.7784 - MSE_v: 1.2518 - acc: 0.5333\n",
      "epoch: 103 - MSE: 0.7779 - MSE_v: 1.2525 - acc: 0.5333\n",
      "epoch: 104 - MSE: 0.7774 - MSE_v: 1.2533 - acc: 0.5333\n",
      "epoch: 105 - MSE: 0.7769 - MSE_v: 1.254 - acc: 0.5333\n",
      "epoch: 106 - MSE: 0.7765 - MSE_v: 1.2548 - acc: 0.5333\n",
      "epoch: 107 - MSE: 0.776 - MSE_v: 1.2555 - acc: 0.5333\n",
      "epoch: 108 - MSE: 0.7756 - MSE_v: 1.2563 - acc: 0.5333\n",
      "epoch: 109 - MSE: 0.7753 - MSE_v: 1.2571 - acc: 0.5333\n",
      "epoch: 110 - MSE: 0.7749 - MSE_v: 1.2578 - acc: 0.5333\n",
      "epoch: 111 - MSE: 0.7745 - MSE_v: 1.2585 - acc: 0.5333\n",
      "epoch: 112 - MSE: 0.7742 - MSE_v: 1.2593 - acc: 0.5333\n",
      "epoch: 113 - MSE: 0.7739 - MSE_v: 1.26 - acc: 0.5333\n",
      "epoch: 114 - MSE: 0.7736 - MSE_v: 1.2607 - acc: 0.4667\n",
      "epoch: 115 - MSE: 0.7733 - MSE_v: 1.2615 - acc: 0.4667\n",
      "epoch: 116 - MSE: 0.773 - MSE_v: 1.2622 - acc: 0.4667\n",
      "epoch: 117 - MSE: 0.7727 - MSE_v: 1.2629 - acc: 0.4667\n",
      "epoch: 118 - MSE: 0.7725 - MSE_v: 1.2636 - acc: 0.4667\n",
      "epoch: 119 - MSE: 0.7722 - MSE_v: 1.2643 - acc: 0.4667\n",
      "epoch: 120 - MSE: 0.772 - MSE_v: 1.265 - acc: 0.4667\n",
      "epoch: 121 - MSE: 0.7717 - MSE_v: 1.2656 - acc: 0.4667\n",
      "epoch: 122 - MSE: 0.7715 - MSE_v: 1.2663 - acc: 0.4667\n",
      "epoch: 123 - MSE: 0.7713 - MSE_v: 1.267 - acc: 0.4667\n",
      "epoch: 124 - MSE: 0.7711 - MSE_v: 1.2676 - acc: 0.4667\n",
      "epoch: 125 - MSE: 0.7709 - MSE_v: 1.2683 - acc: 0.4667\n",
      "epoch: 126 - MSE: 0.7707 - MSE_v: 1.2689 - acc: 0.4667\n",
      "epoch: 127 - MSE: 0.7705 - MSE_v: 1.2696 - acc: 0.4667\n",
      "epoch: 128 - MSE: 0.7704 - MSE_v: 1.2702 - acc: 0.4667\n",
      "epoch: 129 - MSE: 0.7702 - MSE_v: 1.2708 - acc: 0.4667\n",
      "epoch: 130 - MSE: 0.77 - MSE_v: 1.2714 - acc: 0.4667\n",
      "epoch: 131 - MSE: 0.7699 - MSE_v: 1.272 - acc: 0.4667\n",
      "epoch: 132 - MSE: 0.7697 - MSE_v: 1.2726 - acc: 0.4667\n",
      "epoch: 133 - MSE: 0.7696 - MSE_v: 1.2732 - acc: 0.4667\n",
      "epoch: 134 - MSE: 0.7694 - MSE_v: 1.2738 - acc: 0.4667\n",
      "epoch: 135 - MSE: 0.7693 - MSE_v: 1.2743 - acc: 0.4667\n",
      "epoch: 136 - MSE: 0.7692 - MSE_v: 1.2749 - acc: 0.4667\n",
      "epoch: 137 - MSE: 0.769 - MSE_v: 1.2754 - acc: 0.4667\n",
      "epoch: 138 - MSE: 0.7689 - MSE_v: 1.276 - acc: 0.4667\n",
      "epoch: 139 - MSE: 0.7688 - MSE_v: 1.2765 - acc: 0.4667\n",
      "epoch: 140 - MSE: 0.7687 - MSE_v: 1.2771 - acc: 0.4667\n",
      "epoch: 141 - MSE: 0.7686 - MSE_v: 1.2776 - acc: 0.4667\n",
      "epoch: 142 - MSE: 0.7685 - MSE_v: 1.2781 - acc: 0.4667\n",
      "epoch: 143 - MSE: 0.7684 - MSE_v: 1.2786 - acc: 0.4667\n",
      "epoch: 144 - MSE: 0.7683 - MSE_v: 1.2791 - acc: 0.4667\n",
      "epoch: 145 - MSE: 0.7682 - MSE_v: 1.2796 - acc: 0.4667\n",
      "epoch: 146 - MSE: 0.7681 - MSE_v: 1.2801 - acc: 0.4667\n",
      "epoch: 147 - MSE: 0.768 - MSE_v: 1.2806 - acc: 0.4667\n",
      "epoch: 148 - MSE: 0.7679 - MSE_v: 1.2811 - acc: 0.4667\n",
      "epoch: 149 - MSE: 0.7678 - MSE_v: 1.2815 - acc: 0.4667\n",
      "epoch: 150 - MSE: 0.7677 - MSE_v: 1.282 - acc: 0.4667\n",
      "epoch: 151 - MSE: 0.7676 - MSE_v: 1.2824 - acc: 0.4667\n",
      "epoch: 152 - MSE: 0.7676 - MSE_v: 1.2829 - acc: 0.4667\n",
      "epoch: 153 - MSE: 0.7675 - MSE_v: 1.2833 - acc: 0.4667\n",
      "epoch: 154 - MSE: 0.7674 - MSE_v: 1.2838 - acc: 0.4667\n",
      "epoch: 155 - MSE: 0.7673 - MSE_v: 1.2842 - acc: 0.4667\n",
      "epoch: 156 - MSE: 0.7673 - MSE_v: 1.2846 - acc: 0.4667\n",
      "epoch: 157 - MSE: 0.7672 - MSE_v: 1.2851 - acc: 0.4667\n",
      "epoch: 158 - MSE: 0.7671 - MSE_v: 1.2855 - acc: 0.4667\n",
      "epoch: 159 - MSE: 0.7671 - MSE_v: 1.2859 - acc: 0.4667\n",
      "epoch: 160 - MSE: 0.767 - MSE_v: 1.2863 - acc: 0.4667\n",
      "epoch: 161 - MSE: 0.7669 - MSE_v: 1.2867 - acc: 0.4667\n",
      "epoch: 162 - MSE: 0.7669 - MSE_v: 1.2871 - acc: 0.4667\n",
      "epoch: 163 - MSE: 0.7668 - MSE_v: 1.2875 - acc: 0.5333\n",
      "epoch: 164 - MSE: 0.7668 - MSE_v: 1.2878 - acc: 0.5333\n",
      "epoch: 165 - MSE: 0.7667 - MSE_v: 1.2882 - acc: 0.5333\n",
      "epoch: 166 - MSE: 0.7667 - MSE_v: 1.2886 - acc: 0.5333\n",
      "epoch: 167 - MSE: 0.7666 - MSE_v: 1.289 - acc: 0.5333\n",
      "epoch: 168 - MSE: 0.7666 - MSE_v: 1.2893 - acc: 0.5333\n",
      "epoch: 169 - MSE: 0.7665 - MSE_v: 1.2897 - acc: 0.5333\n",
      "epoch: 170 - MSE: 0.7665 - MSE_v: 1.29 - acc: 0.5333\n",
      "epoch: 171 - MSE: 0.7664 - MSE_v: 1.2904 - acc: 0.5333\n",
      "epoch: 172 - MSE: 0.7664 - MSE_v: 1.2907 - acc: 0.5333\n",
      "epoch: 173 - MSE: 0.7663 - MSE_v: 1.2911 - acc: 0.5333\n",
      "epoch: 174 - MSE: 0.7663 - MSE_v: 1.2914 - acc: 0.5333\n",
      "epoch: 175 - MSE: 0.7662 - MSE_v: 1.2917 - acc: 0.5333\n",
      "epoch: 176 - MSE: 0.7662 - MSE_v: 1.2921 - acc: 0.5333\n",
      "epoch: 177 - MSE: 0.7661 - MSE_v: 1.2924 - acc: 0.5333\n",
      "epoch: 178 - MSE: 0.7661 - MSE_v: 1.2927 - acc: 0.5333\n",
      "epoch: 179 - MSE: 0.7661 - MSE_v: 1.293 - acc: 0.5333\n",
      "epoch: 180 - MSE: 0.766 - MSE_v: 1.2933 - acc: 0.5333\n",
      "epoch: 181 - MSE: 0.766 - MSE_v: 1.2937 - acc: 0.5333\n",
      "epoch: 182 - MSE: 0.7659 - MSE_v: 1.294 - acc: 0.5333\n",
      "epoch: 183 - MSE: 0.7659 - MSE_v: 1.2943 - acc: 0.5333\n",
      "epoch: 184 - MSE: 0.7659 - MSE_v: 1.2946 - acc: 0.5333\n",
      "epoch: 185 - MSE: 0.7658 - MSE_v: 1.2949 - acc: 0.5333\n",
      "epoch: 186 - MSE: 0.7658 - MSE_v: 1.2952 - acc: 0.5333\n",
      "epoch: 187 - MSE: 0.7658 - MSE_v: 1.2954 - acc: 0.5333\n",
      "epoch: 188 - MSE: 0.7657 - MSE_v: 1.2957 - acc: 0.5333\n",
      "epoch: 189 - MSE: 0.7657 - MSE_v: 1.296 - acc: 0.5333\n",
      "epoch: 190 - MSE: 0.7657 - MSE_v: 1.2963 - acc: 0.5333\n",
      "epoch: 191 - MSE: 0.7656 - MSE_v: 1.2966 - acc: 0.5333\n",
      "epoch: 192 - MSE: 0.7656 - MSE_v: 1.2968 - acc: 0.5333\n",
      "epoch: 193 - MSE: 0.7656 - MSE_v: 1.2971 - acc: 0.5333\n",
      "epoch: 194 - MSE: 0.7656 - MSE_v: 1.2974 - acc: 0.5333\n",
      "epoch: 195 - MSE: 0.7655 - MSE_v: 1.2976 - acc: 0.5333\n",
      "epoch: 196 - MSE: 0.7655 - MSE_v: 1.2979 - acc: 0.5333\n",
      "epoch: 197 - MSE: 0.7655 - MSE_v: 1.2982 - acc: 0.5333\n",
      "epoch: 198 - MSE: 0.7654 - MSE_v: 1.2984 - acc: 0.5333\n",
      "epoch: 199 - MSE: 0.7654 - MSE_v: 1.2987 - acc: 0.5333\n",
      "epoch: 200 - MSE: 0.7654 - MSE_v: 1.2989 - acc: 0.5333\n",
      "epoch: 201 - MSE: 0.7654 - MSE_v: 1.2992 - acc: 0.5333\n",
      "epoch: 202 - MSE: 0.7653 - MSE_v: 1.2994 - acc: 0.5333\n",
      "epoch: 203 - MSE: 0.7653 - MSE_v: 1.2997 - acc: 0.5333\n",
      "epoch: 204 - MSE: 0.7653 - MSE_v: 1.2999 - acc: 0.5333\n",
      "epoch: 205 - MSE: 0.7653 - MSE_v: 1.3002 - acc: 0.5333\n",
      "epoch: 206 - MSE: 0.7652 - MSE_v: 1.3004 - acc: 0.5333\n",
      "epoch: 207 - MSE: 0.7652 - MSE_v: 1.3006 - acc: 0.5333\n",
      "epoch: 208 - MSE: 0.7652 - MSE_v: 1.3009 - acc: 0.5333\n",
      "epoch: 209 - MSE: 0.7652 - MSE_v: 1.3011 - acc: 0.5333\n",
      "epoch: 210 - MSE: 0.7652 - MSE_v: 1.3013 - acc: 0.5333\n",
      "epoch: 211 - MSE: 0.7651 - MSE_v: 1.3016 - acc: 0.5333\n",
      "epoch: 212 - MSE: 0.7651 - MSE_v: 1.3018 - acc: 0.5333\n",
      "epoch: 213 - MSE: 0.7651 - MSE_v: 1.302 - acc: 0.5333\n",
      "epoch: 214 - MSE: 0.7651 - MSE_v: 1.3022 - acc: 0.5333\n",
      "epoch: 215 - MSE: 0.765 - MSE_v: 1.3025 - acc: 0.4667\n",
      "epoch: 216 - MSE: 0.765 - MSE_v: 1.3027 - acc: 0.4667\n",
      "epoch: 217 - MSE: 0.765 - MSE_v: 1.3029 - acc: 0.4667\n",
      "epoch: 218 - MSE: 0.765 - MSE_v: 1.3031 - acc: 0.4667\n",
      "epoch: 219 - MSE: 0.765 - MSE_v: 1.3033 - acc: 0.4667\n",
      "epoch: 220 - MSE: 0.765 - MSE_v: 1.3035 - acc: 0.4667\n",
      "epoch: 221 - MSE: 0.7649 - MSE_v: 1.3037 - acc: 0.4667\n",
      "epoch: 222 - MSE: 0.7649 - MSE_v: 1.304 - acc: 0.4667\n",
      "epoch: 223 - MSE: 0.7649 - MSE_v: 1.3042 - acc: 0.4667\n",
      "epoch: 224 - MSE: 0.7649 - MSE_v: 1.3044 - acc: 0.4667\n",
      "epoch: 225 - MSE: 0.7649 - MSE_v: 1.3046 - acc: 0.4667\n",
      "epoch: 226 - MSE: 0.7648 - MSE_v: 1.3048 - acc: 0.4667\n",
      "epoch: 227 - MSE: 0.7648 - MSE_v: 1.305 - acc: 0.4667\n",
      "epoch: 228 - MSE: 0.7648 - MSE_v: 1.3052 - acc: 0.4667\n",
      "epoch: 229 - MSE: 0.7648 - MSE_v: 1.3054 - acc: 0.4667\n",
      "epoch: 230 - MSE: 0.7648 - MSE_v: 1.3056 - acc: 0.4667\n",
      "epoch: 231 - MSE: 0.7648 - MSE_v: 1.3057 - acc: 0.4667\n",
      "epoch: 232 - MSE: 0.7647 - MSE_v: 1.3059 - acc: 0.4667\n",
      "epoch: 233 - MSE: 0.7647 - MSE_v: 1.3061 - acc: 0.4667\n",
      "epoch: 234 - MSE: 0.7647 - MSE_v: 1.3063 - acc: 0.4667\n",
      "epoch: 235 - MSE: 0.7647 - MSE_v: 1.3065 - acc: 0.4667\n",
      "epoch: 236 - MSE: 0.7647 - MSE_v: 1.3067 - acc: 0.4667\n",
      "epoch: 237 - MSE: 0.7647 - MSE_v: 1.3069 - acc: 0.4667\n",
      "epoch: 238 - MSE: 0.7647 - MSE_v: 1.3071 - acc: 0.4667\n",
      "epoch: 239 - MSE: 0.7646 - MSE_v: 1.3072 - acc: 0.4667\n",
      "epoch: 240 - MSE: 0.7646 - MSE_v: 1.3074 - acc: 0.4667\n",
      "epoch: 241 - MSE: 0.7646 - MSE_v: 1.3076 - acc: 0.4667\n",
      "epoch: 242 - MSE: 0.7646 - MSE_v: 1.3078 - acc: 0.4667\n",
      "epoch: 243 - MSE: 0.7646 - MSE_v: 1.308 - acc: 0.4667\n",
      "epoch: 244 - MSE: 0.7646 - MSE_v: 1.3081 - acc: 0.4667\n",
      "epoch: 245 - MSE: 0.7646 - MSE_v: 1.3083 - acc: 0.4667\n",
      "epoch: 246 - MSE: 0.7645 - MSE_v: 1.3085 - acc: 0.4667\n",
      "epoch: 247 - MSE: 0.7645 - MSE_v: 1.3087 - acc: 0.4667\n",
      "epoch: 248 - MSE: 0.7645 - MSE_v: 1.3088 - acc: 0.4667\n",
      "epoch: 249 - MSE: 0.7645 - MSE_v: 1.309 - acc: 0.4667\n",
      "epoch: 250 - MSE: 0.7645 - MSE_v: 1.3092 - acc: 0.4667\n",
      "epoch: 251 - MSE: 0.7645 - MSE_v: 1.3093 - acc: 0.4667\n",
      "epoch: 252 - MSE: 0.7645 - MSE_v: 1.3095 - acc: 0.4667\n",
      "epoch: 253 - MSE: 0.7645 - MSE_v: 1.3097 - acc: 0.4667\n",
      "epoch: 254 - MSE: 0.7644 - MSE_v: 1.3098 - acc: 0.4667\n",
      "epoch: 255 - MSE: 0.7644 - MSE_v: 1.31 - acc: 0.4667\n",
      "epoch: 256 - MSE: 0.7644 - MSE_v: 1.3102 - acc: 0.4667\n",
      "epoch: 257 - MSE: 0.7644 - MSE_v: 1.3103 - acc: 0.4667\n",
      "epoch: 258 - MSE: 0.7644 - MSE_v: 1.3105 - acc: 0.4667\n",
      "epoch: 259 - MSE: 0.7644 - MSE_v: 1.3107 - acc: 0.4667\n",
      "epoch: 260 - MSE: 0.7644 - MSE_v: 1.3108 - acc: 0.4667\n",
      "epoch: 261 - MSE: 0.7644 - MSE_v: 1.311 - acc: 0.4667\n",
      "epoch: 262 - MSE: 0.7644 - MSE_v: 1.3111 - acc: 0.4667\n",
      "epoch: 263 - MSE: 0.7643 - MSE_v: 1.3113 - acc: 0.4667\n",
      "epoch: 264 - MSE: 0.7643 - MSE_v: 1.3115 - acc: 0.4667\n",
      "epoch: 265 - MSE: 0.7643 - MSE_v: 1.3116 - acc: 0.4667\n",
      "epoch: 266 - MSE: 0.7643 - MSE_v: 1.3118 - acc: 0.4667\n",
      "epoch: 267 - MSE: 0.7643 - MSE_v: 1.3119 - acc: 0.4667\n",
      "epoch: 268 - MSE: 0.7643 - MSE_v: 1.3121 - acc: 0.4667\n",
      "epoch: 269 - MSE: 0.7643 - MSE_v: 1.3122 - acc: 0.4667\n",
      "epoch: 270 - MSE: 0.7643 - MSE_v: 1.3124 - acc: 0.4667\n",
      "epoch: 271 - MSE: 0.7643 - MSE_v: 1.3125 - acc: 0.4667\n",
      "epoch: 272 - MSE: 0.7643 - MSE_v: 1.3127 - acc: 0.4667\n",
      "epoch: 273 - MSE: 0.7642 - MSE_v: 1.3128 - acc: 0.4667\n",
      "epoch: 274 - MSE: 0.7642 - MSE_v: 1.313 - acc: 0.4667\n",
      "epoch: 275 - MSE: 0.7642 - MSE_v: 1.3131 - acc: 0.4667\n",
      "epoch: 276 - MSE: 0.7642 - MSE_v: 1.3133 - acc: 0.4667\n",
      "epoch: 277 - MSE: 0.7642 - MSE_v: 1.3134 - acc: 0.4667\n",
      "epoch: 278 - MSE: 0.7642 - MSE_v: 1.3135 - acc: 0.4667\n",
      "epoch: 279 - MSE: 0.7642 - MSE_v: 1.3137 - acc: 0.4667\n",
      "epoch: 280 - MSE: 0.7642 - MSE_v: 1.3138 - acc: 0.4667\n",
      "epoch: 281 - MSE: 0.7642 - MSE_v: 1.314 - acc: 0.4667\n",
      "epoch: 282 - MSE: 0.7642 - MSE_v: 1.3141 - acc: 0.4667\n",
      "epoch: 283 - MSE: 0.7642 - MSE_v: 1.3142 - acc: 0.4667\n",
      "epoch: 284 - MSE: 0.7641 - MSE_v: 1.3144 - acc: 0.4667\n",
      "epoch: 285 - MSE: 0.7641 - MSE_v: 1.3145 - acc: 0.4667\n",
      "epoch: 286 - MSE: 0.7641 - MSE_v: 1.3147 - acc: 0.4667\n",
      "epoch: 287 - MSE: 0.7641 - MSE_v: 1.3148 - acc: 0.4667\n",
      "epoch: 288 - MSE: 0.7641 - MSE_v: 1.3149 - acc: 0.4667\n",
      "epoch: 289 - MSE: 0.7641 - MSE_v: 1.3151 - acc: 0.4667\n",
      "epoch: 290 - MSE: 0.7641 - MSE_v: 1.3152 - acc: 0.4667\n",
      "epoch: 291 - MSE: 0.7641 - MSE_v: 1.3153 - acc: 0.4667\n",
      "epoch: 292 - MSE: 0.7641 - MSE_v: 1.3155 - acc: 0.4667\n",
      "epoch: 293 - MSE: 0.7641 - MSE_v: 1.3156 - acc: 0.4667\n",
      "epoch: 294 - MSE: 0.7641 - MSE_v: 1.3157 - acc: 0.4667\n",
      "epoch: 295 - MSE: 0.7641 - MSE_v: 1.3159 - acc: 0.4667\n",
      "epoch: 296 - MSE: 0.7641 - MSE_v: 1.316 - acc: 0.4667\n",
      "epoch: 297 - MSE: 0.764 - MSE_v: 1.3161 - acc: 0.4667\n",
      "epoch: 298 - MSE: 0.764 - MSE_v: 1.3163 - acc: 0.4667\n",
      "epoch: 299 - MSE: 0.764 - MSE_v: 1.3164 - acc: 0.4667\n",
      "epoch: 300 - MSE: 0.764 - MSE_v: 1.3165 - acc: 0.4667\n",
      "epoch: 301 - MSE: 0.764 - MSE_v: 1.3166 - acc: 0.4667\n",
      "epoch: 302 - MSE: 0.764 - MSE_v: 1.3168 - acc: 0.4667\n",
      "epoch: 303 - MSE: 0.764 - MSE_v: 1.3169 - acc: 0.4667\n",
      "epoch: 304 - MSE: 0.764 - MSE_v: 1.317 - acc: 0.4667\n",
      "epoch: 305 - MSE: 0.764 - MSE_v: 1.3171 - acc: 0.4667\n",
      "epoch: 306 - MSE: 0.764 - MSE_v: 1.3173 - acc: 0.4667\n",
      "epoch: 307 - MSE: 0.764 - MSE_v: 1.3174 - acc: 0.4667\n",
      "epoch: 308 - MSE: 0.764 - MSE_v: 1.3175 - acc: 0.4667\n",
      "epoch: 309 - MSE: 0.764 - MSE_v: 1.3176 - acc: 0.4667\n",
      "epoch: 310 - MSE: 0.764 - MSE_v: 1.3178 - acc: 0.4667\n",
      "epoch: 311 - MSE: 0.7639 - MSE_v: 1.3179 - acc: 0.4667\n",
      "epoch: 312 - MSE: 0.7639 - MSE_v: 1.318 - acc: 0.4667\n",
      "epoch: 313 - MSE: 0.7639 - MSE_v: 1.3181 - acc: 0.4667\n",
      "epoch: 314 - MSE: 0.7639 - MSE_v: 1.3182 - acc: 0.4667\n",
      "epoch: 315 - MSE: 0.7639 - MSE_v: 1.3184 - acc: 0.4667\n",
      "epoch: 316 - MSE: 0.7639 - MSE_v: 1.3185 - acc: 0.4667\n",
      "epoch: 317 - MSE: 0.7639 - MSE_v: 1.3186 - acc: 0.4667\n",
      "epoch: 318 - MSE: 0.7639 - MSE_v: 1.3187 - acc: 0.4667\n",
      "epoch: 319 - MSE: 0.7639 - MSE_v: 1.3188 - acc: 0.4667\n",
      "epoch: 320 - MSE: 0.7639 - MSE_v: 1.3189 - acc: 0.4667\n",
      "epoch: 321 - MSE: 0.7639 - MSE_v: 1.3191 - acc: 0.4667\n",
      "epoch: 322 - MSE: 0.7639 - MSE_v: 1.3192 - acc: 0.4667\n",
      "epoch: 323 - MSE: 0.7639 - MSE_v: 1.3193 - acc: 0.4667\n",
      "epoch: 324 - MSE: 0.7639 - MSE_v: 1.3194 - acc: 0.4667\n",
      "epoch: 325 - MSE: 0.7639 - MSE_v: 1.3195 - acc: 0.4667\n",
      "epoch: 326 - MSE: 0.7639 - MSE_v: 1.3196 - acc: 0.4667\n",
      "epoch: 327 - MSE: 0.7639 - MSE_v: 1.3197 - acc: 0.4667\n",
      "epoch: 328 - MSE: 0.7638 - MSE_v: 1.3199 - acc: 0.4667\n",
      "epoch: 329 - MSE: 0.7638 - MSE_v: 1.32 - acc: 0.4667\n",
      "epoch: 330 - MSE: 0.7638 - MSE_v: 1.3201 - acc: 0.4667\n",
      "epoch: 331 - MSE: 0.7638 - MSE_v: 1.3202 - acc: 0.4667\n",
      "epoch: 332 - MSE: 0.7638 - MSE_v: 1.3203 - acc: 0.4667\n",
      "epoch: 333 - MSE: 0.7638 - MSE_v: 1.3204 - acc: 0.4667\n",
      "epoch: 334 - MSE: 0.7638 - MSE_v: 1.3205 - acc: 0.4667\n",
      "epoch: 335 - MSE: 0.7638 - MSE_v: 1.3206 - acc: 0.4667\n",
      "epoch: 336 - MSE: 0.7638 - MSE_v: 1.3207 - acc: 0.4667\n",
      "epoch: 337 - MSE: 0.7638 - MSE_v: 1.3208 - acc: 0.4667\n",
      "epoch: 338 - MSE: 0.7638 - MSE_v: 1.3209 - acc: 0.4667\n",
      "epoch: 339 - MSE: 0.7638 - MSE_v: 1.321 - acc: 0.4667\n",
      "epoch: 340 - MSE: 0.7638 - MSE_v: 1.3211 - acc: 0.4667\n",
      "epoch: 341 - MSE: 0.7638 - MSE_v: 1.3213 - acc: 0.4667\n",
      "epoch: 342 - MSE: 0.7638 - MSE_v: 1.3214 - acc: 0.4667\n",
      "epoch: 343 - MSE: 0.7638 - MSE_v: 1.3215 - acc: 0.4667\n",
      "epoch: 344 - MSE: 0.7638 - MSE_v: 1.3216 - acc: 0.4667\n",
      "epoch: 345 - MSE: 0.7638 - MSE_v: 1.3217 - acc: 0.4667\n",
      "epoch: 346 - MSE: 0.7638 - MSE_v: 1.3218 - acc: 0.4667\n",
      "epoch: 347 - MSE: 0.7638 - MSE_v: 1.3219 - acc: 0.4667\n",
      "epoch: 348 - MSE: 0.7637 - MSE_v: 1.322 - acc: 0.4667\n",
      "epoch: 349 - MSE: 0.7637 - MSE_v: 1.3221 - acc: 0.4667\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, 350, 0.001, 1, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 1, 1, 0, 1, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model2.predict(X_valid)\n",
    "pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 1, 0, 0, 2, 1, 0, 0, 2, 2, 1, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46666666865348816"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.accuracy(Y_valid, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
