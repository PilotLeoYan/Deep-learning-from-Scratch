{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "tf.keras.config.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "M: int = 100\n",
    "N: int = 5\n",
    "N_CLASS: int = 3\n",
    "\n",
    "X, Y = make_classification(n_samples=M, n_features=N, n_classes=N_CLASS,\n",
    "                           n_informative=N-1, n_redundant=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = torch.nn.functional.one_hot(torch.tensor(Y).long(), 3)\n",
    "Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85, 5]) torch.Size([85, 3])\n",
      "torch.Size([15, 5]) torch.Size([15, 3])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid = torch.tensor(X[:85]), torch.tensor(X[85:])\n",
    "Y_train, Y_valid = Y_hat[:85], Y_hat[85:]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainables parameters\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W} &\\in \\mathbb{R}^{N \\times Q} \\\\\n",
    "\\mathbf{b} &\\in \\mathbb{R}^{Q}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $N$ is the number of features and $Q$ is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self, n_features: int, n_classes: int):\n",
    "        self.w = torch.randn(n_features, n_classes)\n",
    "        self.b = torch.randn(n_classes)\n",
    "\n",
    "    def copy_params(self, tf_model) -> None:\n",
    "        \"\"\"Copy the parameters from a TensorFlow model to this PyTorch model.\n",
    "\n",
    "        Args:\n",
    "            tf_model: A TensorFlow model from which to copy the parameters.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.w.copy_(torch.tensor(tf_model.weights[0].numpy()))\n",
    "        self.b.copy_(torch.tensor(tf_model.weights[1].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sum and softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted sum\n",
    "$$\n",
    "\\mathbf{Z}(\\mathbf{X}) = \\mathbf{X} \\mathbf{W} + \\mathbf{b} \\\\\n",
    "\\mathbf{Z} : \\mathbb{R}^{M \\times N} \\rightarrow \\mathbb{R}^{M \\times Q}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax function\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}_{i,:})_{j} = \\frac{\\exp(\\mathbf{Z}_{i,:})_{j}}\n",
    "{\\sum_{k=1}^{Q}(\\exp(\\mathbf{Z}_{i,:})_{k})}\n",
    "$$\n",
    "then:\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}_{i,:}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{1} &\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{2} &\n",
    "    \\cdots &\n",
    "    \\sigma(\\mathbf{Z}_{i,:})_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\sigma(\\mathbf{Z}) = \\begin{bmatrix}\n",
    "    \\sigma(\\mathbf{Z}_{1,:}) \\\\\n",
    "    \\sigma(\\mathbf{Z}_{2,:}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\mathbf{Z}_{M,:})\n",
    "\\end{bmatrix} \\\\\n",
    "\\sigma(\\mathbf{Z}) : \\mathbb{R}^{M \\times Q} \\rightarrow \\mathbb{R}^{M \\times Q}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # weighted sum\n",
    "    z = torch.matmul(x, self.w) + self.b\n",
    "    # softmax function\n",
    "    z_exp = torch.exp(z)\n",
    "    y_pred = z_exp / z_exp.sum(1, keepdims=True)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function: Cross Entropy-loss:\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum_{i=1}^{M} \\sum_{k=1}^{Q}(\n",
    "    Y_{ik} \\log(\\hat{Y}_{ik})\n",
    ") \\\\\n",
    "L(\\mathbf{\\hat{Y}}) : \\mathbb{R}^{M \\times Q} \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "**Remark**: for this case $\\mathbf{\\hat{Y}}$ is $\\sigma(\\mathbf{Z})$. It is not obligatory to use softmax for CE.<br>\n",
    "Vectorized form:\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum_{i=1}^{M} \\left(\n",
    "    \\mathbf{y}_{i,:}^T \\log(\\mathbf{\\hat{y}}_{i,:})\n",
    "\\right)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "L(\\mathbf{\\hat{Y}}) = - \\frac{1}{M} \\sum \\left(\n",
    "    \\mathbf{Y} \\odot \\log(\\mathbf{\\hat{Y}})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def evaluate(self, x: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = self.predict(x)\n",
    "    loss = y_true * torch.log(y_pred)\n",
    "    return - loss.sum().item() / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} =\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}}\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} =\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}}\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}\n",
    "$$\n",
    "where their shapes are:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{N \\times Q} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{Q} \\\\\n",
    "\\frac{\\partial L}{\\partial \\sigma} &\\in \\mathbb{R}^{M \\times Q} \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}} &\\in \\mathbb{R}^{(M \\times Q) \\times (M \\times Q)} \\\\\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} &\\in \\mathbb{R}^{(M \\times Q) \\times (N \\times Q)} \\\\\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} &\\in \\mathbb{R}^{(M \\times Q) \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted sum derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}$\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1N} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2N} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{M1} & x_{M2} & \\cdots & x_{MN}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\\\\n",
    "    \\mathbf{x}_{2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{W} = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1Q} \\\\\n",
    "    w_{21} & w_{22} & \\cdots & w_{2Q} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{N1} & w_{N2} & \\cdots & w_{NQ} \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\mathbf{w}_{1} &\n",
    "    \\mathbf{w}_{2} &\n",
    "    \\cdots &\n",
    "    \\mathbf{w}_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\mathbf{x}_{p}^T = \\begin{bmatrix} x_{p1} & x_{p2} & \\cdots & x_{pN} \\end{bmatrix}$ \n",
    "and $\\mathbf{w}_{q} = \\begin{bmatrix} w_{1q} & w_{2q} & \\cdots & w_{Nq} \\end{bmatrix}^T$\n",
    ", then\n",
    "$$\n",
    "\\mathbf{XW} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\mathbf{w}_{1} & \\mathbf{x}_{1}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{1}^T \\mathbf{w}_{Q} \\\\\n",
    "    \\mathbf{x}_{2}^T \\mathbf{w}_{1} & \\mathbf{x}_{2}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{2}^T \\mathbf{w}_{Q} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T \\mathbf{w}_{1} & \\mathbf{x}_{M}^T \\mathbf{w}_{2} & \\cdots & \\mathbf{x}_{M}^T \\mathbf{w}_{Q}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{XW}_{pq}}{\\partial w_{ij}} = \n",
    "\\frac{\\partial \\mathbf{x}_{p}^T\\mathbf{w}_{q}}{\\partial w_{ij}} = \\begin{cases}\n",
    "    x_{pi} & \\text{ if } q=j \\\\ \n",
    "    0 & \\text{ if } q\\neq j \n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p = 1, ..., M$, $q, j = 1, ..., Q$ and $i = 1, ..., N$. <br>\n",
    "Vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} = \n",
    "\\mathbb{I} \\otimes \\mathbf{X}\n",
    "$$\n",
    "where $\\otimes$ is Kronecker product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}}$\n",
    "$$\n",
    "\\mathbf{XW + b} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_{1}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{1}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{1}^T \\mathbf{w}_{Q}+b_Q \\\\\n",
    "    \\mathbf{x}_{2}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{2}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{2}^T \\mathbf{w}_{Q}+b_Q  \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{x}_{M}^T \\mathbf{w}_{1}+b_1 & \\mathbf{x}_{M}^T \\mathbf{w}_{2}+b_2 & \\cdots & \\mathbf{x}_{M}^T \\mathbf{w}_{Q}+b_Q \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "therefore:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{(XW+b)}_{pq}}{\\partial b_{i}} = \n",
    "\\frac{\\partial \\mathbf{x}_{p}^T\\mathbf{w}_{q} + b_{q}}{\\partial b_{i}} = \\begin{cases}\n",
    "    1 & \\text{ if } q=i \\\\ \n",
    "    0 & \\text{ if } q\\neq i\n",
    "\\end{cases}\n",
    "$$\n",
    "for all $p = 1, ..., M$ and $q,i = 1, ..., Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax derivative summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i=p,:}} = \\text{diag}(\\sigma(\\mathbf{Z}_{p,:})) - \\sigma(\\mathbf{Z}_{p,:}) \\sigma(\\mathbf{Z}_{p,:})^T\n",
    "$$\n",
    "\n",
    "Second case:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\mathbf{Z})_{p,:}}{\\partial \\mathbf{Z}_{i\\neq p,:}} = \\mathbf{0}\n",
    "$$\n",
    "Please check [Softmax Function and Gradient](softmax_function_and_gradient.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-entropy derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this CE:\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\hat{Y}}) &= - \\frac{1}{M} \\sum_{i=1}^{M} \\left(\n",
    "    \\mathbf{y}_{i,:}^T \\log(\\mathbf{\\hat{y}}_{i,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\left(\n",
    "    \\mathbf{y}_{1,:}^T \\log(\\mathbf{\\hat{y}}_{1,:}) +\n",
    "    \\mathbf{y}_{2,:}^T \\log(\\mathbf{\\hat{y}}_{2,:}) +\n",
    "    ... + \n",
    "    \\mathbf{y}_{M,:}^T \\log(\\mathbf{\\hat{y}}_{M,:})\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p,:}} &= \\begin{bmatrix}\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p1}} & \n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p2}} &\n",
    "    \\cdots &\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{pQ}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{1 \\times Q} \\\\\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{Y}}} &= \\begin{bmatrix}\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{1,:}} \\\\\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{2,:}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{M,:}}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{M \\times Q}\n",
    "\\end{align*}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{y}}_{p,:}} &=\n",
    "-\\frac{1}{M} \\frac{\\partial}{\\partial \\mathbf{\\hat{y}}_{p,:}} \\left(\n",
    "    \\mathbf{y}_{1,:}^T \\log(\\mathbf{\\hat{y}}_{1,:}) +\n",
    "    ... +\n",
    "    \\mathbf{y}_{p,:}^T \\log(\\mathbf{\\hat{y}}_{p,:}) +\n",
    "    ... + \n",
    "    \\mathbf{y}_{M,:}^T \\log(\\mathbf{\\hat{y}}_{M,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\frac{\\partial}{\\partial \\mathbf{\\hat{y}}_{p,:}} \\left(\n",
    "    \\mathbf{y}_{p,:}^T \\log(\\mathbf{\\hat{y}}_{p,:})\n",
    "\\right) \\\\\n",
    "&= -\\frac{1}{M} \\left(\n",
    "    \\mathbf{y}_{p,:} \\oslash \\mathbf{\\hat{y}}_{p,:}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\oslash$ is element-wise divide. <br>\n",
    "Therefore\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{\\hat{Y}})}{\\partial \\mathbf{\\hat{Y}}} &= -\\frac{1}{M} \\begin{bmatrix}\n",
    "    \\mathbf{y}_{1,:} \\oslash \\mathbf{\\hat{y}}_{1,:} \\\\\n",
    "    \\mathbf{y}_{2,:} \\oslash \\mathbf{\\hat{y}}_{2,:} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{y}_{M,:} \\oslash \\mathbf{\\hat{y}}_{M,:}\n",
    "\\end{bmatrix} \\\\\n",
    "&= -\\frac{1}{M} \\mathbf{Y} \\oslash \\mathbf{\\hat{Y}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pull it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} &=\n",
    "\\frac{\\partial L}{\\partial \\sigma}\n",
    "\\frac{\\partial \\sigma}{\\partial \\mathbf{Z}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Cyan} (M \\times Q)} \\times ({\\color{Cyan} M \\times Q} \\times {\\color{Orange} M \\times Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Orange} M \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial L}{\\partial \\mathbf{W}}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} &=\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} \n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Orange} (M \\times Q)} \\times ({\\color{Orange} M \\times Q} \\times {\\color{Magenta} N \\times Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} N \\times Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial L}{\\partial \\mathbf{b}}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &=\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}} \n",
    "\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} \\\\\n",
    "&\\in \\mathbb{R}^{{\\color{Orange} (M \\times Q)} \\times ({\\color{Orange} M \\times Q} \\times {\\color{Magenta} Q})} \\\\\n",
    "&\\in \\mathbb{R}^{\\color{Magenta} Q}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "We can simply compute:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \n",
    "\\frac{1}{M} \\mathbf{X}^T \\left(\n",
    "    \\sigma - \\mathbf{Y}\n",
    "\\right)\n",
    "$$\n",
    "but the objective of this repository is to understand piece by piece and not simply give things for done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def update(self, x: torch.Tensor, y_true: torch.Tensor,\n",
    "           y_pred: torch.Tensor, lr: float) -> None:\n",
    "    m, n = x.shape\n",
    "    m, n_classes = y_true.shape\n",
    "\n",
    "    # cross entropy der\n",
    "    delta = -(y_true / y_pred) / m\n",
    "\n",
    "    # softmax der\n",
    "    soft_der = torch.zeros((m, n_classes, m, n_classes))\n",
    "    for i in range(m): soft_der[i,:,i,:] = torch.diag(y_pred[i,:]) - torch.outer(y_pred[i,:], y_pred[i,:])\n",
    "    delta = torch.einsum('pq,pqij->ij', delta, soft_der)\n",
    "\n",
    "    # weighted sum der\n",
    "    identity = torch.eye(n_classes)\n",
    "\n",
    "    ## weight der\n",
    "    w_der = torch.kron(x.unsqueeze(1).unsqueeze(3), identity.unsqueeze(0).unsqueeze(2))\n",
    "    w_der = torch.einsum('pq,pqij->ij', delta, w_der)\n",
    "    self.w -= lr * w_der\n",
    "\n",
    "    ## bias der\n",
    "    self.b  -= lr * delta.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def accuracy(self, y_true, y_pred):\n",
    "    preds = y_pred.argmax(axis=-1)\n",
    "    compare = (y_true.argmax(axis=-1) == preds).type(torch.float32)\n",
    "    return compare.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SoftmaxClassifier)\n",
    "def fit(self, x_train: torch.Tensor, y_train: torch.Tensor, \n",
    "        epochs: int, lr: float, batch_size: int, \n",
    "        x_valid: torch.Tensor, y_valid: torch.Tensor) -> None:\n",
    "    \"\"\"fit the model using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        x_train: Input tensor of shape (n_samples, num_features).\n",
    "        y_train: Target tensor one hot of shape (n_samples, n_classes).\n",
    "        epochs: Number of epochs to train.\n",
    "        lr: learning rate (0, 1).\n",
    "        batch_size: Int number of batch.\n",
    "        x_valid: Input tensor of shape (n_valid_samples, num_features).\n",
    "        y_valid: Input tensor one hot of shape (n_valid_samples, n_valid_classes).\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        num_batch = 0\n",
    "        for batch in range(0, len(y_train), batch_size):\n",
    "            num_batch += 1\n",
    "            x_b = x_train[batch:batch+batch_size]\n",
    "            y_b = y_train[batch:batch+batch_size]\n",
    "\n",
    "            y_pred = self.predict(x_b)\n",
    "            loss += self.evaluate(x_b, y_b)\n",
    "\n",
    "            self.update(x_b, y_b, y_pred, lr)\n",
    "\n",
    "        loss = round(loss / num_batch, 4)\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        acc = round(self.accuracy(y_valid, self.predict(x_valid)), 4)\n",
    "        print(f'epoch: {epoch} - MSE: {loss} - MSE_v: {loss_v} - acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch vs TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCHS = 16\n",
    "BATCH = len(X_train) // 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558ms/step - accuracy: 0.0000e+00 - loss: 0.4980\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m18\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> (144.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18\u001b[0m (144.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> (144.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18\u001b[0m (144.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TFModel = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=N_CLASS, activation='softmax')\n",
    "])\n",
    "\n",
    "TFModel.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=LR),\n",
    "    metrics = [tf.keras.metrics.Accuracy()]\n",
    ")\n",
    "\n",
    "TFModel.evaluate(X_train[:1], Y_train[:1])\n",
    "\n",
    "TFModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifier(N, N_CLASS)\n",
    "model.copy_params(TFModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(tensor_true, tensor_pred) -> float:\n",
    "    \"\"\"\n",
    "     Calculates the percentage error between two tensors or floats.\n",
    "\n",
    "     If the arguments are simple floats or ints, calculate the percentage error between them.\n",
    "     If the arguments are Numpy ndarray and PyTorch tensor, calculate the percentage error between them.\n",
    "     If the argumens are PyTorch tensors, calculate the percentage error between them.\n",
    "\n",
    "     Args:\n",
    "         tensor_true: The true tensor or true float.\n",
    "         pred_tensor: The predicted tensor or the predicted float.\n",
    "\n",
    "     Returns:\n",
    "         The percentage error between the tensors or floats.\n",
    "     \"\"\"\n",
    "    if isinstance(tensor_true, (float, int)) and isinstance(tensor_pred, (float, int)):\n",
    "        return np.abs(tensor_true - tensor_pred) / np.abs(tensor_true) * 100\n",
    "    elif type(tensor_true) is np.ndarray and type(tensor_pred) is torch.Tensor:\n",
    "        e = np.abs(tensor_true - tensor_pred.numpy()) / np.abs(tensor_true)\n",
    "        return np.mean(e) * 100\n",
    "    e = torch.abs(tensor_true - tensor_pred) / torch.abs(tensor_true)\n",
    "    return torch.mean(e) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.316886891720421e-14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.predict(X_train, batch_size=len(X_train))\n",
    "predict = model.predict(X_train)\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0000e+00 - loss: 1.6837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.evaluate(X_train, Y_train, batch_size=len(X_train))[0]\n",
    "predict = model.evaluate(X_train, Y_train)\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0000e+00 - loss: 1.8048 - val_accuracy: 0.0000e+00 - val_loss: 1.9880\n",
      "Epoch 2/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 1.7202 - val_accuracy: 0.0000e+00 - val_loss: 1.9247\n",
      "Epoch 3/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 1.6423 - val_accuracy: 0.0000e+00 - val_loss: 1.8664\n",
      "Epoch 4/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.5718 - val_accuracy: 0.0000e+00 - val_loss: 1.8133\n",
      "Epoch 5/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 1.5091 - val_accuracy: 0.0000e+00 - val_loss: 1.7651\n",
      "Epoch 6/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.4544 - val_accuracy: 0.0000e+00 - val_loss: 1.7216\n",
      "Epoch 7/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.4072 - val_accuracy: 0.0000e+00 - val_loss: 1.6824\n",
      "Epoch 8/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.3668 - val_accuracy: 0.0000e+00 - val_loss: 1.6468\n",
      "Epoch 9/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.3319 - val_accuracy: 0.0000e+00 - val_loss: 1.6145\n",
      "Epoch 10/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.3017 - val_accuracy: 0.0000e+00 - val_loss: 1.5849\n",
      "Epoch 11/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.2752 - val_accuracy: 0.0000e+00 - val_loss: 1.5576\n",
      "Epoch 12/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.2517 - val_accuracy: 0.0000e+00 - val_loss: 1.5322\n",
      "Epoch 13/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.2305 - val_accuracy: 0.0000e+00 - val_loss: 1.5085\n",
      "Epoch 14/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.2113 - val_accuracy: 0.0000e+00 - val_loss: 1.4863\n",
      "Epoch 15/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 1.1937 - val_accuracy: 0.0000e+00 - val_loss: 1.4653\n",
      "Epoch 16/16\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 1.1774 - val_accuracy: 0.0000e+00 - val_loss: 1.4454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19439fb3250>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFModel.fit(X_train, Y_train, batch_size=BATCH, epochs=EPOCHS,\n",
    "            shuffle=False, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.9439420934033271,\n",
       " 1.8021994481590227,\n",
       " 1.6718211303221795,\n",
       " 1.5545473262674114,\n",
       " 1.4516629543920239,\n",
       " 1.3635510372070827,\n",
       " 1.2895111117048481,\n",
       " 1.2279633852114837,\n",
       " 1.1768915756793612,\n",
       " 1.1342669605696454,\n",
       " 1.098304917643161,\n",
       " 1.0675545585920798,\n",
       " 1.0408871511852702,\n",
       " 1.0174434438939994,\n",
       " 0.9965742493713193,\n",
       " 0.9777884971389093]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFModel.history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 1.9439 - MSE_v: 1.988 - acc: 0.2\n",
      "epoch: 1 - MSE: 1.8022 - MSE_v: 1.9247 - acc: 0.2\n",
      "epoch: 2 - MSE: 1.6718 - MSE_v: 1.8664 - acc: 0.2\n",
      "epoch: 3 - MSE: 1.5545 - MSE_v: 1.8133 - acc: 0.2667\n",
      "epoch: 4 - MSE: 1.4517 - MSE_v: 1.7651 - acc: 0.2667\n",
      "epoch: 5 - MSE: 1.3636 - MSE_v: 1.7216 - acc: 0.3333\n",
      "epoch: 6 - MSE: 1.2895 - MSE_v: 1.6824 - acc: 0.3333\n",
      "epoch: 7 - MSE: 1.228 - MSE_v: 1.6468 - acc: 0.3333\n",
      "epoch: 8 - MSE: 1.1769 - MSE_v: 1.6145 - acc: 0.3333\n",
      "epoch: 9 - MSE: 1.1343 - MSE_v: 1.5849 - acc: 0.3333\n",
      "epoch: 10 - MSE: 1.0983 - MSE_v: 1.5576 - acc: 0.3333\n",
      "epoch: 11 - MSE: 1.0676 - MSE_v: 1.5322 - acc: 0.3333\n",
      "epoch: 12 - MSE: 1.0409 - MSE_v: 1.5085 - acc: 0.3333\n",
      "epoch: 13 - MSE: 1.0174 - MSE_v: 1.4863 - acc: 0.3333\n",
      "epoch: 14 - MSE: 0.9966 - MSE_v: 1.4653 - acc: 0.3333\n",
      "epoch: 15 - MSE: 0.9778 - MSE_v: 1.4454 - acc: 0.2667\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, EPOCHS, LR, BATCH, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.641674717706504e-14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.weights[0].numpy()\n",
    "predict = model.w\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5868532473458515e-14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predict = TFModel.weights[1].numpy()\n",
    "predict = model.b\n",
    "\n",
    "error(tf_predict, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4401, -1.4111, -0.8354])\n",
      "tensor([[ 1.0526e+00, -5.9961e-01,  2.8065e-02],\n",
      "        [-2.3228e-01,  9.5730e-01,  4.8130e-01],\n",
      "        [-1.5735e+00, -7.7481e-01,  2.9745e-01],\n",
      "        [-1.4995e-02,  6.8289e-01,  2.2599e-04],\n",
      "        [-9.1810e-01, -1.0410e+00,  1.2621e+00]])\n"
     ]
    }
   ],
   "source": [
    "model2 = SoftmaxClassifier(N, N_CLASS)\n",
    "\n",
    "print(model2.b)\n",
    "print(model2.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9475972815460652"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - MSE: 2.1194 - MSE_v: 1.8911 - acc: 0.4667\n",
      "epoch: 1 - MSE: 2.0286 - MSE_v: 1.8378 - acc: 0.4667\n",
      "epoch: 2 - MSE: 1.9436 - MSE_v: 1.7874 - acc: 0.4667\n",
      "epoch: 3 - MSE: 1.8641 - MSE_v: 1.7397 - acc: 0.4667\n",
      "epoch: 4 - MSE: 1.7899 - MSE_v: 1.6945 - acc: 0.4667\n",
      "epoch: 5 - MSE: 1.7207 - MSE_v: 1.6515 - acc: 0.4667\n",
      "epoch: 6 - MSE: 1.6563 - MSE_v: 1.6105 - acc: 0.4667\n",
      "epoch: 7 - MSE: 1.5962 - MSE_v: 1.5714 - acc: 0.4667\n",
      "epoch: 8 - MSE: 1.5403 - MSE_v: 1.5341 - acc: 0.4667\n",
      "epoch: 9 - MSE: 1.4881 - MSE_v: 1.4984 - acc: 0.4667\n",
      "epoch: 10 - MSE: 1.4395 - MSE_v: 1.4643 - acc: 0.4667\n",
      "epoch: 11 - MSE: 1.3941 - MSE_v: 1.4317 - acc: 0.4667\n",
      "epoch: 12 - MSE: 1.3518 - MSE_v: 1.4005 - acc: 0.4667\n",
      "epoch: 13 - MSE: 1.3122 - MSE_v: 1.3706 - acc: 0.4667\n",
      "epoch: 14 - MSE: 1.2753 - MSE_v: 1.3421 - acc: 0.5333\n",
      "epoch: 15 - MSE: 1.2409 - MSE_v: 1.3148 - acc: 0.5333\n",
      "epoch: 16 - MSE: 1.2087 - MSE_v: 1.2886 - acc: 0.5333\n",
      "epoch: 17 - MSE: 1.1786 - MSE_v: 1.2636 - acc: 0.5333\n",
      "epoch: 18 - MSE: 1.1504 - MSE_v: 1.2397 - acc: 0.5333\n",
      "epoch: 19 - MSE: 1.1241 - MSE_v: 1.2168 - acc: 0.5333\n",
      "epoch: 20 - MSE: 1.0995 - MSE_v: 1.1949 - acc: 0.5333\n",
      "epoch: 21 - MSE: 1.0765 - MSE_v: 1.1739 - acc: 0.6\n",
      "epoch: 22 - MSE: 1.055 - MSE_v: 1.1537 - acc: 0.6\n",
      "epoch: 23 - MSE: 1.0348 - MSE_v: 1.1344 - acc: 0.6\n",
      "epoch: 24 - MSE: 1.0159 - MSE_v: 1.1159 - acc: 0.6\n",
      "epoch: 25 - MSE: 0.9981 - MSE_v: 1.0981 - acc: 0.6\n",
      "epoch: 26 - MSE: 0.9814 - MSE_v: 1.081 - acc: 0.6\n",
      "epoch: 27 - MSE: 0.9658 - MSE_v: 1.0646 - acc: 0.6\n",
      "epoch: 28 - MSE: 0.9511 - MSE_v: 1.0488 - acc: 0.6\n",
      "epoch: 29 - MSE: 0.9372 - MSE_v: 1.0336 - acc: 0.6\n",
      "epoch: 30 - MSE: 0.9241 - MSE_v: 1.019 - acc: 0.6667\n",
      "epoch: 31 - MSE: 0.9118 - MSE_v: 1.0049 - acc: 0.6667\n",
      "epoch: 32 - MSE: 0.9002 - MSE_v: 0.9913 - acc: 0.6667\n",
      "epoch: 33 - MSE: 0.8892 - MSE_v: 0.9782 - acc: 0.6667\n",
      "epoch: 34 - MSE: 0.8788 - MSE_v: 0.9656 - acc: 0.6667\n",
      "epoch: 35 - MSE: 0.8689 - MSE_v: 0.9534 - acc: 0.6667\n",
      "epoch: 36 - MSE: 0.8596 - MSE_v: 0.9416 - acc: 0.6667\n",
      "epoch: 37 - MSE: 0.8507 - MSE_v: 0.9302 - acc: 0.6667\n",
      "epoch: 38 - MSE: 0.8423 - MSE_v: 0.9192 - acc: 0.6667\n",
      "epoch: 39 - MSE: 0.8343 - MSE_v: 0.9085 - acc: 0.6667\n",
      "epoch: 40 - MSE: 0.8266 - MSE_v: 0.8982 - acc: 0.6667\n",
      "epoch: 41 - MSE: 0.8194 - MSE_v: 0.8882 - acc: 0.6667\n",
      "epoch: 42 - MSE: 0.8124 - MSE_v: 0.8786 - acc: 0.6667\n",
      "epoch: 43 - MSE: 0.8058 - MSE_v: 0.8692 - acc: 0.6667\n",
      "epoch: 44 - MSE: 0.7994 - MSE_v: 0.8601 - acc: 0.6667\n",
      "epoch: 45 - MSE: 0.7934 - MSE_v: 0.8513 - acc: 0.6667\n",
      "epoch: 46 - MSE: 0.7875 - MSE_v: 0.8428 - acc: 0.6667\n",
      "epoch: 47 - MSE: 0.782 - MSE_v: 0.8345 - acc: 0.6667\n",
      "epoch: 48 - MSE: 0.7766 - MSE_v: 0.8265 - acc: 0.6667\n",
      "epoch: 49 - MSE: 0.7715 - MSE_v: 0.8186 - acc: 0.6667\n",
      "epoch: 50 - MSE: 0.7666 - MSE_v: 0.8111 - acc: 0.6667\n",
      "epoch: 51 - MSE: 0.7618 - MSE_v: 0.8037 - acc: 0.6667\n",
      "epoch: 52 - MSE: 0.7573 - MSE_v: 0.7965 - acc: 0.6667\n",
      "epoch: 53 - MSE: 0.7529 - MSE_v: 0.7896 - acc: 0.6667\n",
      "epoch: 54 - MSE: 0.7487 - MSE_v: 0.7828 - acc: 0.6667\n",
      "epoch: 55 - MSE: 0.7446 - MSE_v: 0.7763 - acc: 0.6667\n",
      "epoch: 56 - MSE: 0.7407 - MSE_v: 0.7699 - acc: 0.6\n",
      "epoch: 57 - MSE: 0.7369 - MSE_v: 0.7636 - acc: 0.6\n",
      "epoch: 58 - MSE: 0.7332 - MSE_v: 0.7576 - acc: 0.6\n",
      "epoch: 59 - MSE: 0.7297 - MSE_v: 0.7517 - acc: 0.6\n",
      "epoch: 60 - MSE: 0.7263 - MSE_v: 0.746 - acc: 0.6\n",
      "epoch: 61 - MSE: 0.723 - MSE_v: 0.7404 - acc: 0.6\n",
      "epoch: 62 - MSE: 0.7198 - MSE_v: 0.7349 - acc: 0.6\n",
      "epoch: 63 - MSE: 0.7167 - MSE_v: 0.7296 - acc: 0.6\n",
      "epoch: 64 - MSE: 0.7137 - MSE_v: 0.7245 - acc: 0.6\n",
      "epoch: 65 - MSE: 0.7108 - MSE_v: 0.7195 - acc: 0.6\n",
      "epoch: 66 - MSE: 0.708 - MSE_v: 0.7146 - acc: 0.6\n",
      "epoch: 67 - MSE: 0.7053 - MSE_v: 0.7098 - acc: 0.6\n",
      "epoch: 68 - MSE: 0.7026 - MSE_v: 0.7052 - acc: 0.6\n",
      "epoch: 69 - MSE: 0.7001 - MSE_v: 0.7006 - acc: 0.6\n",
      "epoch: 70 - MSE: 0.6976 - MSE_v: 0.6962 - acc: 0.6\n",
      "epoch: 71 - MSE: 0.6952 - MSE_v: 0.6919 - acc: 0.6\n",
      "epoch: 72 - MSE: 0.6929 - MSE_v: 0.6877 - acc: 0.6\n",
      "epoch: 73 - MSE: 0.6906 - MSE_v: 0.6836 - acc: 0.6\n",
      "epoch: 74 - MSE: 0.6884 - MSE_v: 0.6796 - acc: 0.6\n",
      "epoch: 75 - MSE: 0.6862 - MSE_v: 0.6757 - acc: 0.6\n",
      "epoch: 76 - MSE: 0.6842 - MSE_v: 0.6719 - acc: 0.6\n",
      "epoch: 77 - MSE: 0.6821 - MSE_v: 0.6682 - acc: 0.6\n",
      "epoch: 78 - MSE: 0.6802 - MSE_v: 0.6646 - acc: 0.6\n",
      "epoch: 79 - MSE: 0.6782 - MSE_v: 0.6611 - acc: 0.6\n",
      "epoch: 80 - MSE: 0.6764 - MSE_v: 0.6576 - acc: 0.6\n",
      "epoch: 81 - MSE: 0.6746 - MSE_v: 0.6543 - acc: 0.6\n",
      "epoch: 82 - MSE: 0.6728 - MSE_v: 0.651 - acc: 0.6\n",
      "epoch: 83 - MSE: 0.6711 - MSE_v: 0.6478 - acc: 0.6\n",
      "epoch: 84 - MSE: 0.6694 - MSE_v: 0.6446 - acc: 0.6\n",
      "epoch: 85 - MSE: 0.6677 - MSE_v: 0.6416 - acc: 0.6\n",
      "epoch: 86 - MSE: 0.6661 - MSE_v: 0.6386 - acc: 0.6\n",
      "epoch: 87 - MSE: 0.6646 - MSE_v: 0.6357 - acc: 0.6\n",
      "epoch: 88 - MSE: 0.6631 - MSE_v: 0.6328 - acc: 0.6\n",
      "epoch: 89 - MSE: 0.6616 - MSE_v: 0.63 - acc: 0.6\n",
      "epoch: 90 - MSE: 0.6601 - MSE_v: 0.6273 - acc: 0.6\n",
      "epoch: 91 - MSE: 0.6587 - MSE_v: 0.6246 - acc: 0.6\n",
      "epoch: 92 - MSE: 0.6573 - MSE_v: 0.622 - acc: 0.6\n",
      "epoch: 93 - MSE: 0.656 - MSE_v: 0.6195 - acc: 0.6\n",
      "epoch: 94 - MSE: 0.6547 - MSE_v: 0.617 - acc: 0.6\n",
      "epoch: 95 - MSE: 0.6534 - MSE_v: 0.6145 - acc: 0.6\n",
      "epoch: 96 - MSE: 0.6521 - MSE_v: 0.6122 - acc: 0.6\n",
      "epoch: 97 - MSE: 0.6509 - MSE_v: 0.6098 - acc: 0.6\n",
      "epoch: 98 - MSE: 0.6497 - MSE_v: 0.6076 - acc: 0.6\n",
      "epoch: 99 - MSE: 0.6485 - MSE_v: 0.6053 - acc: 0.6667\n",
      "epoch: 100 - MSE: 0.6473 - MSE_v: 0.6031 - acc: 0.6667\n",
      "epoch: 101 - MSE: 0.6462 - MSE_v: 0.601 - acc: 0.6667\n",
      "epoch: 102 - MSE: 0.6451 - MSE_v: 0.5989 - acc: 0.6667\n",
      "epoch: 103 - MSE: 0.644 - MSE_v: 0.5969 - acc: 0.6667\n",
      "epoch: 104 - MSE: 0.6429 - MSE_v: 0.5949 - acc: 0.6667\n",
      "epoch: 105 - MSE: 0.6419 - MSE_v: 0.5929 - acc: 0.6667\n",
      "epoch: 106 - MSE: 0.6409 - MSE_v: 0.591 - acc: 0.6667\n",
      "epoch: 107 - MSE: 0.6399 - MSE_v: 0.5891 - acc: 0.6667\n",
      "epoch: 108 - MSE: 0.6389 - MSE_v: 0.5873 - acc: 0.6667\n",
      "epoch: 109 - MSE: 0.6379 - MSE_v: 0.5854 - acc: 0.6667\n",
      "epoch: 110 - MSE: 0.637 - MSE_v: 0.5837 - acc: 0.6667\n",
      "epoch: 111 - MSE: 0.6361 - MSE_v: 0.5819 - acc: 0.6667\n",
      "epoch: 112 - MSE: 0.6351 - MSE_v: 0.5802 - acc: 0.6667\n",
      "epoch: 113 - MSE: 0.6343 - MSE_v: 0.5786 - acc: 0.6667\n",
      "epoch: 114 - MSE: 0.6334 - MSE_v: 0.5769 - acc: 0.6667\n",
      "epoch: 115 - MSE: 0.6325 - MSE_v: 0.5753 - acc: 0.6667\n",
      "epoch: 116 - MSE: 0.6317 - MSE_v: 0.5738 - acc: 0.6667\n",
      "epoch: 117 - MSE: 0.6308 - MSE_v: 0.5722 - acc: 0.6667\n",
      "epoch: 118 - MSE: 0.63 - MSE_v: 0.5707 - acc: 0.6667\n",
      "epoch: 119 - MSE: 0.6292 - MSE_v: 0.5692 - acc: 0.6667\n",
      "epoch: 120 - MSE: 0.6284 - MSE_v: 0.5678 - acc: 0.6667\n",
      "epoch: 121 - MSE: 0.6277 - MSE_v: 0.5664 - acc: 0.6667\n",
      "epoch: 122 - MSE: 0.6269 - MSE_v: 0.565 - acc: 0.6667\n",
      "epoch: 123 - MSE: 0.6262 - MSE_v: 0.5636 - acc: 0.6667\n",
      "epoch: 124 - MSE: 0.6254 - MSE_v: 0.5622 - acc: 0.6667\n",
      "epoch: 125 - MSE: 0.6247 - MSE_v: 0.5609 - acc: 0.6667\n",
      "epoch: 126 - MSE: 0.624 - MSE_v: 0.5596 - acc: 0.6667\n",
      "epoch: 127 - MSE: 0.6233 - MSE_v: 0.5583 - acc: 0.6667\n",
      "epoch: 128 - MSE: 0.6226 - MSE_v: 0.5571 - acc: 0.6667\n",
      "epoch: 129 - MSE: 0.6219 - MSE_v: 0.5558 - acc: 0.6667\n",
      "epoch: 130 - MSE: 0.6212 - MSE_v: 0.5546 - acc: 0.6667\n",
      "epoch: 131 - MSE: 0.6206 - MSE_v: 0.5535 - acc: 0.6667\n",
      "epoch: 132 - MSE: 0.6199 - MSE_v: 0.5523 - acc: 0.6667\n",
      "epoch: 133 - MSE: 0.6193 - MSE_v: 0.5511 - acc: 0.6667\n",
      "epoch: 134 - MSE: 0.6187 - MSE_v: 0.55 - acc: 0.6667\n",
      "epoch: 135 - MSE: 0.618 - MSE_v: 0.5489 - acc: 0.6667\n",
      "epoch: 136 - MSE: 0.6174 - MSE_v: 0.5478 - acc: 0.6667\n",
      "epoch: 137 - MSE: 0.6168 - MSE_v: 0.5467 - acc: 0.6667\n",
      "epoch: 138 - MSE: 0.6162 - MSE_v: 0.5457 - acc: 0.6667\n",
      "epoch: 139 - MSE: 0.6157 - MSE_v: 0.5447 - acc: 0.6667\n",
      "epoch: 140 - MSE: 0.6151 - MSE_v: 0.5436 - acc: 0.7333\n",
      "epoch: 141 - MSE: 0.6145 - MSE_v: 0.5426 - acc: 0.7333\n",
      "epoch: 142 - MSE: 0.614 - MSE_v: 0.5417 - acc: 0.7333\n",
      "epoch: 143 - MSE: 0.6134 - MSE_v: 0.5407 - acc: 0.7333\n",
      "epoch: 144 - MSE: 0.6129 - MSE_v: 0.5397 - acc: 0.7333\n",
      "epoch: 145 - MSE: 0.6123 - MSE_v: 0.5388 - acc: 0.7333\n",
      "epoch: 146 - MSE: 0.6118 - MSE_v: 0.5379 - acc: 0.7333\n",
      "epoch: 147 - MSE: 0.6113 - MSE_v: 0.537 - acc: 0.7333\n",
      "epoch: 148 - MSE: 0.6107 - MSE_v: 0.5361 - acc: 0.7333\n",
      "epoch: 149 - MSE: 0.6102 - MSE_v: 0.5352 - acc: 0.7333\n",
      "epoch: 150 - MSE: 0.6097 - MSE_v: 0.5344 - acc: 0.7333\n",
      "epoch: 151 - MSE: 0.6092 - MSE_v: 0.5335 - acc: 0.7333\n",
      "epoch: 152 - MSE: 0.6087 - MSE_v: 0.5327 - acc: 0.7333\n",
      "epoch: 153 - MSE: 0.6083 - MSE_v: 0.5319 - acc: 0.7333\n",
      "epoch: 154 - MSE: 0.6078 - MSE_v: 0.531 - acc: 0.7333\n",
      "epoch: 155 - MSE: 0.6073 - MSE_v: 0.5302 - acc: 0.7333\n",
      "epoch: 156 - MSE: 0.6068 - MSE_v: 0.5295 - acc: 0.7333\n",
      "epoch: 157 - MSE: 0.6064 - MSE_v: 0.5287 - acc: 0.7333\n",
      "epoch: 158 - MSE: 0.6059 - MSE_v: 0.5279 - acc: 0.7333\n",
      "epoch: 159 - MSE: 0.6055 - MSE_v: 0.5272 - acc: 0.7333\n",
      "epoch: 160 - MSE: 0.605 - MSE_v: 0.5264 - acc: 0.7333\n",
      "epoch: 161 - MSE: 0.6046 - MSE_v: 0.5257 - acc: 0.7333\n",
      "epoch: 162 - MSE: 0.6042 - MSE_v: 0.525 - acc: 0.7333\n",
      "epoch: 163 - MSE: 0.6037 - MSE_v: 0.5243 - acc: 0.7333\n",
      "epoch: 164 - MSE: 0.6033 - MSE_v: 0.5236 - acc: 0.7333\n",
      "epoch: 165 - MSE: 0.6029 - MSE_v: 0.5229 - acc: 0.7333\n",
      "epoch: 166 - MSE: 0.6025 - MSE_v: 0.5222 - acc: 0.7333\n",
      "epoch: 167 - MSE: 0.6021 - MSE_v: 0.5215 - acc: 0.7333\n",
      "epoch: 168 - MSE: 0.6017 - MSE_v: 0.5209 - acc: 0.7333\n",
      "epoch: 169 - MSE: 0.6013 - MSE_v: 0.5202 - acc: 0.7333\n",
      "epoch: 170 - MSE: 0.6009 - MSE_v: 0.5196 - acc: 0.7333\n",
      "epoch: 171 - MSE: 0.6005 - MSE_v: 0.519 - acc: 0.7333\n",
      "epoch: 172 - MSE: 0.6001 - MSE_v: 0.5183 - acc: 0.7333\n",
      "epoch: 173 - MSE: 0.5997 - MSE_v: 0.5177 - acc: 0.7333\n",
      "epoch: 174 - MSE: 0.5994 - MSE_v: 0.5171 - acc: 0.7333\n",
      "epoch: 175 - MSE: 0.599 - MSE_v: 0.5165 - acc: 0.7333\n",
      "epoch: 176 - MSE: 0.5986 - MSE_v: 0.5159 - acc: 0.7333\n",
      "epoch: 177 - MSE: 0.5983 - MSE_v: 0.5154 - acc: 0.7333\n",
      "epoch: 178 - MSE: 0.5979 - MSE_v: 0.5148 - acc: 0.7333\n",
      "epoch: 179 - MSE: 0.5975 - MSE_v: 0.5142 - acc: 0.7333\n",
      "epoch: 180 - MSE: 0.5972 - MSE_v: 0.5137 - acc: 0.7333\n",
      "epoch: 181 - MSE: 0.5968 - MSE_v: 0.5131 - acc: 0.7333\n",
      "epoch: 182 - MSE: 0.5965 - MSE_v: 0.5126 - acc: 0.7333\n",
      "epoch: 183 - MSE: 0.5962 - MSE_v: 0.512 - acc: 0.7333\n",
      "epoch: 184 - MSE: 0.5958 - MSE_v: 0.5115 - acc: 0.7333\n",
      "epoch: 185 - MSE: 0.5955 - MSE_v: 0.511 - acc: 0.7333\n",
      "epoch: 186 - MSE: 0.5951 - MSE_v: 0.5105 - acc: 0.7333\n",
      "epoch: 187 - MSE: 0.5948 - MSE_v: 0.51 - acc: 0.7333\n",
      "epoch: 188 - MSE: 0.5945 - MSE_v: 0.5095 - acc: 0.7333\n",
      "epoch: 189 - MSE: 0.5942 - MSE_v: 0.509 - acc: 0.7333\n",
      "epoch: 190 - MSE: 0.5939 - MSE_v: 0.5085 - acc: 0.7333\n",
      "epoch: 191 - MSE: 0.5935 - MSE_v: 0.508 - acc: 0.7333\n",
      "epoch: 192 - MSE: 0.5932 - MSE_v: 0.5075 - acc: 0.8667\n",
      "epoch: 193 - MSE: 0.5929 - MSE_v: 0.5071 - acc: 0.8667\n",
      "epoch: 194 - MSE: 0.5926 - MSE_v: 0.5066 - acc: 0.8667\n",
      "epoch: 195 - MSE: 0.5923 - MSE_v: 0.5061 - acc: 0.8667\n",
      "epoch: 196 - MSE: 0.592 - MSE_v: 0.5057 - acc: 0.8667\n",
      "epoch: 197 - MSE: 0.5917 - MSE_v: 0.5052 - acc: 0.8667\n",
      "epoch: 198 - MSE: 0.5914 - MSE_v: 0.5048 - acc: 0.8667\n",
      "epoch: 199 - MSE: 0.5911 - MSE_v: 0.5043 - acc: 0.8667\n",
      "epoch: 200 - MSE: 0.5909 - MSE_v: 0.5039 - acc: 0.8667\n",
      "epoch: 201 - MSE: 0.5906 - MSE_v: 0.5035 - acc: 0.8667\n",
      "epoch: 202 - MSE: 0.5903 - MSE_v: 0.5031 - acc: 0.8667\n",
      "epoch: 203 - MSE: 0.59 - MSE_v: 0.5026 - acc: 0.8667\n",
      "epoch: 204 - MSE: 0.5897 - MSE_v: 0.5022 - acc: 0.8667\n",
      "epoch: 205 - MSE: 0.5895 - MSE_v: 0.5018 - acc: 0.8667\n",
      "epoch: 206 - MSE: 0.5892 - MSE_v: 0.5014 - acc: 0.8667\n",
      "epoch: 207 - MSE: 0.5889 - MSE_v: 0.501 - acc: 0.8667\n",
      "epoch: 208 - MSE: 0.5887 - MSE_v: 0.5006 - acc: 0.8667\n",
      "epoch: 209 - MSE: 0.5884 - MSE_v: 0.5002 - acc: 0.8667\n",
      "epoch: 210 - MSE: 0.5881 - MSE_v: 0.4999 - acc: 0.8667\n",
      "epoch: 211 - MSE: 0.5879 - MSE_v: 0.4995 - acc: 0.8667\n",
      "epoch: 212 - MSE: 0.5876 - MSE_v: 0.4991 - acc: 0.8667\n",
      "epoch: 213 - MSE: 0.5874 - MSE_v: 0.4987 - acc: 0.8667\n",
      "epoch: 214 - MSE: 0.5871 - MSE_v: 0.4984 - acc: 0.8667\n",
      "epoch: 215 - MSE: 0.5869 - MSE_v: 0.498 - acc: 0.8667\n",
      "epoch: 216 - MSE: 0.5866 - MSE_v: 0.4977 - acc: 0.8667\n",
      "epoch: 217 - MSE: 0.5864 - MSE_v: 0.4973 - acc: 0.8667\n",
      "epoch: 218 - MSE: 0.5861 - MSE_v: 0.4969 - acc: 0.8667\n",
      "epoch: 219 - MSE: 0.5859 - MSE_v: 0.4966 - acc: 0.8667\n",
      "epoch: 220 - MSE: 0.5857 - MSE_v: 0.4963 - acc: 0.8667\n",
      "epoch: 221 - MSE: 0.5854 - MSE_v: 0.4959 - acc: 0.8667\n",
      "epoch: 222 - MSE: 0.5852 - MSE_v: 0.4956 - acc: 0.8667\n",
      "epoch: 223 - MSE: 0.585 - MSE_v: 0.4953 - acc: 0.8667\n",
      "epoch: 224 - MSE: 0.5847 - MSE_v: 0.4949 - acc: 0.8667\n",
      "epoch: 225 - MSE: 0.5845 - MSE_v: 0.4946 - acc: 0.8667\n",
      "epoch: 226 - MSE: 0.5843 - MSE_v: 0.4943 - acc: 0.8667\n",
      "epoch: 227 - MSE: 0.5841 - MSE_v: 0.494 - acc: 0.8667\n",
      "epoch: 228 - MSE: 0.5838 - MSE_v: 0.4937 - acc: 0.8667\n",
      "epoch: 229 - MSE: 0.5836 - MSE_v: 0.4933 - acc: 0.8667\n",
      "epoch: 230 - MSE: 0.5834 - MSE_v: 0.493 - acc: 0.8667\n",
      "epoch: 231 - MSE: 0.5832 - MSE_v: 0.4927 - acc: 0.8667\n",
      "epoch: 232 - MSE: 0.583 - MSE_v: 0.4924 - acc: 0.8667\n",
      "epoch: 233 - MSE: 0.5828 - MSE_v: 0.4921 - acc: 0.8667\n",
      "epoch: 234 - MSE: 0.5825 - MSE_v: 0.4918 - acc: 0.8667\n",
      "epoch: 235 - MSE: 0.5823 - MSE_v: 0.4915 - acc: 0.8667\n",
      "epoch: 236 - MSE: 0.5821 - MSE_v: 0.4913 - acc: 0.8667\n",
      "epoch: 237 - MSE: 0.5819 - MSE_v: 0.491 - acc: 0.8667\n",
      "epoch: 238 - MSE: 0.5817 - MSE_v: 0.4907 - acc: 0.8667\n",
      "epoch: 239 - MSE: 0.5815 - MSE_v: 0.4904 - acc: 0.8667\n",
      "epoch: 240 - MSE: 0.5813 - MSE_v: 0.4901 - acc: 0.8667\n",
      "epoch: 241 - MSE: 0.5811 - MSE_v: 0.4899 - acc: 0.8667\n",
      "epoch: 242 - MSE: 0.5809 - MSE_v: 0.4896 - acc: 0.8667\n",
      "epoch: 243 - MSE: 0.5807 - MSE_v: 0.4893 - acc: 0.8667\n",
      "epoch: 244 - MSE: 0.5805 - MSE_v: 0.489 - acc: 0.8667\n",
      "epoch: 245 - MSE: 0.5803 - MSE_v: 0.4888 - acc: 0.8667\n",
      "epoch: 246 - MSE: 0.5802 - MSE_v: 0.4885 - acc: 0.8667\n",
      "epoch: 247 - MSE: 0.58 - MSE_v: 0.4883 - acc: 0.8667\n",
      "epoch: 248 - MSE: 0.5798 - MSE_v: 0.488 - acc: 0.8667\n",
      "epoch: 249 - MSE: 0.5796 - MSE_v: 0.4878 - acc: 0.8667\n",
      "epoch: 250 - MSE: 0.5794 - MSE_v: 0.4875 - acc: 0.8667\n",
      "epoch: 251 - MSE: 0.5792 - MSE_v: 0.4873 - acc: 0.8667\n",
      "epoch: 252 - MSE: 0.5791 - MSE_v: 0.487 - acc: 0.8667\n",
      "epoch: 253 - MSE: 0.5789 - MSE_v: 0.4868 - acc: 0.8667\n",
      "epoch: 254 - MSE: 0.5787 - MSE_v: 0.4865 - acc: 0.8667\n",
      "epoch: 255 - MSE: 0.5785 - MSE_v: 0.4863 - acc: 0.8667\n",
      "epoch: 256 - MSE: 0.5783 - MSE_v: 0.486 - acc: 0.8667\n",
      "epoch: 257 - MSE: 0.5782 - MSE_v: 0.4858 - acc: 0.8667\n",
      "epoch: 258 - MSE: 0.578 - MSE_v: 0.4856 - acc: 0.8667\n",
      "epoch: 259 - MSE: 0.5778 - MSE_v: 0.4853 - acc: 0.8667\n",
      "epoch: 260 - MSE: 0.5777 - MSE_v: 0.4851 - acc: 0.8667\n",
      "epoch: 261 - MSE: 0.5775 - MSE_v: 0.4849 - acc: 0.8667\n",
      "epoch: 262 - MSE: 0.5773 - MSE_v: 0.4847 - acc: 0.8667\n",
      "epoch: 263 - MSE: 0.5772 - MSE_v: 0.4844 - acc: 0.8667\n",
      "epoch: 264 - MSE: 0.577 - MSE_v: 0.4842 - acc: 0.8667\n",
      "epoch: 265 - MSE: 0.5768 - MSE_v: 0.484 - acc: 0.8667\n",
      "epoch: 266 - MSE: 0.5767 - MSE_v: 0.4838 - acc: 0.8667\n",
      "epoch: 267 - MSE: 0.5765 - MSE_v: 0.4836 - acc: 0.8667\n",
      "epoch: 268 - MSE: 0.5763 - MSE_v: 0.4834 - acc: 0.8667\n",
      "epoch: 269 - MSE: 0.5762 - MSE_v: 0.4832 - acc: 0.8667\n",
      "epoch: 270 - MSE: 0.576 - MSE_v: 0.483 - acc: 0.8667\n",
      "epoch: 271 - MSE: 0.5759 - MSE_v: 0.4827 - acc: 0.8667\n",
      "epoch: 272 - MSE: 0.5757 - MSE_v: 0.4825 - acc: 0.8667\n",
      "epoch: 273 - MSE: 0.5756 - MSE_v: 0.4823 - acc: 0.8667\n",
      "epoch: 274 - MSE: 0.5754 - MSE_v: 0.4821 - acc: 0.8667\n",
      "epoch: 275 - MSE: 0.5753 - MSE_v: 0.4819 - acc: 0.8667\n",
      "epoch: 276 - MSE: 0.5751 - MSE_v: 0.4817 - acc: 0.8667\n",
      "epoch: 277 - MSE: 0.575 - MSE_v: 0.4815 - acc: 0.8667\n",
      "epoch: 278 - MSE: 0.5748 - MSE_v: 0.4814 - acc: 0.8667\n",
      "epoch: 279 - MSE: 0.5747 - MSE_v: 0.4812 - acc: 0.8667\n",
      "epoch: 280 - MSE: 0.5745 - MSE_v: 0.481 - acc: 0.8667\n",
      "epoch: 281 - MSE: 0.5744 - MSE_v: 0.4808 - acc: 0.8667\n",
      "epoch: 282 - MSE: 0.5742 - MSE_v: 0.4806 - acc: 0.8667\n",
      "epoch: 283 - MSE: 0.5741 - MSE_v: 0.4804 - acc: 0.8667\n",
      "epoch: 284 - MSE: 0.5739 - MSE_v: 0.4802 - acc: 0.8667\n",
      "epoch: 285 - MSE: 0.5738 - MSE_v: 0.48 - acc: 0.8667\n",
      "epoch: 286 - MSE: 0.5737 - MSE_v: 0.4799 - acc: 0.8667\n",
      "epoch: 287 - MSE: 0.5735 - MSE_v: 0.4797 - acc: 0.8667\n",
      "epoch: 288 - MSE: 0.5734 - MSE_v: 0.4795 - acc: 0.8667\n",
      "epoch: 289 - MSE: 0.5733 - MSE_v: 0.4793 - acc: 0.8667\n",
      "epoch: 290 - MSE: 0.5731 - MSE_v: 0.4792 - acc: 0.8667\n",
      "epoch: 291 - MSE: 0.573 - MSE_v: 0.479 - acc: 0.8667\n",
      "epoch: 292 - MSE: 0.5729 - MSE_v: 0.4788 - acc: 0.8667\n",
      "epoch: 293 - MSE: 0.5727 - MSE_v: 0.4786 - acc: 0.8667\n",
      "epoch: 294 - MSE: 0.5726 - MSE_v: 0.4785 - acc: 0.8667\n",
      "epoch: 295 - MSE: 0.5725 - MSE_v: 0.4783 - acc: 0.8667\n",
      "epoch: 296 - MSE: 0.5723 - MSE_v: 0.4781 - acc: 0.8667\n",
      "epoch: 297 - MSE: 0.5722 - MSE_v: 0.478 - acc: 0.8667\n",
      "epoch: 298 - MSE: 0.5721 - MSE_v: 0.4778 - acc: 0.8667\n",
      "epoch: 299 - MSE: 0.5719 - MSE_v: 0.4776 - acc: 0.8667\n",
      "epoch: 300 - MSE: 0.5718 - MSE_v: 0.4775 - acc: 0.8667\n",
      "epoch: 301 - MSE: 0.5717 - MSE_v: 0.4773 - acc: 0.8667\n",
      "epoch: 302 - MSE: 0.5716 - MSE_v: 0.4772 - acc: 0.8667\n",
      "epoch: 303 - MSE: 0.5714 - MSE_v: 0.477 - acc: 0.8667\n",
      "epoch: 304 - MSE: 0.5713 - MSE_v: 0.4769 - acc: 0.8667\n",
      "epoch: 305 - MSE: 0.5712 - MSE_v: 0.4767 - acc: 0.8667\n",
      "epoch: 306 - MSE: 0.5711 - MSE_v: 0.4765 - acc: 0.8667\n",
      "epoch: 307 - MSE: 0.571 - MSE_v: 0.4764 - acc: 0.8667\n",
      "epoch: 308 - MSE: 0.5708 - MSE_v: 0.4762 - acc: 0.8667\n",
      "epoch: 309 - MSE: 0.5707 - MSE_v: 0.4761 - acc: 0.8667\n",
      "epoch: 310 - MSE: 0.5706 - MSE_v: 0.4759 - acc: 0.8667\n",
      "epoch: 311 - MSE: 0.5705 - MSE_v: 0.4758 - acc: 0.8667\n",
      "epoch: 312 - MSE: 0.5704 - MSE_v: 0.4757 - acc: 0.8667\n",
      "epoch: 313 - MSE: 0.5702 - MSE_v: 0.4755 - acc: 0.8667\n",
      "epoch: 314 - MSE: 0.5701 - MSE_v: 0.4754 - acc: 0.8667\n",
      "epoch: 315 - MSE: 0.57 - MSE_v: 0.4752 - acc: 0.8667\n",
      "epoch: 316 - MSE: 0.5699 - MSE_v: 0.4751 - acc: 0.8667\n",
      "epoch: 317 - MSE: 0.5698 - MSE_v: 0.4749 - acc: 0.8667\n",
      "epoch: 318 - MSE: 0.5697 - MSE_v: 0.4748 - acc: 0.8667\n",
      "epoch: 319 - MSE: 0.5696 - MSE_v: 0.4747 - acc: 0.8667\n",
      "epoch: 320 - MSE: 0.5695 - MSE_v: 0.4745 - acc: 0.8667\n",
      "epoch: 321 - MSE: 0.5693 - MSE_v: 0.4744 - acc: 0.8667\n",
      "epoch: 322 - MSE: 0.5692 - MSE_v: 0.4743 - acc: 0.8667\n",
      "epoch: 323 - MSE: 0.5691 - MSE_v: 0.4741 - acc: 0.8667\n",
      "epoch: 324 - MSE: 0.569 - MSE_v: 0.474 - acc: 0.8667\n",
      "epoch: 325 - MSE: 0.5689 - MSE_v: 0.4739 - acc: 0.8667\n",
      "epoch: 326 - MSE: 0.5688 - MSE_v: 0.4737 - acc: 0.8667\n",
      "epoch: 327 - MSE: 0.5687 - MSE_v: 0.4736 - acc: 0.8667\n",
      "epoch: 328 - MSE: 0.5686 - MSE_v: 0.4735 - acc: 0.8667\n",
      "epoch: 329 - MSE: 0.5685 - MSE_v: 0.4733 - acc: 0.8667\n",
      "epoch: 330 - MSE: 0.5684 - MSE_v: 0.4732 - acc: 0.8667\n",
      "epoch: 331 - MSE: 0.5683 - MSE_v: 0.4731 - acc: 0.8667\n",
      "epoch: 332 - MSE: 0.5682 - MSE_v: 0.473 - acc: 0.8667\n",
      "epoch: 333 - MSE: 0.5681 - MSE_v: 0.4728 - acc: 0.8667\n",
      "epoch: 334 - MSE: 0.568 - MSE_v: 0.4727 - acc: 0.8667\n",
      "epoch: 335 - MSE: 0.5679 - MSE_v: 0.4726 - acc: 0.8667\n",
      "epoch: 336 - MSE: 0.5678 - MSE_v: 0.4725 - acc: 0.8667\n",
      "epoch: 337 - MSE: 0.5677 - MSE_v: 0.4723 - acc: 0.8667\n",
      "epoch: 338 - MSE: 0.5676 - MSE_v: 0.4722 - acc: 0.8667\n",
      "epoch: 339 - MSE: 0.5675 - MSE_v: 0.4721 - acc: 0.8667\n",
      "epoch: 340 - MSE: 0.5674 - MSE_v: 0.472 - acc: 0.8667\n",
      "epoch: 341 - MSE: 0.5673 - MSE_v: 0.4719 - acc: 0.8667\n",
      "epoch: 342 - MSE: 0.5672 - MSE_v: 0.4718 - acc: 0.8667\n",
      "epoch: 343 - MSE: 0.5671 - MSE_v: 0.4716 - acc: 0.8667\n",
      "epoch: 344 - MSE: 0.567 - MSE_v: 0.4715 - acc: 0.8667\n",
      "epoch: 345 - MSE: 0.5669 - MSE_v: 0.4714 - acc: 0.8667\n",
      "epoch: 346 - MSE: 0.5668 - MSE_v: 0.4713 - acc: 0.8667\n",
      "epoch: 347 - MSE: 0.5667 - MSE_v: 0.4712 - acc: 0.8667\n",
      "epoch: 348 - MSE: 0.5666 - MSE_v: 0.4711 - acc: 0.8667\n",
      "epoch: 349 - MSE: 0.5665 - MSE_v: 0.471 - acc: 0.8667\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, 350, 0.001, 1, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 2, 0, 2, 1, 1, 1, 2, 0, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model2.predict(X_valid)\n",
    "pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 2, 0, 2, 0, 2, 1, 1, 1, 2, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666746139526"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.accuracy(Y_valid, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
