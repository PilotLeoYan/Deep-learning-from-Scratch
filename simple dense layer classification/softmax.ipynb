{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.config import set_floatx\n",
    "import torch\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "\n",
    "set_floatx('float64')\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO\\n- cambiar la definición de softmax\\n-- cambiar sumatoria ^q => ^Q\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO\n",
    "- cambiar la definición de softmax\n",
    "-- cambiar sumatoria ^q => ^Q\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(a_true, a_pred):\n",
    "    e = np.abs(a_true.numpy() - a_pred.numpy()) / np.abs(a_true.numpy())\n",
    "    return np.mean(e) * 100\n",
    "\n",
    "def error_g(a_true, a_pred):\n",
    "    e = np.abs(a_true - a_pred.numpy()) / np.abs(a_true)\n",
    "    return np.mean(e) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one example/entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0000, -3.0000,  4.0000, -1.5000, -4.5000]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, Q = 100, 5\n",
    "X = torch.randint(-10, 11, (1, Q)) / 2\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax\n",
    "$$\n",
    "\\text{softmax}(\\boldsymbol{o})_j = \\frac{\\exp(o)_j}{\\sum_{k=1}^{q} \\exp(o_k)}\n",
    "$$\n",
    "where $q$ is the number of clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.67922497e-01, 6.64113473e-04, 7.28288856e-01, 2.97635009e-03,\n",
       "        1.48183746e-04]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOFTMAX = Softmax()\n",
    "tf_soft = SOFTMAX(X)\n",
    "tf_soft.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.67922497e-01, 6.64113473e-04, 7.28288856e-01, 2.97635009e-03,\n",
       "        1.48183746e-04]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_1(x):\n",
    "    exp = torch.exp(x)\n",
    "    return exp / exp.sum()\n",
    "\n",
    "my_soft = softmax_1(X)\n",
    "my_soft.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2886447193544136e-14"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error\n",
    "error(tf_soft, my_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiples examples/entrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randint(-10, 11, (M, Q)) / 2\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_soft = SOFTMAX(X)\n",
    "tf_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    exp = torch.exp(x)\n",
    "    return exp / exp.sum(dim=-1, keepdims=True)\n",
    "\n",
    "my_soft = softmax(X)\n",
    "my_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1781507582645494e-14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# error\n",
    "error(tf_soft, my_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial \\text{softmax}(o)_j}{\\partial \\boldsymbol{o}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 1)\n",
      "(5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00406404],\n",
       "       [-0.00406404],\n",
       "       [-0.00055001],\n",
       "       [ 0.14326054],\n",
       "       [-0.13458246]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_Q = 3\n",
    "\n",
    "X = torch.randint(-10, 11, (1, Q)) / 2\n",
    "\n",
    "def soft(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp[0,NUM_Q] / np.sum(exp, axis=-1)\n",
    "\n",
    "gradient = jacobian(soft)\n",
    "grad = gradient(X.numpy()).T #.T because its numerator layout\n",
    "print(grad.shape)\n",
    "grad = grad[:,:,0]\n",
    "print(grad.shape)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial \\boldsymbol{o}} \\in (1, Q) \\times (1) \\Leftrightarrow (Q,1)\n",
    "$$\n",
    "because $\\boldsymbol{o} \\in (1, Q)$ and $\\text{softmax}(\\boldsymbol{o})_j \\in \\mathbb{R}$\n",
    "<br> <br>\n",
    "Then, the jacobian in **Denominator layout** is:\n",
    "$$\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial \\boldsymbol{o}} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_1} & \\cdots & \\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_j} & \\cdots & \\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_Q}\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "there are two diferente types of the derivatives:\n",
    "1. $\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_{i=j}}$\n",
    "\n",
    "2. $\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_{i\\neq j}}$\n",
    "\n",
    "$\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_{i=j}}$\n",
    "$$\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_{i=j}} = \\text{softmax}(\\boldsymbol{o})_j\\left (1-\\text{softmax}(\\boldsymbol{o})_j \\right)\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_{i\\neq j}}$\n",
    "$$\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial o_{i\\neq j}} = -\\text{softmax}(\\boldsymbol{o})_j \\text{softmax}(\\boldsymbol{o})_i\n",
    "$$\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial \\boldsymbol{o}} = \\begin{bmatrix}\n",
    "-\\text{softmax}(\\boldsymbol{o})_j \\text{softmax}(\\boldsymbol{o})_{1} & \\cdots & \\text{softmax}(\\boldsymbol{o})_j\\left (1-\\text{softmax}(\\boldsymbol{o})_j \\right) & \\cdots & -\\text{softmax}(\\boldsymbol{o})_j \\text{softmax}(\\boldsymbol{o})_Q\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "Vectorized form:\n",
    "$$\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol{o})_j}{\\partial \\boldsymbol{o}} = \\text{softmax}(\\boldsymbol{o})_j \\odot \\begin{bmatrix}\n",
    "-\\text{softmax}(\\boldsymbol{o})_{1} & \\cdots & 1-\\text{softmax}(\\boldsymbol{o})_j & \\cdots & -\\text{softmax}(\\boldsymbol{o})_Q\n",
    "\\end{bmatrix}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00406404],\n",
       "       [-0.00406404],\n",
       "       [-0.00055001],\n",
       "       [ 0.14326054],\n",
       "       [-0.13458246]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_der_1(x, j):\n",
    "    soft = softmax(x).T\n",
    "    soft_j = soft[j,0].item() # get softmax(o)_j\n",
    "    soft *= -1 \n",
    "    soft[j,0] += 1\n",
    "    return soft_j * soft\n",
    "\n",
    "my_gradient = softmax_der_1(X, NUM_Q)\n",
    "my_gradient.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.047897635964624e-14"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# error\n",
    "error_g(grad, my_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial \\text{softmax}(\\boldsymbol{o})}{\\partial \\boldsymbol{o}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del gradient, grad, X, my_gradient\n",
    "\n",
    "X = torch.randint(-10, 11, (1, Q)) / 2\n",
    "\n",
    "def soft_numpy(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "gradient = jacobian(soft_numpy)\n",
    "grad = gradient(X.numpy()).T\n",
    "print(grad.shape)\n",
    "\n",
    "grad = grad[:,0,:,0]\n",
    "grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using jacobian matrix as **Denominator layout** and the derivatives that we found, then this derivative is:\n",
    "$$\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{o})}{\\partial \\boldsymbol{o}} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol o)_1}{\\partial \\boldsymbol o} & \\frac{\\partial \\text{soft}(\\boldsymbol o)_2}{\\partial \\boldsymbol o} & \\cdots & \\frac{\\partial \\text{soft}(\\boldsymbol o)_Q}{\\partial \\boldsymbol o}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{o})_1}{\\partial \\boldsymbol{o}_1} & \\frac{\\partial \\text{soft}(\\boldsymbol{o})_2}{\\partial \\boldsymbol{o}_1} & \\cdots  & \\frac{\\partial \\text{soft}(\\boldsymbol{o})_Q}{\\partial \\boldsymbol{o}_1}\\\\ \n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{o})_1}{\\partial \\boldsymbol{o}_2} & \\frac{\\partial \\text{soft}(\\boldsymbol{o})_2}{\\partial \\boldsymbol{o}_2} & \\cdots & \\frac{\\partial \\text{soft}(\\boldsymbol{o})_Q}{\\partial \\boldsymbol{o}_2}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{o})_1}{\\partial \\boldsymbol{o}_Q} & \\frac{\\partial \\text{soft}(\\boldsymbol{o})_2}{\\partial \\boldsymbol{o}_Q} & \\cdots & \\frac{\\partial \\text{soft}(\\boldsymbol{o})_Q}{\\partial \\boldsymbol{o}_Q}\n",
    "\\end{bmatrix} \\in (Q,Q)\n",
    "$$\n",
    "**Remark:** we abbreviate \"softmax\" to \"soft\". <br>\n",
    "And using this derivatives:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol o)_j}{\\partial o_{i=j}} &= \\text{soft}(\\boldsymbol o)_j\\left (1-\\text{soft}(\\boldsymbol o)_j \\right) \\\\\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol o)_j}{\\partial o_{i\\neq j}} &= -\\text{soft}(\\boldsymbol o)_j \\text{soft}(\\boldsymbol o)_i\n",
    "\\end{align}\n",
    "$$\n",
    "Thefore:\n",
    "$$\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{o})}{\\partial \\boldsymbol{o}} = \\begin{bmatrix}\n",
    "\\text{soft}(\\boldsymbol{o})_1(1-\\text{soft}(\\boldsymbol{o})_1) & -\\text{soft}(\\boldsymbol{o})_1\\text{soft}(\\boldsymbol{o})_2 & \\cdots & -\\text{soft}(\\boldsymbol{o})_1\\text{soft}(\\boldsymbol{o})_Q\\\\ \n",
    "-\\text{soft}(\\boldsymbol{o})_2\\text{soft}(\\boldsymbol{o})_1 & \\text{soft}(\\boldsymbol{o})_2(1-\\text{soft}(\\boldsymbol{o})_2) & \\cdots & -\\text{soft}(\\boldsymbol{o})_2\\text{soft}(\\boldsymbol{o})_Q\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "-\\text{soft}(\\boldsymbol{o})_Q\\text{soft}(\\boldsymbol{o})_1 & -\\text{soft}(\\boldsymbol{o})_Q\\text{soft}(\\boldsymbol{o})_2 & \\cdots & \\text{soft}(\\boldsymbol{o})_Q(1-\\text{soft}(\\boldsymbol{o})_Q)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Vectorized form:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{o})}{\\partial \\boldsymbol{o}} &= \\begin{bmatrix}\n",
    "\\text{soft}(\\boldsymbol{o})_1 & 0 & \\cdots & 0\\\\ \n",
    "0 & \\text{soft}(\\boldsymbol{o})_2 & \\cdots & 0\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "0 & 0 & \\cdots & \\text{soft}(\\boldsymbol{o})_Q\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "\\text{soft}(\\boldsymbol{o})_1^2 & \\text{soft}(\\boldsymbol{o})_1\\text{soft}(\\boldsymbol{o})_2 & \\cdots & \\text{soft}(\\boldsymbol{o})_1\\text{soft}(\\boldsymbol{o})_Q\\\\ \n",
    "\\text{soft}(\\boldsymbol{o})_2\\text{soft}(\\boldsymbol{o})_1 & \\text{soft}(\\boldsymbol{o})_2^2 & \\cdots & \\text{soft}(\\boldsymbol{o})_2\\text{soft}(\\boldsymbol{o})_Q\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "\\text{soft}(\\boldsymbol{o})_Q\\text{soft}(\\boldsymbol{o})_1 & \\text{soft}(\\boldsymbol{o})_Q\\text{soft}(\\boldsymbol{o})_2 & \\cdots & \\text{soft}(\\boldsymbol{o})_Q^2\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\text{diag}(\\text{soft}(\\boldsymbol{o})) - \\text{soft}(\\boldsymbol{o})^T\\text{soft}(\\boldsymbol{o})\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.77839290e-02, -3.64423465e-06, -3.28043549e-04,\n",
       "        -6.58893081e-03, -1.08633104e-02],\n",
       "       [-3.64423465e-06,  2.01165356e-04, -3.64423465e-06,\n",
       "        -7.31964096e-05, -1.20680477e-04],\n",
       "       [-3.28043549e-04, -3.64423465e-06,  1.77839290e-02,\n",
       "        -6.58893081e-03, -1.08633104e-02],\n",
       "       [-6.58893081e-03, -7.31964096e-05, -6.58893081e-03,\n",
       "         2.31446480e-01, -2.18195422e-01],\n",
       "       [-1.08633104e-02, -1.20680477e-04, -1.08633104e-02,\n",
       "        -2.18195422e-01,  2.40042723e-01]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7784e-02, -3.6442e-06, -3.2804e-04, -6.5889e-03, -1.0863e-02],\n",
       "        [-3.6442e-06,  2.0117e-04, -3.6442e-06, -7.3196e-05, -1.2068e-04],\n",
       "        [-3.2804e-04, -3.6442e-06,  1.7784e-02, -6.5889e-03, -1.0863e-02],\n",
       "        [-6.5889e-03, -7.3196e-05, -6.5889e-03,  2.3145e-01, -2.1820e-01],\n",
       "        [-1.0863e-02, -1.2068e-04, -1.0863e-02, -2.1820e-01,  2.4004e-01]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_der_2(x):\n",
    "    s = softmax(x)[0,:]\n",
    "    return torch.diag(s) - torch.outer(s, s)\n",
    "\n",
    "my_gradient = softmax_der_2(X)\n",
    "my_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0853235392070533e-14"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_g(grad, my_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\mathrm{d} \\text{softmax}(\\boldsymbol{O})}{\\mathrm{d} \\boldsymbol{O}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{O} \\in (M,Q)\n",
    "$$\n",
    "where $M$ is number of examples/entrys. Then softmax function is:\n",
    "$$\n",
    "\\text{soft}(\\boldsymbol{O}) = \\begin{bmatrix}\n",
    "\\text{soft}(\\boldsymbol{O}_{1,:})\\\\ \n",
    "\\text{soft}(\\boldsymbol{O}_{2,:})\\\\ \n",
    "\\vdots\\\\ \n",
    "\\text{soft}(\\boldsymbol{O}_{M,:})\n",
    "\\end{bmatrix} \\in (M,Q)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\boldsymbol{O}_{p,:} = \\begin{bmatrix}\n",
    "O_{p,1} & O_{p,2} & \\cdots & O_{p,Q}\n",
    "\\end{bmatrix} \\in (1,Q)\n",
    "$$\n",
    "for all $p=1,...,M$ <br>\n",
    "therefore\n",
    "$$\n",
    "\\text{soft}(\\boldsymbol{O}_{p,:}) = \\begin{bmatrix}\n",
    "\\text{soft}(\\boldsymbol{O}_{p,:})_1 & \\text{soft}(\\boldsymbol{O}_{p,:})_2 & \\cdots & \\text{soft}(\\boldsymbol{O}_{p,:})_Q\n",
    "\\end{bmatrix}\\in (1,Q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "del gradient, grad, X, my_gradient\n",
    "\n",
    "X = torch.randint(-10, 11, (M, Q)) / 2\n",
    "\n",
    "def soft_numpy(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "gradient = jacobian(soft_numpy) #this uses numerator layout\n",
    "grad = gradient(X.numpy())\n",
    "print(grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need know its shape:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\mathrm{d} {\\color{Cyan} \\text{soft}(\\boldsymbol{O})}}{\\mathrm{d} {\\color{Orange} \\boldsymbol{O}}} &\\in {\\color{Orange} (M,Q)} \\times {\\color{Cyan} (M,Q)}\\\\\n",
    "\\frac{\\partial {\\color{Cyan} \\text{soft}(\\boldsymbol{O}_{p,:})}}{\\partial {\\color{Orange} \\boldsymbol{O}}} &\\in {\\color{Orange} (M,Q)} \\times {\\color{Cyan} (1,Q)} \\\\\n",
    "\\frac{\\partial {\\color{Cyan} \\text{soft}(\\boldsymbol{O}_{p,:})}}{\\partial {\\color{Orange} \\boldsymbol{O}_{q,:}}} &\\in {\\color{Orange} (1,Q)} \\times {\\color{Cyan} (1,Q)} \\Leftrightarrow (Q,Q)\n",
    "\\end{align}\n",
    "$$\n",
    "for all $p,q = 1, ..., M$. The last derivative $\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q,:}}$ is similar to our $\\frac{\\partial \\text{soft}(\\boldsymbol{o})}{\\partial \\boldsymbol{o}}$ but with some extra axis. <br>\n",
    "Then, we can use jacobian as **Denominator layout** like:\n",
    "$$\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{1,:}} & \\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{2,:}} & \\cdots & \\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{M,:}}\n",
    "\\end{bmatrix}^T \\in (M,Q) \\times (1,Q)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\mathrm{d} \\text{soft}(\\boldsymbol{O})}{\\mathrm{d} \\boldsymbol{O}} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{1,:})}{\\partial \\boldsymbol{O}} & \\frac{\\partial \\text{soft}(\\boldsymbol{O}_{2,:})}{\\partial \\boldsymbol{O}} & \\cdots & \\frac{\\partial \\text{soft}(\\boldsymbol{O}_{M,:})}{\\partial \\boldsymbol{O}}\n",
    "\\end{bmatrix} \\in (M,Q) \\times (M,Q)\n",
    "$$\n",
    "for all $p,q = 1, ..., M$. <br>\n",
    "But, how to compute $\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q,:}}$? There are two case:\n",
    "1. $\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q=p,:}}$\n",
    "\n",
    "2. $\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q \\neq p,:}}$\n",
    "\n",
    "$\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q=p,:}}$\n",
    "$$\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q=p,:}}=\\text{diag}(\\text{soft}(\\boldsymbol{O}_{p,:})) - \\text{soft}(\\boldsymbol{O}_{p,:})^T\\text{soft}(\\boldsymbol{O}_{p,:})\n",
    "$$\n",
    "$\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q \\neq p,:}}$\n",
    "$$\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q \\neq p,:}}=\\boldsymbol{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5, 100, 5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_der_3(x):\n",
    "    m, q_ = x.shape\n",
    "    out = torch.zeros((m,q_,m,q_))\n",
    "    for p in range(m):\n",
    "        for q in range(m):\n",
    "            if p == q:\n",
    "                out[p,:,q,:] = softmax_der_2(x[None,p,:])\n",
    "            #else: out[p,:,q,:] is zero by default\n",
    "    return out\n",
    "\n",
    "my_gradient = softmax_der_3(X)\n",
    "my_gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.963567018821016e-16"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 1e-32 to avoid division by zero\n",
    "error_g(grad + 1e-32, my_gradient + 1e-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5, 100, 5])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_der_4(x):\n",
    "    # a bit better performance that softmax_der_3\n",
    "    m, q_ = x.shape\n",
    "    out = torch.zeros((m,q_,m,q_))\n",
    "    for p in range(m):\n",
    "        out[p,:,p,:] = softmax_der_2(x[None,p,:])\n",
    "    return out\n",
    "\n",
    "my_gradient = softmax_der_4(X)\n",
    "my_gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.963567018821016e-16"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 1e-32 to avoid division by zero\n",
    "error_g(grad + 1e-32, my_gradient + 1e-32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{o}_i \\in \\mathbb{R} \\\\\n",
    "\\text{softmax}(\\boldsymbol{o})_j \\in \\mathbb{R}\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "then\n",
    "$$\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol o)_j}{\\partial o_{i=j}} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Derivative for this case $i=j$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol o)_j}{\\partial o_{i=j}} &= \\frac{\\partial}{\\partial o_j} \\left(\\frac{\\exp(o_j)}{\\sum_{k=1}^{q} \\exp{o_i}} \\right) \\\\\n",
    "&= \\frac{\\frac{\\partial}{\\partial o_j}(\\exp{(o_j)}) \\sum_{k=1}^{q} \\exp{(o_k)} -\\exp{(o_j)} \\frac{\\partial}{\\partial o_j} (\\sum_{k=1}^{q} \\exp{(o_k)})}{\\left( \\sum_{k=1}^{q} \\exp{(o_k)} \\right)^2} \\\\\n",
    "&= \\frac{\\exp{(o_j)} (\\sum_{k=1}^{q} \\exp{(o_k)}) -\\exp{(o_j)}^2}{\\left( \\sum_{k=1}^{q} \\exp{(o_k)} \\right)^2} \\\\\n",
    "&= \\frac{\\exp{(o_j)} \\left( (\\sum_{k=1}^{q} \\exp{(o_k)}) -\\exp{(o_j)} \\right)}{\\left( \\sum_{k=1}^{q} \\exp{(o_k)} \\right)^2} \\\\\n",
    "&= \\frac{\\exp{(o_j)}}{\\sum_{k=1}^{q} \\exp{(o_k)}} \\left( \\frac{\\sum_{k=1}^{q} \\exp{(o_k)} -\\exp{(o_j)}}{\\sum_{k=1}^{q} \\exp{(o_k)}} \\right)\\\\\n",
    "&= \\text{softmax}(o)_j \\left(\\frac{\\sum_{k=1}^{q} \\exp{(o_k)}}{\\sum_{k=1}^{q} \\exp{(o_k)}} -\\frac{\\exp{(o_j)}}{\\sum_{k=1}^{q} \\exp{(o_k)}}\\right)\\\\\n",
    "&= \\text{softmax}(o)_j \\left(1 -\\frac{\\exp{(o_j)}}{\\sum_{k=1}^{q} \\exp{(o_k)}}\\right)\\\\\n",
    "&= \\text{softmax}(\\boldsymbol o)_j\\left (1-\\text{softmax}(\\boldsymbol o)_j \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "Derivative for this case $i\\neq j$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{softmax}(\\boldsymbol o)_j}{\\partial o_{i\\neq j}} &= \\frac{\\partial}{\\partial o_i} \\left(\\frac{\\exp(o_j)}{\\sum_{k=1}^{q} \\exp{o_i}} \\right) \\\\\n",
    "&= \\frac{\\frac{\\partial}{\\partial o_i}(\\exp{(o_j)}) \\sum_{k=1}^{q} \\exp{(o_k)} -\\exp{(o_j)} \\frac{\\partial}{\\partial o_i} (\\sum_{k=1}^{q} \\exp{(o_k)})}{\\left( \\sum_{k=1}^{q} \\exp{(o_k)} \\right)^2} \\\\\n",
    "&= -\\frac{\\exp{(o_j)} \\exp{(o_i)}}{\\left( \\sum_{k=1}^{q} \\exp{(o_k)} \\right)^2} \\\\\n",
    "&= -\\frac{\\exp{(o_j)}}{\\sum_{k=1}^{q} \\exp{(o_k)}} \\frac{\\exp{(o_i)}}{\\sum_{k=1}^{q} \\exp{(o_k)}} \\\\\n",
    "&= -\\text{softmax}(\\boldsymbol o)_j \\text{softmax}(\\boldsymbol o)_i\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{O} \\in (M,Q) \\\\\n",
    "\\boldsymbol{O}_{p,:} = \\begin{bmatrix}\n",
    "O_{p,1} & O_{p,2} & \\cdots & O_{p,Q}\n",
    "\\end{bmatrix} \\in (1,Q)\\\\\n",
    "\\text{soft}(\\boldsymbol{O}_{p,:}) = \\begin{bmatrix}\n",
    "\\text{soft}(O_{p,:})_1 & \\text{soft}(O_{p,:})_2 & \\cdots & \\text{soft}(O_{p,:})_Q\n",
    "\\end{bmatrix} \\in (1,Q)\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q,:}} \\in (1,Q) \\times (1,Q) \\Leftrightarrow (Q,Q)\n",
    "$$\n",
    "for all $p,q = 1, ..., M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q,:}} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{q,1}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{q,1}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{q,1}} \\\\\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{q,2}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{q,2}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{q,2}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{q,Q}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{q,Q}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{q,Q}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "for case $q=p$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q=p,:}} &= \\begin{bmatrix}\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{p,1}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{p,1}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{p,1}} \\\\\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{p,2}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{p,2}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{p,2}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{p,Q}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{p,Q}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{p,Q}} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\text{soft}(O_{p,:})_1(1-\\text{soft}(O_{p,:})_1) & -\\text{soft}(O_{p,:})_1\\text{soft}(O_{p,:})_2 & \\cdots & -\\text{soft}(O_{p,:})_1\\text{soft}(O_{p,:})_Q \\\\\n",
    "-\\text{soft}(O_{p,:})_2\\text{soft}(O_{p,:})_1 & \\text{soft}(O_{p,:})_2(1-\\text{soft}(O_{p,:})_2) & \\cdots & -\\text{soft}(O_{p,:})_2\\text{soft}(O_{p,:})_Q \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-\\text{soft}(O_{p,:})_Q\\text{soft}(O_{p,:})_1 & -\\text{soft}(O_{p,:})_Q\\text{soft}(O_{p,:})_2 & \\cdots & \\text{soft}(O_{p,:})_Q(1-\\text{soft}(O_{p,:})_Q) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\text{soft}(O_{p,:})_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\text{soft}(O_{p,:})_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\text{soft}(O_{p,:})_Q\n",
    "\\end{bmatrix} -\\\\\n",
    "& \\begin{bmatrix}\n",
    "\\text{soft}(O_{p,:})_1^2 & \\text{soft}(O_{p,:})_1\\text{soft}(O_{p,:})_2 & \\cdots & \\text{soft}(O_{p,:})_1\\text{soft}(O_{p,:})_Q \\\\\n",
    "\\text{soft}(O_{p,:})_2\\text{soft}(O_{p,:})_1 & \\text{soft}(O_{p,:})_2^2 & \\cdots & \\text{soft}(O_{p,:})_2\\text{soft}(O_{p,:})_Q \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{soft}(O_{p,:})_Q\\text{soft}(O_{p,:})_1 & \\text{soft}(O_{p,:})_Q\\text{soft}(O_{p,:})_2 & \\cdots & \\text{soft}(O_{p,:})_Q^2 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\text{diag}(\\text{soft}(\\boldsymbol{O}_{p,:})) -\\text{soft}(\\boldsymbol{O}_{p,:})^T \\text{soft}(\\boldsymbol{O}_{p,:})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for case $q \\neq p$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{soft}(\\boldsymbol{O}_{p,:})}{\\partial \\boldsymbol{O}_{q\\neq p,:}} &= \\begin{bmatrix}\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{q,1}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{q,1}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{q,1}} \\\\\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{q,2}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{q,2}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{q,2}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\text{soft}(O_{p,:})_1}{\\partial O_{q,Q}} & \\frac{\\partial \\text{soft}(O_{p,:})_2}{\\partial O_{q,Q}} & \\cdots & \\frac{\\partial \\text{soft}(O_{p,:})_Q}{\\partial O_{q,Q}} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\boldsymbol{0}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "example, if $p=1$ and $q=2$:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
