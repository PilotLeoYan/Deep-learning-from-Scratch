{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.config.set_floatx('float64')\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "$$\n",
    "\\mathbf{X} \\in \\mathbb{R}^{M,N} \\\\\n",
    "\\mathbf{y} \\in \\mathbb{R}^{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(100, 1)\n",
      "0.32968640688761863\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M, N, U = 100, 4, 1\n",
    "TRUE_B = random.random()\n",
    "X, Y, TRUE_W = make_regression(n_samples=M, n_features=N, n_targets=U, n_informative=N-1, bias=TRUE_B, noise=1, coef=True)\n",
    "\n",
    "if U == 1: Y = Y.reshape((-1, 1))\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "print(TRUE_B)\n",
    "print(TRUE_W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights\n",
    "Parameters trainables:\n",
    "$$\n",
    "\\mathbf{w} \\in \\mathbb{R}^{N} \\\\\n",
    "b \\in \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self, number_features : int):\n",
    "        self.w = torch.randn(number_features, 1)\n",
    "        self.b = torch.randn(1)\n",
    "\n",
    "    def copyParams(self, tf_model):\n",
    "        self.w.copy_(torch.tensor(tf_model.weights[0].numpy()))\n",
    "        self.b.copy_(torch.tensor(tf_model.weights[1].numpy()))\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weigthed sum:\n",
    "$$\n",
    "\\mathbf{\\hat{y}} \\left( \\mathbf{X} \\right) = \\mathbf{X} \\mathbf{w} + b\\\\\n",
    "\\mathbf{\\hat{y}} : \\mathbb{R}^{M,N} \\rightarrow \\mathbb{R}^{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def predict(self, x):\n",
    "    return torch.tensor(x) @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE\n",
    "Loss function: Mean Squared Error\n",
    "$$\n",
    "L\\left ( \\mathbf{\\hat{y}} \\right ) = \\frac{1}{M} \\sum_{i=1}^{M} \\left ( \\mathbf{\\hat{y}}_i - \\mathbf{y}_i \\right )^{2} + \\lambda \\left \\| \\boldsymbol{w}\\right \\|_2^2 \\\\\n",
    "L : \\mathbb{R}^{M} \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Vectorized form:\n",
    "$$\n",
    "\\begin{align}\n",
    "L\\left ( \\mathbf{\\hat{y}} \\right ) &= \\frac{1}{M} \\left ( \\mathbf{e}^{T} \\mathbf{e} \\right )  + \\lambda \\left \\| \\boldsymbol{w} \\right \\|_2^2\\\\\n",
    "\\mathbf{e} &:= \\mathbf{\\hat{y}} - \\mathbf{y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: $\\lambda$ is called \"Hyperparameters\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def __evaluate__(self, x, y_true):\n",
    "    # original loss\n",
    "    e = self.predict(x) - torch.tensor(y_true)\n",
    "    return ((e.T @ e) / e.numel()).item()\n",
    "\n",
    "@add_to_class(SimpleLinearRegression)\n",
    "def evaluate(self, x, y_true):\n",
    "    # new loss\n",
    "    return (self.__evaluate__(x, y_true) + self.lambd * (self.w**2).sum()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{w}} = \n",
    "\\frac{\\partial }{\\partial \\boldsymbol{w}} \\left ({\\color{Red} \\frac{1}{M} (\\boldsymbol{e}^T \\boldsymbol{e})} \\right)\n",
    "+ \\frac{\\partial }{\\partial \\boldsymbol{w}} \\left( {\\color{Blue} \\lambda \\left \\| \\boldsymbol{w} \\right \\|_2^2} \\right)\n",
    "$$\n",
    "where the ${\\color{Red} \\text{red}}$ part is the original MSE loss function and the ${\\color{Blue} \\text{blue}}$ is the regularizer.\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\boldsymbol{w}} \\left ( \\frac{1}{M} (\\boldsymbol{e}^T \\boldsymbol{e}) \\right) = \\frac{2}{M} \\mathbf{X}^{T} \\left( \\mathbf{\\hat{y}} - \\mathbf{y} \\right)\n",
    "$$\n",
    "\n",
    "and:\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\boldsymbol{w}} \\left( \\lambda \\left \\| \\boldsymbol{w} \\right \\|_2^2 \\right) = 2 \\lambda \\boldsymbol{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update weights\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w} & \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} L = \\boldsymbol{w} - \\alpha \\left( \\frac{2}{M} \\mathbf{X}^{T} \\left( \\mathbf{\\hat{y}} - \\mathbf{y} \\right) + 2 \\lambda \\boldsymbol{w} \\right)\\\\\n",
    "b & \\leftarrow b - \\alpha \\nabla_{b} L = b - \\alpha \\left(\\frac{2}{M} \\mathbf{1} \\left( \\mathbf{\\hat{y}} - \\mathbf{y} \\right) \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def update(self, x, y_true, y_pred, lr : float):\n",
    "    m = len(y_true)\n",
    "    e = y_pred - y_true\n",
    "    self.w -= lr * (2 / m * (x.T @ e) + (2 * self.lambd * self.w))\n",
    "    self.b -= lr * 2 / m * (torch.ones_like(y_true).T @ e)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def fit(self, x_train, y_train, epochs : int, lr : float, batch_size : int, x_valid, y_valid):\n",
    "    self.loss_history = list()\n",
    "    self.loss_v_history = list()\n",
    "    for i in range(epochs):\n",
    "        for batch in range(0, len(x_train), batch_size):\n",
    "            x_t = torch.tensor(x_train[batch:batch+batch_size])\n",
    "            y_t = torch.tensor(y_train[batch:batch+batch_size])\n",
    "\n",
    "            y_p = self.predict(x_t)\n",
    "            \n",
    "            self.update(x_t, y_t, y_p, lr)\n",
    "\n",
    "        self.loss_history.append(self.evaluate(x_train, y_train))\n",
    "        self.loss_v_history.append(self.evaluate(x_valid, y_valid))\n",
    "        print('iter: {} - MSE: {} - MSEv: {}'.format(i, round(self.loss_history[-1],4), round(self.loss_v_history[-1],4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 4) (85, 1)\n",
      "(15, 4) (15, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = X[:85], Y[:85]\n",
    "X_valid, Y_valid = X[85:], Y[85:]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step - loss: 5097.5269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m5\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> (40.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5\u001b[0m (40.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> (40.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5\u001b[0m (40.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LAMBD = 0.03\n",
    "LR = 0.01\n",
    "EPOCHS = 5\n",
    "BATCH = len(X_train) // 3\n",
    "'''\n",
    "BATCH <- M : [x_1, ..., x_M]\n",
    "BATCH <- a : [x_1, ..., x_a]\n",
    "'''\n",
    "\n",
    "TFModel = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=U, \n",
    "                          activation='linear',\n",
    "                          kernel_regularizer = tf.keras.regularizers.L2(l2=LAMBD)),\n",
    "])\n",
    "\n",
    "TFModel.compile(\n",
    "    loss = tf.keras.losses.MSE,\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=LR)\n",
    ")\n",
    "\n",
    "TFModel.evaluate(X_train[:1], Y_train[:1])\n",
    "\n",
    "TFModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My model scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "MyModel = SimpleLinearRegression(N)\n",
    "MyModel.copyParams(TFModel)\n",
    "\n",
    "MyModel.lambd = LAMBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "5.948146695248522e-15\n"
     ]
    }
   ],
   "source": [
    "tf_predict = TFModel.predict(X_train, batch_size=len(X_train))\n",
    "my_predict = MyModel.predict(X_train)\n",
    "\n",
    "print(np.mean(np.abs((tf_predict - my_predict.numpy()) / tf_predict)) * 100)\n",
    "\n",
    "del tf_predict\n",
    "del my_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2703.1289\n",
      "1.6822999702075096e-14\n"
     ]
    }
   ],
   "source": [
    "tf_loss = TFModel.evaluate(X_train, Y_train, batch_size=len(X_train))\n",
    "my_loss = MyModel.evaluate(X_train, Y_train)\n",
    "\n",
    "print(np.mean(np.abs((tf_loss - my_loss) / tf_loss)) * 100)\n",
    "\n",
    "del tf_loss\n",
    "del my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 3079.9163 - val_loss: 1843.1484\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - loss: 2306.4426 - val_loss: 1498.0261\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1800.2439 - val_loss: 1250.0793\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1457.3982 - val_loss: 1065.4913\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1215.9447 - val_loss: 923.3408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c8565096d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFModel.fit(X_train, Y_train, batch_size=BATCH, epochs=EPOCHS, shuffle=False,\n",
    "            validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 - MSE: 2124.41 - MSEv: 1843.1484\n",
      "iter: 1 - MSE: 1725.384 - MSEv: 1498.0261\n",
      "iter: 2 - MSE: 1439.2224 - MSEv: 1250.0794\n",
      "iter: 3 - MSE: 1225.627 - MSEv: 1065.4913\n",
      "iter: 4 - MSE: 1060.0559 - MSEv: 923.3408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lords\\AppData\\Local\\Temp\\ipykernel_9932\\1366431534.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x) @ self.w + self.b\n"
     ]
    }
   ],
   "source": [
    "MyModel.fit(X_train, Y_train, EPOCHS, LR, BATCH, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2040176509456703e-14\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "tf_new_w = TFModel.weights[0].numpy()\n",
    "my_new_w = MyModel.w.numpy()\n",
    "\n",
    "print(np.mean(np.abs((tf_new_w - my_new_w) / tf_new_w)) * 100)\n",
    "\n",
    "del tf_new_w\n",
    "del my_new_w\n",
    "\n",
    "tf_new_b = TFModel.weights[1].numpy()\n",
    "my_new_b = MyModel.b.numpy()\n",
    "\n",
    "print(np.mean(np.abs((tf_new_b - my_new_b) / tf_new_b)) * 100)\n",
    "\n",
    "del tf_new_b\n",
    "del my_new_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\lambda \\to 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5703])\n",
      "tensor([[ 1.0227],\n",
      "        [ 0.7532],\n",
      "        [ 0.1319],\n",
      "        [-0.0224]])\n"
     ]
    }
   ],
   "source": [
    "model1 = SimpleLinearRegression(N)\n",
    "\n",
    "model1.lambd = 0\n",
    "\n",
    "print(model1.b)\n",
    "print(model1.w)\n",
    "\n",
    "B_copy = model1.b.clone()\n",
    "W_copy = model1.w.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lords\\AppData\\Local\\Temp\\ipykernel_9932\\1366431534.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x) @ self.w + self.b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 - MSE: 87.785 - MSEv: 66.8598\n",
      "iter: 1 - MSE: 4.3427 - MSEv: 3.3519\n",
      "iter: 2 - MSE: 1.1225 - MSEv: 0.7832\n",
      "iter: 3 - MSE: 0.9854 - MSEv: 0.5888\n",
      "iter: 4 - MSE: 0.9791 - MSEv: 0.5529\n",
      "iter: 5 - MSE: 0.9788 - MSEv: 0.5426\n",
      "iter: 6 - MSE: 0.9788 - MSEv: 0.5393\n",
      "iter: 7 - MSE: 0.9788 - MSEv: 0.5382\n",
      "iter: 8 - MSE: 0.9788 - MSEv: 0.5378\n",
      "iter: 9 - MSE: 0.9788 - MSEv: 0.5377\n",
      "iter: 10 - MSE: 0.9788 - MSEv: 0.5376\n",
      "iter: 11 - MSE: 0.9788 - MSEv: 0.5376\n",
      "iter: 12 - MSE: 0.9788 - MSEv: 0.5376\n",
      "iter: 13 - MSE: 0.9788 - MSEv: 0.5376\n",
      "iter: 14 - MSE: 0.9788 - MSEv: 0.5376\n"
     ]
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train, 15, 0.01, 1, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.59282878003542e+18\n",
      "7.2653245400988515\n"
     ]
    }
   ],
   "source": [
    "my_new_w = model1.w.numpy()\n",
    "\n",
    "print(np.mean(np.abs((TRUE_W - my_new_w + 1e-16) / (TRUE_W + 1e-16))) * 100)\n",
    "\n",
    "del my_new_w\n",
    "\n",
    "my_new_b = model1.b.numpy()\n",
    "\n",
    "print(np.mean(np.abs((TRUE_B - my_new_b) / TRUE_B)) * 100)\n",
    "\n",
    "del my_new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5703])\n",
      "tensor([[ 1.0227],\n",
      "        [ 0.7532],\n",
      "        [ 0.1319],\n",
      "        [-0.0224]])\n"
     ]
    }
   ],
   "source": [
    "model2 = SimpleLinearRegression(N)\n",
    "\n",
    "model2.lambd = 3\n",
    "\n",
    "model2.b.copy_(B_copy)\n",
    "model2.w.copy_(W_copy)\n",
    "\n",
    "print(model2.b)\n",
    "print(model2.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lords\\AppData\\Local\\Temp\\ipykernel_9932\\1366431534.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x) @ self.w + self.b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 - MSE: 1346.354 - MSEv: 1885.6678\n",
      "iter: 1 - MSE: 1352.1553 - MSEv: 1891.811\n",
      "iter: 2 - MSE: 1353.4237 - MSEv: 1892.919\n",
      "iter: 3 - MSE: 1353.6516 - MSEv: 1893.1185\n",
      "iter: 4 - MSE: 1353.6921 - MSEv: 1893.154\n",
      "iter: 5 - MSE: 1353.6993 - MSEv: 1893.1603\n",
      "iter: 6 - MSE: 1353.7006 - MSEv: 1893.1614\n",
      "iter: 7 - MSE: 1353.7008 - MSEv: 1893.1616\n",
      "iter: 8 - MSE: 1353.7009 - MSEv: 1893.1617\n",
      "iter: 9 - MSE: 1353.7009 - MSEv: 1893.1617\n",
      "iter: 10 - MSE: 1353.7009 - MSEv: 1893.1617\n",
      "iter: 11 - MSE: 1353.7009 - MSEv: 1893.1617\n",
      "iter: 12 - MSE: 1353.7009 - MSEv: 1893.1617\n",
      "iter: 13 - MSE: 1353.7009 - MSEv: 1893.1617\n",
      "iter: 14 - MSE: 1353.7009 - MSEv: 1893.1617\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, 15, 0.01, 1, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3985397564566083e+18\n",
      "1154.1058337864947\n"
     ]
    }
   ],
   "source": [
    "my_new_w = model2.w.numpy()\n",
    "\n",
    "print(np.mean(np.abs((TRUE_W - my_new_w + 1e-16) / (TRUE_W + 1e-16))) * 100)\n",
    "\n",
    "del my_new_w\n",
    "\n",
    "my_new_b = model2.b.numpy()\n",
    "\n",
    "print(np.mean(np.abs((TRUE_B - my_new_b) / TRUE_B)) * 100)\n",
    "\n",
    "del my_new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5703])\n",
      "tensor([[ 1.0227],\n",
      "        [ 0.7532],\n",
      "        [ 0.1319],\n",
      "        [-0.0224]])\n"
     ]
    }
   ],
   "source": [
    "model3 = SimpleLinearRegression(N)\n",
    "\n",
    "model3.lambd = 0.001\n",
    "\n",
    "model3.b.copy_(B_copy)\n",
    "model3.w.copy_(W_copy)\n",
    "\n",
    "print(model3.b)\n",
    "print(model3.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lords\\AppData\\Local\\Temp\\ipykernel_9932\\1366431534.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x) @ self.w + self.b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 - MSE: 88.2303 - MSEv: 69.1165\n",
      "iter: 1 - MSE: 4.484 - MSEv: 6.0747\n",
      "iter: 2 - MSE: 1.1522 - MSEv: 3.5678\n",
      "iter: 3 - MSE: 0.9905 - MSEv: 3.3842\n",
      "iter: 4 - MSE: 0.9793 - MSEv: 3.3506\n",
      "iter: 5 - MSE: 0.9781 - MSEv: 3.3409\n",
      "iter: 6 - MSE: 0.978 - MSEv: 3.3378\n",
      "iter: 7 - MSE: 0.9779 - MSEv: 3.3367\n",
      "iter: 8 - MSE: 0.9779 - MSEv: 3.3363\n",
      "iter: 9 - MSE: 0.9779 - MSEv: 3.3362\n",
      "iter: 10 - MSE: 0.9779 - MSEv: 3.3362\n",
      "iter: 11 - MSE: 0.9779 - MSEv: 3.3362\n",
      "iter: 12 - MSE: 0.9779 - MSEv: 3.3362\n",
      "iter: 13 - MSE: 0.9779 - MSEv: 3.3362\n",
      "iter: 14 - MSE: 0.9779 - MSEv: 3.3362\n"
     ]
    }
   ],
   "source": [
    "model3.fit(X_train, Y_train, 15, 0.01, 1, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5878620720961987e+18\n",
      "8.676180964243684\n"
     ]
    }
   ],
   "source": [
    "my_new_w = model3.w.numpy()\n",
    "\n",
    "print(np.mean(np.abs((TRUE_W - my_new_w + 1e-16) / (TRUE_W + 1e-16))) * 100)\n",
    "\n",
    "del my_new_w\n",
    "\n",
    "my_new_b = model3.b.numpy()\n",
    "\n",
    "print(np.mean(np.abs((TRUE_B - my_new_b) / TRUE_B)) * 100)\n",
    "\n",
    "del my_new_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
