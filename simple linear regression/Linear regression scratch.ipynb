{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.config.set_floatx('float64')\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):  \n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "$$\n",
    "\\mathbf{X} \\in \\mathbb{R}^{M,N} \\\\\n",
    "\\mathbf{y} \\in \\mathbb{R}^{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(100, 1)\n",
      "0.43370068789684035\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import random\n",
    "\n",
    "M, N, U = 100, 4, 1\n",
    "TRUE_B = random.random()\n",
    "X, Y, TRUE_W = make_regression(n_samples=M, n_features=N, n_targets=U, n_informative=N-1, bias=TRUE_B, noise=1, coef=True)\n",
    "\n",
    "if U == 1: Y = Y.reshape((-1, 1))\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "print(TRUE_B)\n",
    "print(TRUE_W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights\n",
    "Parameters trainables:\n",
    "$$\n",
    "\\mathbf{w} \\in \\mathbb{R}^{N} \\\\\n",
    "b \\in \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self, number_features : int):\n",
    "        self.w = torch.randn(number_features, 1)\n",
    "        self.b = torch.randn(1)\n",
    "\n",
    "    def copyParams(self, tf_model):\n",
    "        self.w.copy_(torch.tensor(tf_model.weights[0].numpy()))\n",
    "        self.b.copy_(torch.tensor(tf_model.weights[1].numpy()))\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weigthed sum:\n",
    "$$\n",
    "\\mathbf{\\hat{y}} \\left( \\mathbf{X} \\right) = \\mathbf{X} \\mathbf{w} + b\\\\\n",
    "\\mathbf{\\hat{y}} : \\mathbb{R}^{M,N} \\rightarrow \\mathbb{R}^{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def predict(self, x):\n",
    "    return torch.tensor(x) @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE\n",
    "Loss function: Mean Squared Error\n",
    "$$\n",
    "L\\left ( \\mathbf{\\hat{y}} \\right ) = \\frac{1}{M} \\sum_{i=1}^{M} \\left ( \\mathbf{\\hat{y}}_i - \\mathbf{y}_i \\right )^{2} \\\\\n",
    "L : \\mathbb{R}^{M} \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Vectorized form:\n",
    "$$\n",
    "\\begin{align}\n",
    "L\\left ( \\mathbf{\\hat{y}} \\right ) &= \\frac{1}{M} \\left ( \\mathbf{e}^{T} \\mathbf{e} \\right ) \\\\\n",
    "\\mathbf{e} &:= \\mathbf{\\hat{y}} - \\mathbf{y}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def evaluate(self, x, y_true):\n",
    "    e = self.predict(x) - torch.tensor(y_true)\n",
    "    return ((e.T @ e) / e.numel()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "Gradient Descent: **Denomitator layout notation**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = \n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}}\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}}\n",
    "$$\n",
    "\n",
    "where their shapes are:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} \\in \\mathbb{R}^{N},\n",
    "\\frac{\\partial L}{\\partial b} \\in \\mathbb{R},\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}} \\in \\mathbb{R}^{N,M}, \n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b} \\in \\mathbb{R}^{1,M},\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}} \\in \\mathbb{R}^{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weigthed sum derivative\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{\\hat{y}} &= \\mathbf{X} \\mathbf{w} + b \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1^{T} \\\\ \n",
    "\\mathbf{x}_2^{T} \\\\ \n",
    "\\vdots  \\\\ \n",
    "\\mathbf{x}_{M}^{T}\n",
    "\\end{bmatrix}\n",
    "\\mathbf{w} + b \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1^{T} \\mathbf{w} + b \\\\ \n",
    "\\mathbf{x}_2^{T} \\mathbf{w} + b \\\\ \n",
    "\\vdots  \\\\ \n",
    "\\mathbf{x}_{M}^{T} \\mathbf{w} + b\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\mathbf{x}_{i}^T = \\begin{bmatrix}\n",
    "x_{i,1} & x_{i,2} & \\cdots & x_{i,N}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b}\n",
    "$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial b} &=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\hat{y}_1}{\\partial b} & \\frac{\\partial \\hat{y}_2}{\\partial b} & \\cdots & \\frac{\\partial \\hat{y}_M}{\\partial b} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\mathbf{1} \\in \\mathbb{R}^{1,M}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}}\n",
    "$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\hat{y}_1}{\\partial w_1} & \\frac{\\partial \\hat{y}_2}{\\partial w_1} & \\cdots & \\frac{\\partial \\hat{y}_M}{\\partial w_1} \\\\\n",
    "\\frac{\\partial \\hat{y}_1}{\\partial w_2} & \\frac{\\partial \\hat{y}_2}{\\partial w_2} & & \\frac{\\partial \\hat{y}_M}{\\partial w_2} \\\\\n",
    "\\vdots  &  & \\ddots  & \\vdots \\\\ \n",
    "\\frac{\\partial \\hat{y}_1}{\\partial w_N} & \\frac{\\partial \\hat{y}_2}{\\partial w_N} & \\cdots  & \\frac{\\partial \\hat{y}_M}{\\partial w_N}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "wherer\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial w_j} &=\n",
    "\\frac{\\partial }{\\partial w_j}\\left ( x_{i,1} w_{1} + x_{i,2} w_{2} + \\cdots + x_{i,j} w_{j} + \\cdots + x_{i,N} w_{N} \\right ) \\\\\n",
    "&= x_{i,j}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "then:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}} &=\n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{2,1} & \\cdots & x_{M,1} \\\\\n",
    "x_{1,2} & x_{2,2} & & x_{M,2} \\\\\n",
    "\\vdots  &  & \\ddots  & \\vdots \\\\ \n",
    "x_{1,N} & x_{2,N} & \\cdots  & x_{M,N}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\mathbf{X}^T\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE derivavite:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}} =\n",
    "\\frac{\\partial \\mathbf{e}}{\\partial \\mathbf{\\hat{y}}}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{e}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{e}}{\\partial \\mathbf{\\hat{y}}} \\in \\mathbb{R}^{M,M}, \n",
    "\\frac{\\partial L}{\\partial \\mathbf{e}} \\in \\mathbb{R}^{M}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathbf{e}}{\\partial \\mathbf{\\hat{y}}}\n",
    "$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{e} &= \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 - y_1 \\\\ \n",
    "\\hat{y}_2 - y_2 \\\\ \n",
    "\\vdots  \\\\ \n",
    "\\hat{y}_M - y_M\n",
    "\\end{bmatrix} \\\\\n",
    "\n",
    "\\frac{\\partial \\mathbf{e}}{\\partial \\mathbf{\\hat{y}}} &=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial e_1}{\\partial \\hat{y}_1} & \\frac{\\partial e_2}{\\partial \\hat{y}_1} & \\cdots & \\frac{\\partial e_M}{\\partial \\hat{y}_1} \\\\\n",
    "\\frac{\\partial e_1}{\\partial \\hat{y}_2} & \\frac{\\partial e_2}{\\partial \\hat{y}_2} & & \\frac{\\partial e_M}{\\partial \\hat{y}_2} \\\\ \n",
    "\\vdots  &  & \\ddots  & \\vdots \\\\ \n",
    "\\frac{\\partial e_1}{\\partial \\hat{y}_M} & \\frac{\\partial e_2}{\\partial \\hat{y}_M} & \\cdots & \\frac{\\partial e_M}{\\partial \\hat{y}_M}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\boldsymbol{I} \\in \\mathbb{R}^{M,M}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{e}}\n",
    "$\n",
    "$$\n",
    "\\begin{align}\n",
    "L &= \\frac{1}{M} \\left ( e_{1}^{2} + e_{2}^{2}+ \\cdots + e_{M}^{2}\\right ) \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{e}} &= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial e_1} \\\\ \n",
    "\\frac{\\partial L}{\\partial e_2} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial L}{\\partial e_M}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\frac{2}{M}\n",
    "\\begin{bmatrix}\n",
    "e_1 \\\\ \n",
    "e_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "e_M\n",
    "\\end{bmatrix}\n",
    "= \\frac{2}{M} \\mathbf{e}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathbf{e}}{\\partial \\mathbf{\\hat{y}}}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{e}}\n",
    "$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}} =\n",
    "\\frac{2}{M} \\boldsymbol{I} \\mathbf{e} =\n",
    "\\frac{2}{M} \\mathbf{e} =\n",
    "\\frac{2}{M} \\left( \\mathbf{\\hat{y}} - \\mathbf{y} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}}\n",
    "$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{2}{M} \\mathbf{X}^{T} \\left( \\mathbf{\\hat{y}} - \\mathbf{y} \\right)\n",
    "$$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b}\n",
    "$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{2}{M} \\mathbf{1} \\left( \\mathbf{\\hat{y}} - \\mathbf{y} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update weights\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w} & \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} L = \\boldsymbol{w} - \\alpha \\left( \\frac{2}{M} \\mathbf{X}^{T} \\left( \\mathbf{\\hat{y}} - \\mathbf{y} \\right) \\right)\\\\\n",
    "b & \\leftarrow b - \\alpha \\nabla_{b} L = b - \\alpha \\left(\\frac{2}{M} \\mathbf{1} \\left( \\mathbf{\\hat{y}} - \\mathbf{y} \\right) \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def update(self, x, y_true, y_pred, lr : float):\n",
    "    m = len(y_true)\n",
    "    e = y_pred - y_true\n",
    "    self.w -= lr * 2 / m * (x.T @ e)\n",
    "    self.b -= lr * 2 / m * (torch.ones_like(y_true).T @ e)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(SimpleLinearRegression)\n",
    "def fit(self, x_train, y_train, epochs : int, lr : float, batch_size : int, x_valid, y_valid):\n",
    "    for i in range(epochs):\n",
    "        for batch in range(0, len(x_train), batch_size):\n",
    "            x_t = torch.tensor(x_train[batch:batch+batch_size])\n",
    "            y_t = torch.tensor(y_train[batch:batch+batch_size])\n",
    "\n",
    "            y_p = self.predict(x_t)\n",
    "            \n",
    "            self.update(x_t, y_t, y_p, lr)\n",
    "\n",
    "        loss_v = round(self.evaluate(x_valid, y_valid), 4)\n",
    "        print('iter: {} - MSEv: {}'.format(i, loss_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 4) (85, 1)\n",
      "(15, 4) (15, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = X[:85], Y[:85]\n",
    "X_valid, Y_valid = X[85:], Y[85:]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 964ms/step - loss: 6313.1553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m5\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> (40.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5\u001b[0m (40.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> (40.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5\u001b[0m (40.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR = 0.001\n",
    "EPOCHS = 5\n",
    "BATCH = len(X_train) // 3\n",
    "'''\n",
    "BATCH <- M : [x_1, ..., x_M]\n",
    "BATCH <- a : [x_1, ..., x_a]\n",
    "'''\n",
    "\n",
    "TFModel = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=U, activation='linear'),\n",
    "])\n",
    "\n",
    "TFModel.compile(\n",
    "    loss = tf.keras.losses.MSE,\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=LR)\n",
    ")\n",
    "\n",
    "TFModel.evaluate(X[:1], Y[:1])\n",
    "\n",
    "TFModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My model scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "MyModel = SimpleLinearRegression(N)\n",
    "MyModel.copyParams(TFModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "1.0493574959227039e-14\n"
     ]
    }
   ],
   "source": [
    "tf_predict = TFModel.predict(X_train, batch_size=len(X_train))\n",
    "my_predict = MyModel.predict(X_train)\n",
    "\n",
    "print(np.mean(np.abs((tf_predict - my_predict.numpy()) / tf_predict)) * 100)\n",
    "\n",
    "del tf_predict\n",
    "del my_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 9896.7295\n",
      "1.837970199267847e-14\n"
     ]
    }
   ],
   "source": [
    "tf_loss = TFModel.evaluate(X_train, Y_train, batch_size=len(X_train))\n",
    "my_loss = MyModel.evaluate(X_train, Y_train)\n",
    "\n",
    "print(np.mean(np.abs((tf_loss - my_loss) / tf_loss)) * 100)\n",
    "\n",
    "del tf_loss\n",
    "del my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - loss: 8717.1377 - val_loss: 9188.6982\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 8610.5986 - val_loss: 9067.4307\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8505.5195 - val_loss: 8947.8994\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 8401.8779 - val_loss: 8830.0781\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 8299.6514 - val_loss: 8713.9404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2c1e8793910>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFModel.fit(X_train, Y_train, batch_size=BATCH, epochs=EPOCHS, shuffle=False,\n",
    "            validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 - MSEv: 9188.698\n",
      "iter: 1 - MSEv: 9067.4306\n",
      "iter: 2 - MSEv: 8947.8996\n",
      "iter: 3 - MSEv: 8830.0782\n",
      "iter: 4 - MSEv: 8713.9402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lords\\AppData\\Local\\Temp\\ipykernel_9664\\1366431534.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x) @ self.w + self.b\n"
     ]
    }
   ],
   "source": [
    "MyModel.fit(X_train, Y_train, EPOCHS, LR, BATCH, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1937113788916998e-14\n",
      "2.851870730117414e-14\n"
     ]
    }
   ],
   "source": [
    "tf_new_w = TFModel.weights[0].numpy()\n",
    "my_new_w = MyModel.w.numpy()\n",
    "\n",
    "print(np.mean(np.abs((tf_new_w - my_new_w) / tf_new_w)) * 100)\n",
    "\n",
    "del tf_new_w\n",
    "del my_new_w\n",
    "\n",
    "tf_new_b = TFModel.weights[1].numpy()\n",
    "my_new_b = MyModel.b.numpy()\n",
    "\n",
    "print(np.mean(np.abs((tf_new_b - my_new_b) / tf_new_b)) * 100)\n",
    "\n",
    "del tf_new_b\n",
    "del my_new_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5895])\n",
      "tensor([[-0.0233],\n",
      "        [-0.7321],\n",
      "        [-0.5472],\n",
      "        [ 0.2832]])\n"
     ]
    }
   ],
   "source": [
    "model = SimpleLinearRegression(N)\n",
    "print(model.b)\n",
    "print(model.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9290.23131847044"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lords\\AppData\\Local\\Temp\\ipykernel_9664\\1366431534.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x) @ self.w + self.b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 - MSEv: 383.6558\n",
      "iter: 1 - MSEv: 25.7435\n",
      "iter: 2 - MSEv: 2.9641\n",
      "iter: 3 - MSEv: 1.191\n",
      "iter: 4 - MSEv: 1.0121\n",
      "iter: 5 - MSEv: 0.9854\n",
      "iter: 6 - MSEv: 0.9798\n",
      "iter: 7 - MSEv: 0.9784\n",
      "iter: 8 - MSEv: 0.9781\n",
      "iter: 9 - MSEv: 0.978\n",
      "iter: 10 - MSEv: 0.978\n",
      "iter: 11 - MSEv: 0.978\n",
      "iter: 12 - MSEv: 0.978\n",
      "iter: 13 - MSEv: 0.978\n",
      "iter: 14 - MSEv: 0.978\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, 15, 0.01, 1, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2503])\n",
      "tensor([[85.4215],\n",
      "        [49.3152],\n",
      "        [35.9923],\n",
      "        [ 0.1085]])\n"
     ]
    }
   ],
   "source": [
    "print(model.b)\n",
    "print(model.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43370068789684035\n",
      "[85.44456309 49.37237061 36.00096697  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(TRUE_B)\n",
    "print(TRUE_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9785.152469275066\n",
      "42.27808520735004\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs((TRUE_B - model.w.numpy()) / TRUE_B)) * 100)\n",
    "print(np.mean(np.abs((TRUE_B - model.b.numpy()) / TRUE_B)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9779600866966783"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, Y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
