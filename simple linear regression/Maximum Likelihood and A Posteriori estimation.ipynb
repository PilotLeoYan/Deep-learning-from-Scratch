{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "&p\\left ( y \\mid \\boldsymbol{x}^T \\boldsymbol{\\theta} \\right )\n",
    "= \\mathcal{N} \\left ( y \\mid \\boldsymbol{x}^T \\boldsymbol{\\theta}, \\sigma^2 \\right ) \\\\\n",
    "\\Leftrightarrow &y = \\boldsymbol{x}^T \\boldsymbol{\\theta} + \\epsilon,\n",
    "\\quad \\epsilon \\sim \\mathcal{N}\\left ( 0, \\sigma^2 \\right )\n",
    "\\end{align}\n",
    "$$\n",
    "we assume that the noise variance $\\sigma^2$ is know and focus on learning the model parameters $\\boldsymbol{\\theta}$.\n",
    "'https://mml-book.com.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "M, N = 100, 3\n",
    "X = torch.randint(-10, 11, (M, N), dtype=torch.float64)\n",
    "\n",
    "TrueTheta = torch.randint(-8, 9, (N, 1), dtype=torch.float64)\n",
    "noise_variance = 0.01\n",
    "\n",
    "Y = X @ TrueTheta + torch.normal(0, noise_variance, (M, 1))\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(TrueTheta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a traing set $\\left \\{ \\left ( \\boldsymbol{x}_1, y_1 \\right ), \\cdots ,\\left ( \\boldsymbol{x}_M, y_M \\right ) \\right \\}$. \n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) &= p\\left ( y_1, \\cdots, y_N \\mid \\boldsymbol{x}_1, \\cdots, \\boldsymbol{x}_N, \\boldsymbol{\\theta} \\right ) \\\\\n",
    "&= \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\n",
    "= \\prod_{n=1}^N \\mathcal{N}(y_n | \\boldsymbol x_n^T \\boldsymbol\\theta, \\sigma^2)\n",
    "\\end{align}\n",
    "$$\n",
    "Where we defined $\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}$ and $\\mathcal Y = \\{y_1, \\ldots, y_N\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximun likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem to solve:\n",
    "$$\n",
    "\\boldsymbol\\theta_{ML} = \\arg \\max_{\\boldsymbol\\theta} p(\\mathcal X \\mid \\mathcal Y, \\boldsymbol\\theta)\n",
    "$$\n",
    "To find the optimal parameters, we minimize the negative log-likelihood\n",
    "$$\n",
    "-\\log p(\\mathcal Y \\mid \\mathcal X, \\boldsymbol\\theta) =\n",
    "-\\log \\prod_{m=1}^{M} p(y_m \\mid \\boldsymbol x_M, \\boldsymbol\\theta) =\n",
    "-\\sum_{m=1}^{M} \\log  p(y_m \\mid \\boldsymbol x_M, \\boldsymbol\\theta)\n",
    "$$\n",
    "where we assume that factorizes over the number of data points due to our independence assumption on training set.\n",
    "$$\n",
    "\\log p(y_m \\mid \\boldsymbol x_M, \\boldsymbol\\theta) =\n",
    "-\\frac{1}{2\\sigma^2}\\left ( y_m - \\boldsymbol x_m^T \\boldsymbol\\theta \\right )^2 + \\text{const}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we include all probabilities with the sum on each data point, and we define a function to do this:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol L(\\boldsymbol\\theta) :&= \\frac{1}{2\\sigma^2} \\sum_{m=1}^{M} \\left ( y_m - \\boldsymbol x_m^T \\boldsymbol\\theta \\right )^2 \\\\\n",
    "&= \\frac{1}{2\\sigma^2} \\left ( \\boldsymbol y - \\boldsymbol X \\boldsymbol\\theta \\right )^T \\left ( \\boldsymbol y - \\boldsymbol X \\boldsymbol\\theta \\right )\n",
    "\\end{align}\n",
    "$$\n",
    "we compude the gradient $L$ with respect to the parameters as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\mathrm{d} \\boldsymbol L}{\\mathrm{d} \\boldsymbol\\theta} &= \n",
    "-\\frac{1}{\\sigma^2} \\boldsymbol X^T \\left ( \\boldsymbol y -  \\boldsymbol X \\boldsymbol\\theta \\right ) \\\\\n",
    "&= -\\frac{1}{\\sigma^2} (\\boldsymbol X^T \\boldsymbol y - \\boldsymbol X^T \\boldsymbol X \\boldsymbol\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "now we need to solve $\\frac{\\mathrm{d} \\boldsymbol L}{\\mathrm{d} \\boldsymbol\\theta} = \\boldsymbol 0$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol X^T \\boldsymbol X \\boldsymbol\\theta &= \\boldsymbol X^T \\boldsymbol y \\\\\n",
    "\\boldsymbol\\theta &= (\\boldsymbol X^T \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.0000],\n",
      "        [-3.9999],\n",
      "        [-5.0001]])\n"
     ]
    }
   ],
   "source": [
    "# maximun likelihood estimation\n",
    "MLE = torch.inverse(X.T @ X) @ X.T @ Y\n",
    "print(MLE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
